{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem.</p>"},{"location":"#all-of-the-projects","title":"\ud83d\udd0d All of the Projects","text":"<p>Check out the other projects that are part of the SWE-bench ecosystem!</p>"},{"location":"#leaderboard","title":"\ud83c\udfc6 Leaderboard","text":"<p>You can find the full leaderboard at swebench.com!</p>"},{"location":"#overview","title":"\ud83d\udccb Overview","text":"<p>SWE-bench provides:</p> <ul> <li>\u2705 Real-world GitHub issues - Evaluate LLMs on actual software engineering tasks</li> <li>\u2705 Reproducible evaluation - Docker-based evaluation harness for consistent results</li> <li>\u2705 Multiple datasets - SWE-bench, SWE-bench Lite, SWE-bench Verified, and SWE-bench Multimodal</li> </ul>"},{"location":"#latest-news","title":"\ud83d\udcf0 Latest News","text":"<ul> <li>[Jan. 13, 2025]: SWE-bench Multimodal integration with private test split evaluation</li> <li>[Jan. 11, 2025]: Cloud-based evaluations via Modal</li> <li>[Aug. 13, 2024]: SWE-bench Verified release with 500 engineer-confirmed solvable problems</li> <li>[Jun. 27, 2024]: Fully containerized evaluation harness using Docker</li> <li>[Apr. 2, 2024]: SWE-agent release with state-of-the-art results</li> <li>[Jan. 16, 2024]: SWE-bench accepted to ICLR 2024 as an oral presentation</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Access SWE-bench via Hugging Face\nfrom datasets import load_dataset\nswebench = load_dataset('princeton-nlp/SWE-bench', split='test')\n</code></pre> <pre><code># Setup with Docker\ngit clone git@github.com:princeton-nlp/SWE-bench.git\ncd SWE-bench\npip install -e .\n</code></pre>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":"<ul> <li>Installation - Setup instructions for local and cloud environments</li> <li>Guides</li> <li>Quickstart - Get started with SWE-bench</li> <li>Evaluation - How to evaluate models on SWE-bench</li> <li>Docker Setup - Configure Docker for SWE-bench</li> <li>Datasets - Available datasets and how to use them</li> <li>Create RAG Datasets - Build your own retrieval datasets</li> <li>Reference</li> <li>Harness API - Documentation for the evaluation harness</li> <li>Inference API - Documentation for model inference</li> <li>Versioning - Documentation for versioning</li> <li>FAQ - Frequently asked questions</li> </ul>"},{"location":"#available-resources","title":"\u2b07\ufe0f Available Resources","text":"Datasets Models RAG \ud83d\udcbf SWE-bench \ud83e\udd99 SWE-Llama 13b \ud83e\udd17 \"Oracle\" Retrieval \ud83d\udcbf SWE-bench Lite \ud83e\udd99 SWE-Llama 13b (PEFT) \ud83e\udd17 BM25 Retrieval 13K \ud83d\udcbf SWE-bench Verified \ud83e\udd99 SWE-Llama 7b \ud83e\udd17 BM25 Retrieval 27K \ud83d\udcbf SWE-bench Multimodal \ud83e\udd99 SWE-Llama 7b (PEFT) \ud83e\udd17 BM25 Retrieval 40K/50K"},{"location":"#contributing","title":"\ud83d\udcab Contributing","text":"<p>We welcome contributions from the NLP, Machine Learning, and Software Engineering communities! Please check our contributing guidelines for details.</p>"},{"location":"#citation","title":"\u270d\ufe0f Citation","text":"<pre><code>@inproceedings{\n    jimenez2024swebench,\n    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},\n    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=VTF8yNQM66}\n}\n\n@inproceedings{\n    yang2024swebenchmultimodal,\n    title={{SWE}-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?},\n    author={John Yang and Carlos E. Jimenez and Alex L. Zhang and Kilian Lieret and Joyce Yang and Xindi Wu and Ori Press and Niklas Muennighoff and Gabriel Synnaeve and Karthik R. Narasimhan and Diyi Yang and Sida I. Wang and Ofir Press},\n    booktitle={The Thirteenth International Conference on Learning Representations},\n    year={2025},\n    url={https://openreview.net/forum?id=riTiq3i21b}\n}\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-swe-bench","title":"What is SWE-bench?","text":"<p>SWE-bench is a benchmark for evaluating large language models on real-world software engineering tasks. It consists of GitHub issues and their corresponding fixes, allowing LLMs to be evaluated on their ability to generate patches that resolve these issues.</p>"},{"location":"faq/#which-datasets-are-available","title":"Which datasets are available?","text":"<p>SWE-bench offers five main datasets:</p> <ul> <li>SWE-bench: The full benchmark with 2,294 instances</li> <li>SWE-bench Lite: A smaller subset with 300 instances</li> <li>SWE-bench Verified: 500 instances verified by engineers as solvable</li> <li>SWE-bench Multimodal: 100 development instances with screenshots and UI elements (test eval is on the SWE-bench API.)</li> <li>SWE-bench Multilingual: 300 instances spanning 9 languages and 42 repositories</li> </ul>"},{"location":"faq/#how-does-the-evaluation-work","title":"How does the evaluation work?","text":"<p>The evaluation process: 1. Sets up a Docker environment for a repository 2. Applies the model's generated patch 3. Runs the repository's test suite 4. Determines if the patch successfully resolves the issue</p>"},{"location":"faq/#what-metrics-are-reported","title":"What metrics are reported?","text":"<p>Key metrics include: - Total instances: Number of instances in the dataset - Instances submitted: Number of instances the model attempted - Instances completed: Number of instances that completed the evaluation process - Instances resolved: Number of instances where the model's patch fixed the issue - Instances unresolved: Number of instances where the evaluation completed but the issue wasn't fixed - Instances with empty patches: Number of instances where the model returned an empty patch - Instances with errors: Number of instances where the evaluation encountered errors - Resolution rate: The percentage of submitted instances that were successfully resolved</p>"},{"location":"faq/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"faq/#how-can-i-save-disk-space-when-using-docker","title":"How can I save disk space when using Docker?","text":"<p>Use these commands to clean up Docker resources: <pre><code># Remove unused containers\ndocker container prune\n\n# Remove unused images\ndocker image prune\n\n# Remove all unused Docker objects\ndocker system prune\n</code></pre></p> <p>You can also set <code>--cache_level=env</code> and <code>--clean=True</code> when running <code>swebench.harness.run_evaluation</code> to only dynamically remove instance images after they are used. This will make each run longer, but will use less disk space.</p>"},{"location":"faq/#using-the-benchmark","title":"Using the Benchmark","text":""},{"location":"faq/#how-do-i-run-evaluations-on-my-own-model","title":"How do I run evaluations on my own model?","text":"<p>Generate predictions in the required format and use the evaluation harness: <pre><code>from swebench.harness.run_evaluation import run_evaluation\n\npredictions = [\n    {\n        \"instance_id\": \"repo_owner_name__repo_name-issue_number\",\n        \"model_name_or_path\": \"my-model-name\",\n        \"model_patch\": \"code patch here\"\n    }\n]\n\nresults = run_evaluation(\n    predictions=predictions,\n    dataset_name=\"SWE-bench_Lite\",\n)\n</code></pre></p> <p>More effectively, you can save the predictions to a file and then run the evaluation harness on that file: <pre><code>python -m swebench.harness.run_evaluation --predictions_path predictions.jsonl\n</code></pre></p>"},{"location":"faq/#can-i-run-evaluations-without-using-docker","title":"Can I run evaluations without using Docker?","text":"<p>No. Docker is required for consistent evaluation environments. This ensures that the evaluation is reproducible across different systems.</p>"},{"location":"faq/#how-can-i-speed-up-evaluations","title":"How can I speed up evaluations?","text":"<ul> <li>Use SWE-bench Lite instead of the full benchmark</li> <li>Increase parallelism with the <code>num_workers</code> parameter</li> <li>Use Modal for cloud-based evaluations</li> </ul>"},{"location":"faq/#what-format-should-my-models-output-be-in","title":"What format should my model's output be in?","text":"<p>Your model should produce a diff or patch that can be applied to the original code. The exact format depends on the instance, but we typically recommend the diff generated by <code>git diff</code>.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#my-evaluation-is-stuck-or-taking-too-long","title":"My evaluation is stuck or taking too long","text":"<p>Try: - Reducing the number of parallel workers - Checking Docker resource limits - Making sure you have enough disk space</p>"},{"location":"faq/#docker-build-is-failing-with-network-errors","title":"Docker build is failing with network errors","text":"<p>Make sure your Docker network is properly configured and you have internet access. You can try: <pre><code>docker network ls\ndocker network inspect bridge\n</code></pre></p>"},{"location":"faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"faq/#how-do-i-use-cloud-based-evaluation","title":"How do I use cloud-based evaluation?","text":"<p>Use Modal for cloud-based evaluations: <pre><code>from swebench.harness.modal_eval.run_modal import run_modal_evaluation\n\nresults = run_modal_evaluation(\n    predictions=predictions,\n    dataset_name=\"SWE-bench_Lite\",\n    parallelism=10\n)\n</code></pre></p>"},{"location":"faq/#how-do-i-contribute-to-swe-bench","title":"How do I contribute to SWE-bench?","text":"<p>You can contribute by: - Reporting issues or bugs - Debugging installation and testing issues as you run into them - Suggesting improvements to the benchmark or documentation</p>"},{"location":"faq/#i-want-to-build-a-custom-dataset-how-do-i-do-that","title":"I want to build a custom dataset. How do I do that?","text":"<p>Email us at <code>support@swebench.com</code> and we can talk about helping you build a custom dataset.</p>"},{"location":"installation/","title":"Installation","text":"<p>SWE-bench is designed to be easy to install and run on most systems with Docker support.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing SWE-bench, make sure you have the following prerequisites:</p> <ul> <li>Python 3.9+ - Required for running the package</li> <li>Docker - Essential for the evaluation environment</li> </ul>"},{"location":"installation/#standard-installation","title":"Standard Installation","text":"<p>For most users, the standard installation is the best option:</p> <pre><code># Clone the repository\ngit clone https://github.com/princeton-nlp/SWE-bench.git\ncd SWE-bench\n\n# Install the package\npip install -e .\n</code></pre> <p>This will install the package in development mode, allowing you to make changes to the code if needed.</p>"},{"location":"installation/#install-dependencies-for-dataset-generation-or-rag-inference","title":"Install dependencies for dataset generation or RAG inference","text":"<p>To install the dependencies for dataset generation, you can run the following command: <pre><code>pip install -e \".[make_datasets]\"\n</code></pre></p> <p>To install the dependencies for inference and dataset generation, you can run the following command: <pre><code>pip install -e \".[inference]\"\n</code></pre></p>"},{"location":"installation/#docker-setup","title":"Docker Setup","text":"<p>SWE-bench relies heavily on Docker for its evaluation environment. Make sure Docker is correctly installed and running:</p> <pre><code># Test that Docker is installed correctly\ndocker --version\ndocker run hello-world\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Depending on your use case, you might want to install additional dependencies:</p> <pre><code># For using SWE-Llama models locally\npip install -e \".[llama]\"\n\n# For development and testing\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#cloud-installation-modal","title":"Cloud Installation (Modal)","text":"<p>For running evaluations in the cloud using Modal:</p> <pre><code>pip install modal\nmodal setup  # First-time setup only\npip install -e \".[modal]\"\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation:</p> <ol> <li>Docker permission issues: You might need to add your user to the Docker group</li> <li>Python version conflicts: Make sure you're using Python 3.9+</li> <li>Package conflicts: Consider using a virtual environment</li> </ol> <p>For more detailed troubleshooting, please refer to our FAQ page or open an issue on GitHub. </p>"},{"location":"api/harness/","title":"Harness API","text":""},{"location":"api/harness/#swebench.harness","title":"swebench.harness","text":""},{"location":"api/harness/#swebench.harness.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['docker_build', 'docker_utils', 'grading', 'prepare_images', 'remove_containers', 'reporting', 'run_evaluation', 'utils', 'constants', 'dockerfiles', 'log_parsers', 'modal_eval', 'test_spec']\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants","title":"constants","text":""},{"location":"api/harness/#swebench.harness.constants.SPECS_REDIS","title":"SPECS_REDIS  <code>module-attribute</code>","text":"<pre><code>SPECS_REDIS = {'13115': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/scripting']}, '12472': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/acl --only \"/.*ACL GETUSER.*\"']}, '12272': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/string --only \"/.*(GETRANGE|SETRANGE).*\"']}, '11734': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/bitops']}, '10764': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/zset --only \"BZMPOP\"']}, '10095': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/list --only \"/.*(LPOP|RPOP)\"']}, '9733': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/introspection-2']}, '10068': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/stream --only \"/*XTRIM*\"']}, '11631': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/geo --only \"/.*GEOSEARCH .*\"']}, '11510': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/introspection --only \"/.*MONITOR.*\"']}, '11279': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/acl']}, '13338': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/stream-cgroups']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_JQ","title":"SPECS_JQ  <code>module-attribute</code>","text":"<pre><code>SPECS_JQ = {None: {k: {'build': ['git submodule update --init', 'autoreconf -fi', './configure --with-oniguruma=builtin', 'make clean', 'touch src/parser.y src/lexer.l', 'make -j$(nproc)'], 'test_cmd': ['make check']}for k in ['2839', '2650', '2235', '2658', '2750', '2681', '2919', '2598', '2728']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_JSON","title":"SPECS_JSON  <code>module-attribute</code>","text":"<pre><code>SPECS_JSON = {'4237': {'build': ['mkdir -p build', 'cd build', 'cmake ..', 'make test-udt_cpp11', 'cd ..'], 'test_cmd': ['./build/tests/test-udt_cpp11 -s -r=xml']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_MICROPYTHON","title":"SPECS_MICROPYTHON  <code>module-attribute</code>","text":"<pre><code>SPECS_MICROPYTHON = {'15898': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/ports/unix/ffi_lib.so tests/ports/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i string_format']}, '13569': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/ports/unix/ffi_lib.so tests/ports/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i try']}, '13039': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/unix/ffi_lib.so tests/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i slice']}, '12158': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/unix/ffi_lib.so tests/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -d thread']}, '10095': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate', \"sed -i 's/uint mp_import_stat/mp_import_stat_t mp_import_stat/' mpy-cross/main.c\"], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i basics/fun']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_VALKEY","title":"SPECS_VALKEY  <code>module-attribute</code>","text":"<pre><code>SPECS_VALKEY = {'928': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/cluster/replica-migration --only \"/.*NOREPLICAS.*\"']}, '790': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/cluster/cluster-shards']}, '1499': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/introspection-2']}, '1842': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/acl --only \"/.*ACL LOAD.*\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_FMT","title":"SPECS_FMT  <code>module-attribute</code>","text":"<pre><code>SPECS_FMT = {None: {k: {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target ranges-test'], 'test_cmd': ['ctest --test-dir build -V -R ranges-test']}for k in ['3863', '3158', '2457']}, None: {k: {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target format-test'], 'test_cmd': ['ctest --test-dir build -V -R format-test']}for k in ['3901', '3750', '3248', '2317', '2310']}, '3272': {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target xchar-test'], 'test_cmd': ['ctest --test-dir build -V -R xchar-test']}, '3729': {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target std-test'], 'test_cmd': ['ctest --test-dir build -V -R std-test']}, '1683': {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target printf-test'], 'test_cmd': ['ctest --test-dir build -V -R printf-test']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_C","title":"MAP_REPO_VERSION_TO_SPECS_C  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_C = {'redis/redis': SPECS_REDIS, 'jqlang/jq': SPECS_JQ, 'nlohmann/json': SPECS_JSON, 'micropython/micropython': SPECS_MICROPYTHON, 'valkey-io/valkey': SPECS_VALKEY, 'fmtlib/fmt': SPECS_FMT}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_C","title":"MAP_REPO_TO_INSTALL_C  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_C = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_CADDY","title":"SPECS_CADDY  <code>module-attribute</code>","text":"<pre><code>SPECS_CADDY = {'6411': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go mod tidy'], 'test_cmd': ['go test -v . -run \"TestReplacerNew*\"']}, '6345': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration'], 'test_cmd': ['go test -v ./caddytest/integration']}, '6115': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./modules/caddyhttp/reverseproxy'], 'test_cmd': ['go test -v ./modules/caddyhttp/reverseproxy']}, '6051': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddyconfig/caddyfile'], 'test_cmd': ['go test -v ./caddyconfig/caddyfile']}, '5404': {'docker_specs': {'go_version': '1.20.14'}, 'install': ['go test -c ./caddyconfig/caddyfile'], 'test_cmd': ['go test -v ./caddyconfig/caddyfile']}, '6370': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./cmd'], 'test_cmd': ['go test -v ./cmd']}, '6350': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration -run \"TestCaddyfileAdapt*\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"TestCaddyfileAdapt*\"']}, '6288': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration -run \"TestCaddyfileAdapt*\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"TestCaddyfileAdapt*\"']}, '5995': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration -run \"^TestUriReplace\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"^TestUriReplace\"']}, '4943': {'docker_specs': {'go_version': '1.18.10'}, 'install': ['go test -c ./modules/logging'], 'test_cmd': ['go test -v ./modules/logging']}, '5626': {'docker_specs': {'go_version': '1.19.13'}, 'install': ['go test -c ./caddyconfig/httpcaddyfile -run \"Test.*Import\"'], 'test_cmd': ['go test -v ./caddyconfig/httpcaddyfile -run \"Test.*Import\"']}, '5761': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddyconfig/caddyfile -run \"TestLexer.*\"'], 'test_cmd': ['go test -v ./caddyconfig/caddyfile -run \"TestLexer.*\"']}, '5870': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c . -run \"TestUnsyncedConfigAccess\"'], 'test_cmd': ['go test -v . -run \"TestUnsyncedConfigAccess\"']}, '4774': {'docker_specs': {'go_version': '1.18.10'}, 'install': ['go test -c ./caddytest/integration -run \"TestCaddyfileAdapt*\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"TestCaddyfileAdapt*\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_TERRAFORM","title":"SPECS_TERRAFORM  <code>module-attribute</code>","text":"<pre><code>SPECS_TERRAFORM = {'35611': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./internal/terraform'], 'test_cmd': ['go test -v ./internal/terraform -run \"^TestContext2Apply_provisioner\"']}, '35543': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./internal/terraform'], 'test_cmd': ['go test -v ./internal/terraform -run \"^TestContext2Plan_import\"']}, '34900': {'docker_specs': {'go_version': '1.22.12'}, 'install': ['go test -c ./internal/terraform'], 'test_cmd': ['go test -v ./internal/terraform -run \"(^TestContext2Apply|^TestContext2Plan).*[Ss]ensitive\"']}, '34580': {'docker_specs': {'go_version': '1.21.13'}, 'install': ['go test -c ./internal/command'], 'test_cmd': ['go test -v ./internal/command -run \"^TestFmt\"']}, '34814': {'docker_specs': {'go_version': '1.22.12'}, 'install': ['go test -c ./internal/builtin/provisioners/remote-exec'], 'test_cmd': ['go test -v ./internal/builtin/provisioners/remote-exec']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PROMETHEUS","title":"SPECS_PROMETHEUS  <code>module-attribute</code>","text":"<pre><code>SPECS_PROMETHEUS = {'14861': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql'], 'test_cmd': ['go test -v ./promql -run \"^TestEngine\"']}, '13845': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql ./model/labels'], 'test_cmd': ['go test -v ./promql ./model/labels -run \"^(TestRangeQuery|TestLabels)\"']}, '12874': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tsdb'], 'test_cmd': ['go test -v ./tsdb -run \"^TestHead\"']}, '11859': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tsdb'], 'test_cmd': ['go test -v ./tsdb -run \"^TestSnapshot\"']}, '10720': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql'], 'test_cmd': ['go test -v ./promql -run \"^TestEvaluations\"']}, '10633': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./discovery/puppetdb'], 'test_cmd': ['go test -v ./discovery/puppetdb -run \"TestPuppetDBRefreshWithParameters\"']}, '9248': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql'], 'test_cmd': ['go test -v ./promql -run \"^TestEvaluations\"']}, '15142': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tsdb'], 'test_cmd': ['go test -v ./tsdb -run \"^TestHead\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_HUGO","title":"SPECS_HUGO  <code>module-attribute</code>","text":"<pre><code>SPECS_HUGO = {'12768': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./markup/goldmark/blockquotes/...'], 'test_cmd': ['go test -v ./markup/goldmark/blockquotes/...']}, '12579': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./resources/page'], 'test_cmd': ['go test -v ./resources/page -run \"^TestGroupBy\"']}, '12562': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./hugolib/...'], 'test_cmd': ['go test -v ./hugolib/... -run \"^TestGetPage[^/]\"']}, '12448': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./hugolib/...'], 'test_cmd': ['go test -v ./hugolib/... -run \"^TestRebuild\"']}, '12343': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./resources/page/...'], 'test_cmd': ['go test -v ./resources/page/... -run \"^Test.*Permalink\"']}, '12204': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tpl/tplimpl'], 'test_cmd': ['go test -v ./tpl/tplimpl -run \"^TestEmbedded\"']}, '12171': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./hugolib'], 'test_cmd': ['go test -v ./hugolib -run \"^Test.*Pages\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_GIN","title":"SPECS_GIN  <code>module-attribute</code>","text":"<pre><code>SPECS_GIN = {'4003': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test . -v -run \"TestMethodNotAllowedNoRoute\"']}, '3820': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./binding'], 'test_cmd': ['go test -v ./binding -run \"^TestMapping\"']}, '3741': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestColor\"']}, '2755': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestTree\"']}, '3227': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestRedirect\"']}, '2121': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./...'], 'test_cmd': ['go test -v ./... -run \"^Test.*Reader\"']}, '1957': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestContext.*Bind\"']}, '1805': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^Test.*Router\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_GO","title":"MAP_REPO_VERSION_TO_SPECS_GO  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_GO = {'caddyserver/caddy': SPECS_CADDY, 'hashicorp/terraform': SPECS_TERRAFORM, 'prometheus/prometheus': SPECS_PROMETHEUS, 'gohugoio/hugo': SPECS_HUGO, 'gin-gonic/gin': SPECS_GIN}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_GO","title":"MAP_REPO_TO_INSTALL_GO  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_GO = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_GSON","title":"SPECS_GSON  <code>module-attribute</code>","text":"<pre><code>SPECS_GSON = {'2158': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testByteSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testShortSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testIntSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testLongSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testFloatSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testDoubleSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testPrimitiveIntegerAutoboxedSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testPrimitiveIntegerAutoboxedInASingleElementArraySerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testReallyLongValuesSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testPrimitiveLongAutoboxedSerialization']}, '2024': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.FieldNamingTest#testUpperCaseWithUnderscores', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.NamingPolicyTest#testGsonWithUpperCaseUnderscorePolicySerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.NamingPolicyTest#testGsonWithUpperCaseUnderscorePolicyDeserialiation']}, '2479': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.GsonBuilderTest#testRegisterTypeAdapterForObjectAndJsonElements', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.GsonBuilderTest#testRegisterTypeHierarchyAdapterJsonElements', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.GsonBuilderTest#testModificationAfterCreate']}, '2134': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.util.ISO8601UtilsTest#testDateParseInvalidDay', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.util.ISO8601UtilsTest#testDateParseInvalidMonth', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.util.ISO8601UtilsTest#testDateParseWithDefaultTimezone']}, '2061': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonReaderTest#testHasNextEndOfDocument', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testHasNext_endOfDocument', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonReaderTest#testReadEmptyObject', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonReaderTest#testReadEmptyArray', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_emptyJsonObject', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_filledJsonObject']}, '2311': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.JsonPrimitiveTest#testEqualsIntegerAndBigInteger', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.JsonPrimitiveTest#testLongEqualsBigInteger', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.JsonPrimitiveTest#testEqualsAcrossTypes']}, '1100': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.DefaultDateTypeAdapterTest#testNullValue', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.DefaultDateTypeAdapterTest#testDatePattern', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.DefaultDateTypeAdapterTest#testInvalidDatePattern']}, '1093': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteDoublesWhenLenient', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteBoxedDoublesWhenLenient', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteDoubles', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteBoxedDoubles', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testDoubles']}, '1014': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_emptyJsonObject', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_filledJsonObject']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_DRUID","title":"SPECS_DRUID  <code>module-attribute</code>","text":"<pre><code>SPECS_DRUID = {'15402': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl processing -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.query.groupby.GroupByQueryQueryToolChestTest#testCacheStrategy', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.query.groupby.GroupByQueryQueryToolChestTest#testResultLevelCacheKeyWithSubTotalsSpec', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.query.groupby.GroupByQueryQueryToolChestTest#testMultiColumnCacheStrategy']}, '14092': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl processing,cloud/aws-common,cloud/gcp-common -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl server -Dtest=org.apache.druid.discovery.DruidLeaderClientTest#test503ResponseFromServerAndCacheRefresh', 'mvnd test -B -T 1C -pl server -Dtest=org.apache.druid.discovery.DruidLeaderClientTest#testServerFailureAndRedirect']}, '14136': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl processing -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval2', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval3', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval4', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapFirstContainsSecond', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirst']}, '13704': {'docker_specs': {'java_version': '11'}, 'install': [\"sed -i 's/&lt;resourceBundle&gt;org.apache.apache.resources:apache-jar-resource-bundle:1.5-SNAPSHOT&lt;\\\\/resourceBundle&gt;/&lt;resourceBundle&gt;org.apache.apache.resources:apache-jar-resource-bundle:1.5&lt;\\\\/resourceBundle&gt;/' pom.xml\", 'mvn clean install -B -pl processing -DskipTests -am'], 'test_cmd': ['mvnd test -B -pl processing -Dtest=org.apache.druid.query.aggregation.post.ArithmeticPostAggregatorTest#testPow', 'mvnd test -B -pl processing -Dtest=org.apache.druid.query.aggregation.post.ArithmeticPostAggregatorTest#testDiv', 'mvnd test -B -pl processing -Dtest=org.apache.druid.query.aggregation.post.ArithmeticPostAggregatorTest#testQuotient']}, '16875': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl server -DskipTests -am'], 'test_cmd': ['mvnd test -B -pl server -Dtest=org.apache.druid.server.metrics.WorkerTaskCountStatsMonitorTest#testMonitorWithPeon', 'mvnd test -B -pl server -Dtest=org.apache.druid.server.metrics.WorkerTaskCountStatsMonitorTest#testMonitorWithNulls', 'mvnd test -B -pl server -Dtest=org.apache.druid.server.metrics.WorkerTaskCountStatsMonitorTest#testMonitorIndexer']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_JAVAPARSER","title":"SPECS_JAVAPARSER  <code>module-attribute</code>","text":"<pre><code>SPECS_JAVAPARSER = {'4561': {'docker_specs': {'java_version': '17'}, 'build': ['./mvnw clean install -B -pl javaparser-symbol-solver-testing -DskipTests -am'], 'test_cmd': ['./mvnw test -B -pl javaparser-symbol-solver-testing -Dtest=Issue4560Test', './mvnw test -B -pl javaparser-symbol-solver-testing -Dtest=JavaSymbolSolverTest']}, '4538': {'docker_specs': {'java_version': '17'}, 'build': ['./mvnw clean install -B -pl javaparser-core-testing -DskipTests -am'], 'test_cmd': ['./mvnw test -B -pl javaparser-core-testing -Dtest=NodeTest', './mvnw test -B -pl javaparser-core-testing -Dtest=NodePositionTest']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_LOMBOK","title":"SPECS_LOMBOK  <code>module-attribute</code>","text":"<pre><code>SPECS_LOMBOK = {'3602': {'docker_specs': {'java_version': '11'}, 'pre_install': make_lombok_pre_install_script(['lombok.bytecode.TestPostCompiler']), 'build': ['ant test.compile'], 'test_cmd': ['ant test.instance']}, None: {k: {'docker_specs': {'java_version': '11'}, 'pre_install': make_lombok_pre_install_script(['lombok.transform.TestWithDelombok']), 'build': ['ant test.compile'], 'test_cmd': ['ant test.instance']}for k in ['3312', '3697', '3326', '3674', '3594', '3422', '3215', '3486', '3042', '3052', '2792']}, None: {k: {'docker_specs': {'java_version': '17'}, 'pre_install': make_lombok_pre_install_script(['lombok.transform.TestWithDelombok']), 'build': ['ant test.compile'], 'test_cmd': ['ant test.instance']}for k in ['3571', '3479', '3371', '3350', '3009']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_LUCENE","title":"SPECS_LUCENE  <code>module-attribute</code>","text":"<pre><code>SPECS_LUCENE = {'13494': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.facet.TestStringValueFacetCounts']}, '13704': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.search.TestLatLonDocValuesQueries']}, '13301': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests TestXYPoint.testEqualsAndHashCode -Dtests.seed=3ABEFE4D876DD310 -Dtests.nightly=true -Dtests.locale=es-419 -Dtests.timezone=Asia/Ulaanbaatar -Dtests.asserts=true -Dtests.file.encoding=UTF-8']}, '12626': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.index.TestIndexWriter']}, '12212': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.facet.TestDrillSideways']}, '13170': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.analysis.opennlp.TestOpenNLPSentenceBreakIterator']}, '12196': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.queryparser.classic.TestMultiFieldQueryParser']}, '12022': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.document.TestLatLonShape']}, '11760': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.queries.intervals.TestIntervalBuilder']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_RXJAVA","title":"SPECS_RXJAVA  <code>module-attribute</code>","text":"<pre><code>SPECS_RXJAVA = {'7597': {'docker_specs': {'java_version': '11'}, 'pre_install': make_rxjava_pre_install_script(), 'test_cmd': ['./gradlew test --tests io.reactivex.rxjava3.internal.operators.observable.ObservableSwitchTest']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_JAVA","title":"MAP_REPO_VERSION_TO_SPECS_JAVA  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_JAVA = {'google/gson': SPECS_GSON, 'apache/druid': SPECS_DRUID, 'javaparser/javaparser': SPECS_JAVAPARSER, 'projectlombok/lombok': SPECS_LOMBOK, 'apache/lucene': SPECS_LUCENE, 'reactivex/rxjava': SPECS_RXJAVA}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_JAVA","title":"MAP_REPO_TO_INSTALL_JAVA  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_JAVA = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_XVFB_PREFIX","title":"TEST_XVFB_PREFIX  <code>module-attribute</code>","text":"<pre><code>TEST_XVFB_PREFIX = 'xvfb-run --server-args=\"-screen 0 1280x1024x24 -ac :99\"'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.XVFB_DEPS","title":"XVFB_DEPS  <code>module-attribute</code>","text":"<pre><code>XVFB_DEPS = ['python3', 'python3-pip', 'xvfb', 'x11-xkb-utils', 'xfonts-100dpi', 'xfonts-75dpi', 'xfonts-scalable', 'xfonts-cyrillic', 'x11-apps', 'firefox']\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.X11_DEPS","title":"X11_DEPS  <code>module-attribute</code>","text":"<pre><code>X11_DEPS = ['libx11-xcb1', 'libxcomposite1', 'libxcursor1', 'libxdamage1', 'libxi6', 'libxtst6', 'libnss3', 'libcups2', 'libxss1', 'libxrandr2', 'libasound2', 'libatk1.0-0', 'libgtk-3-0', 'x11-utils']\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_CALYPSO","title":"SPECS_CALYPSO  <code>module-attribute</code>","text":"<pre><code>SPECS_CALYPSO = {None: {k: {'apt-pkgs': ['libsass-dev', 'sassc'], 'install': ['npm install --unsafe-perm'], 'test_cmd': 'npm run test-client', 'docker_specs': {'node_version': k}}for k in ['0.8', '4.2.3', '4.3.0', '5.10.1', '5.11.1', '6.1.0', '6.7.0', '6.9.0', '6.9.1', '6.9.4', '6.10.0', '6.10.2', '6.10.3', '6.11.1', '6.11.2', '6.11.5', '8.9.1', '8.9.3', '8.9.4', '8.11.0', '8.11.2', '10.4.1', '10.5.0', '10.6.0', '10.9.0', '10.10.0', '10.12.0', '10.13.0', '10.14.0', '10.15.2', '10.16.3']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_CHART_JS_TEMPLATE","title":"TEST_CHART_JS_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>TEST_CHART_JS_TEMPLATE = './node_modules/.bin/cross-env NODE_ENV=test ./node_modules/.bin/karma start {} --single-run --coverage --grep --auto-watch false'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_CHART_JS","title":"SPECS_CHART_JS  <code>module-attribute</code>","text":"<pre><code>SPECS_CHART_JS = {None: {k: {'install': ['pnpm install', 'pnpm run build'], 'test_cmd': ['pnpm install', 'pnpm run build', f'{TEST_XVFB_PREFIX} su chromeuser -c \"{format('./karma.conf.cjs')}\"'], 'docker_specs': {'node_version': '21.6.2', 'pnpm_version': '7.9.0', 'run_args': {'cap_add': ['SYS_ADMIN']}}}for k in ['4.0', '4.1', '4.2', '4.3', '4.4']}, None: {k: {'install': ['npm install'], 'test_cmd': ['npm install', 'npm run build', f'{TEST_XVFB_PREFIX} su chromeuser -c \"{format('./karma.conf.js')}\"'], 'docker_specs': {'node_version': '21.6.2', 'run_args': {'cap_add': ['SYS_ADMIN']}}}for k in ['3.0', '3.1', '3.2', '3.3', '3.4', '3.5', '3.6', '3.7', '3.8']}, None: {k: {'install': ['npm install', 'npm install -g gulp-cli'], 'test_cmd': ['npm install', 'gulp build', TEST_XVFB_PREFIX + ' su chromeuser -c \"gulp test\"'], 'docker_specs': {'node_version': '21.6.2', 'run_args': {'cap_add': ['SYS_ADMIN']}}}for k in ['2.0', '2.1', '2.2', '2.3', '2.4', '2.5', '2.6', '2.7', '2.8', '2.9']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_MARKED","title":"SPECS_MARKED  <code>module-attribute</code>","text":"<pre><code>SPECS_MARKED = {None: {k: {'install': ['npm install'], 'test_cmd': './node_modules/.bin/jasmine --no-color --config=jasmine.json', 'docker_specs': {'node_version': '12.22.12'}}for k in ['0.3', '0.5', '0.6', '0.7', '1.0', '1.1', '1.2', '2.0', '3.9', '4.0', '4.1', '5.0']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_P5_JS","title":"SPECS_P5_JS  <code>module-attribute</code>","text":"<pre><code>SPECS_P5_JS = {None: {k: {'apt-pkgs': X11_DEPS, 'install': ['npm install', \"PUPPETEER_SKIP_CHROMIUM_DOWNLOAD='' node node_modules/puppeteer/install.js\", './node_modules/.bin/grunt yui'], 'test_cmd': \"sed -i 's/concurrency:[[:space:]]*[0-9][0-9]*/concurrency: 1/g' Gruntfile.js\\nstdbuf -o 1M ./node_modules/.bin/grunt test --quiet --force\", 'docker_specs': {'node_version': '14.17.3'}}for k in ['0.10', '0.2', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', '1.9']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_REACT_PDF","title":"SPECS_REACT_PDF  <code>module-attribute</code>","text":"<pre><code>SPECS_REACT_PDF = {None: {k: {'apt-pkgs': ['pkg-config', 'build-essential', 'libpixman-1-0', 'libpixman-1-dev', 'libcairo2-dev', 'libpango1.0-dev', 'libjpeg-dev', 'libgif-dev', 'librsvg2-dev'] + X11_DEPS, 'install': ['npm i -g yarn', 'yarn install'], 'test_cmd': 'NODE_OPTIONS=\"--experimental-vm-modules\" ./node_modules/.bin/jest --no-color', 'docker_specs': {'node_version': '18.20.4'}}for k in ['1.0', '1.1', '1.2', '2.0']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.JEST_JSON_JQ_TRANSFORM","title":"JEST_JSON_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>JEST_JSON_JQ_TRANSFORM = 'jq -r \\'.testResults[].assertionResults[] | \"[\" + (.status | ascii_upcase) + \"] \" + ((.ancestorTitles | join(\" &gt; \")) + (if .ancestorTitles | length &gt; 0 then \" &gt; \" else \"\" end) + .title)\\''\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_BABEL","title":"SPECS_BABEL  <code>module-attribute</code>","text":"<pre><code>SPECS_BABEL = {'14532': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest babel-generator --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '13928': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest babel-parser -t \"arrow\" --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '15649': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest packages/babel-traverse/test/scope.js --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '15445': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest packages/babel-generator/test/index.js -t \"generation \" --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '16130': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest babel-helpers --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_VUEJS","title":"SPECS_VUEJS  <code>module-attribute</code>","text":"<pre><code>SPECS_VUEJS = {'11899': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/compiler-sfc/__tests__/compileStyle.spec.ts --no-watch --reporter=verbose'], 'install': ['pnpm i'], 'build': ['pnpm run build compiler-sfc']}, '11870': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/runtime-core/__tests__/helpers/renderList.spec.ts --no-watch --reporter=verbose'], 'install': ['pnpm i']}, '11739': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/runtime-core/__tests__/hydration.spec.ts --no-watch --reporter=verbose -t \"mismatch handling\"'], 'install': ['pnpm i']}, '11915': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/compiler-core/__tests__/parse.spec.ts --no-watch --reporter=verbose -t \"Element\"'], 'install': ['pnpm i']}, '11589': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/runtime-core/__tests__/apiWatch.spec.ts --no-watch --reporter=verbose'], 'install': ['pnpm i']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_DOCUSAURUS","title":"SPECS_DOCUSAURUS  <code>module-attribute</code>","text":"<pre><code>SPECS_DOCUSAURUS = {'10309': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-plugin-content-docs/src/client/__tests__/docsClientUtils.test.ts --verbose']}, '10130': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus/src/server/__tests__/brokenLinks.test.ts --verbose']}, '9897': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-utils/src/__tests__/markdownUtils.test.ts --verbose']}, '9183': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-theme-classic/src/__tests__/options.test.ts --verbose']}, '8927': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-utils/src/__tests__/markdownLinks.test.ts --verbose']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_IMMUTABLEJS","title":"SPECS_IMMUTABLEJS  <code>module-attribute</code>","text":"<pre><code>SPECS_IMMUTABLEJS = {'2006': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'build': ['npm run build'], 'test_cmd': ['npx jest __tests__/Range.ts --verbose']}, '2005': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'build': ['npm run build'], 'test_cmd': [f'npx jest __tests__/OrderedMap.ts __tests__/OrderedSet.ts --silent --json | {JEST_JSON_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_THREEJS","title":"SPECS_THREEJS  <code>module-attribute</code>","text":"<pre><code>SPECS_THREEJS = {'27395': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install --ignore-scripts'], 'test_cmd': ['npx qunit test/unit/src/math/Sphere.tests.js']}, '26589': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install --ignore-scripts'], 'test_cmd': ['npx qunit test/unit/src/objects/Line.tests.js test/unit/src/objects/Mesh.tests.js test/unit/src/objects/Points.tests.js']}, '25687': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install --ignore-scripts'], 'test_cmd': ['npx qunit test/unit/src/core/Object3D.tests.js -f \"/json|clone|copy/i\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PREACT","title":"SPECS_PREACT  <code>module-attribute</code>","text":"<pre><code>SPECS_PREACT = {'4152': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/components.test.js\"']}, '4316': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/events.test.js\"']}, '4245': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useId.test.js\"']}, '4182': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/errorBoundary.test.js\"']}, '4436': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/refs.test.js\"']}, '3763': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/lifecycles/componentDidMount.test.js\"']}, '3739': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useState.test.js\"']}, '3689': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/errorBoundary.test.js\"']}, '3567': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useEffect.test.js\"']}, '3562': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"compat/test/browser/render.test.js\"']}, '3454': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/svg.test.js\"']}, '3345': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useEffect.test.js\"']}, '3062': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}, '3010': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}, '2927': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}, '2896': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"compat/test/browser/memo.test.js\"']}, '2757': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_AXIOS","title":"SPECS_AXIOS  <code>module-attribute</code>","text":"<pre><code>SPECS_AXIOS = {'5892': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': [\"npx mocha test/unit/adapters/http.js -R tap -g 'compression'\"]}, '5316': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'build': ['npm install'], 'test_cmd': [\"npx mocha test/unit/adapters/http.js -R tap -g 'FormData'\"]}, '4738': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': [\"timeout 10s npx mocha -R tap test/unit/adapters/http.js -g 'timeout'\"]}, '4731': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': [\"npx mocha -R tap test/unit/adapters/http.js -g 'body length'\"]}, '6539': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['npx mocha -R tap test/unit/regression/SNYK-JS-AXIOS-7361793.js']}, '5085': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['npx mocha -R tap test/unit/regression/bugs.js']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_JS","title":"MAP_REPO_VERSION_TO_SPECS_JS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_JS = {'Automattic/wp-calypso': SPECS_CALYPSO, 'chartjs/Chart.js': SPECS_CHART_JS, 'markedjs/marked': SPECS_MARKED, 'processing/p5.js': SPECS_P5_JS, 'diegomura/react-pdf': SPECS_REACT_PDF, 'babel/babel': SPECS_BABEL, 'vuejs/core': SPECS_VUEJS, 'facebook/docusaurus': SPECS_DOCUSAURUS, 'immutable-js/immutable-js': SPECS_IMMUTABLEJS, 'mrdoob/three.js': SPECS_THREEJS, 'preactjs/preact': SPECS_PREACT, 'axios/axios': SPECS_AXIOS}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_JS","title":"MAP_REPO_TO_INSTALL_JS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_JS = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PHPSPREADSHEET","title":"SPECS_PHPSPREADSHEET  <code>module-attribute</code>","text":"<pre><code>SPECS_PHPSPREADSHEET = {'4313': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Reader/Ods/FormulaTranslatorTest.php']}, '4214': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Calculation/Functions/MathTrig/RoundDownTest.php']}, '4186': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Writer/Xlsx/FunctionPrefixTest.php']}, '4114': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Worksheet/Issue4112Test.php']}, '3940': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Worksheet/WorksheetTest.php']}, '3903': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Shared/StringHelperTest.php']}, '3570': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Calculation/Functions/LookupRef/VLookupTest.php']}, '3463': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Writer/Xlsx/FunctionPrefixTest.php']}, '3469': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Style/StyleTest.php']}, '3659': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Worksheet/Table/Issue3635Test.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_LARAVEL_FRAMEWORK","title":"SPECS_LARAVEL_FRAMEWORK  <code>module-attribute</code>","text":"<pre><code>SPECS_LARAVEL_FRAMEWORK = {'53914': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Integration/Database/DatabaseConnectionsTest.php']}, '53206': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Support/SupportJsTest.php']}, '52866': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Container/ContextualAttributeBindingTest.php']}, '52684': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Support/SupportStrTest.php']}, '52680': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Database/DatabaseEloquentInverseRelationTest.php']}, '52451': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': [\"vendor/bin/phpunit --testdox --colors=never tests/Validation/ValidationValidatorTest.php --filter 'custom'\"]}, '53949': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Support/OnceTest.php']}, '51890': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': [\"vendor/bin/phpunit --testdox --colors=never tests/Validation/ValidationValidatorTest.php --filter 'attribute'\"]}, '51195': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/View/Blade/BladeVerbatimTest.php']}, '48636': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Database/DatabaseEloquentModelTest.php']}, '48573': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Cache/CacheArrayStoreTest.php']}, '46234': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Routing/RoutingUrlGeneratorTest.php']}, '53696': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Database/DatabaseSchemaBlueprintTest.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PHP_CS_FIXER","title":"SPECS_PHP_CS_FIXER  <code>module-attribute</code>","text":"<pre><code>SPECS_PHP_CS_FIXER = {'8367': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Import/FullyQualifiedStrictTypesFixerTest.php']}, '8331': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/LanguageConstruct/NullableTypeDeclarationFixerTest.php']}, '8075': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/PhpUnit/PhpUnitAttributesFixerTest.php']}, '8064': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/StringNotation/SimpleToComplexStringVariableFixerTest.php']}, '7998': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Casing/ConstantCaseFixerTest.php']}, '7875': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Whitespace/StatementIndentationFixerTest.php']}, '7635': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Import/FullyQualifiedStrictTypesFixerTest.php']}, '7523': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Operator/BinaryOperatorSpacesFixerTest.php']}, '8256': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/PhpTag/BlankLineAfterOpeningTagFixerTest.php']}, '7663': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Whitespace/StatementIndentationFixerTest.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_CARBON","title":"SPECS_CARBON  <code>module-attribute</code>","text":"<pre><code>SPECS_CARBON = {'3103': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonImmutable/SettersTest.php']}, '3098': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/ConstructTest.php']}, '3073': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/TotalTest.php']}, '3041': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonPeriod/CreateTest.php']}, '3005': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/ConstructTest.php']}, '2981': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/TotalTest.php']}, '2813': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'build': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Factory/FactoryTest.php']}, '2752': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonImmutable/IsTest.php']}, '2665': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Carbon/RoundTest.php']}, '2762': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/RoundingTest.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_PHP","title":"MAP_REPO_VERSION_TO_SPECS_PHP  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_PHP = {'phpoffice/phpspreadsheet': SPECS_PHPSPREADSHEET, 'laravel/framework': SPECS_LARAVEL_FRAMEWORK, 'php-cs-fixer/php-cs-fixer': SPECS_PHP_CS_FIXER, 'briannesbitt/carbon': SPECS_CARBON}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_PHP","title":"MAP_REPO_TO_INSTALL_PHP  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_PHP = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_PYTEST","title":"TEST_PYTEST  <code>module-attribute</code>","text":"<pre><code>TEST_PYTEST = 'pytest -rA'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_PYTEST_VERBOSE","title":"TEST_PYTEST_VERBOSE  <code>module-attribute</code>","text":"<pre><code>TEST_PYTEST_VERBOSE = 'pytest -rA --tb=long'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_ASTROPY_PYTEST","title":"TEST_ASTROPY_PYTEST  <code>module-attribute</code>","text":"<pre><code>TEST_ASTROPY_PYTEST = 'pytest -rA -vv -o console_output_style=classic --tb=no'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_DJANGO","title":"TEST_DJANGO  <code>module-attribute</code>","text":"<pre><code>TEST_DJANGO = './tests/runtests.py --verbosity 2 --settings=test_sqlite --parallel 1'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_DJANGO_NO_PARALLEL","title":"TEST_DJANGO_NO_PARALLEL  <code>module-attribute</code>","text":"<pre><code>TEST_DJANGO_NO_PARALLEL = './tests/runtests.py --verbosity 2'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_SEABORN","title":"TEST_SEABORN  <code>module-attribute</code>","text":"<pre><code>TEST_SEABORN = 'pytest --no-header -rA'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_SEABORN_VERBOSE","title":"TEST_SEABORN_VERBOSE  <code>module-attribute</code>","text":"<pre><code>TEST_SEABORN_VERBOSE = 'pytest -rA --tb=long'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_SPHINX","title":"TEST_SPHINX  <code>module-attribute</code>","text":"<pre><code>TEST_SPHINX = 'tox --current-env -epy39 -v --'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_SYMPY","title":"TEST_SYMPY  <code>module-attribute</code>","text":"<pre><code>TEST_SYMPY = \"PYTHONWARNINGS='ignore::UserWarning,ignore::SyntaxWarning' bin/test -C --verbose\"\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TEST_SYMPY_VERBOSE","title":"TEST_SYMPY_VERBOSE  <code>module-attribute</code>","text":"<pre><code>TEST_SYMPY_VERBOSE = 'bin/test -C --verbose'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_SKLEARN","title":"SPECS_SKLEARN  <code>module-attribute</code>","text":"<pre><code>SPECS_SKLEARN = {k: {'python': '3.6', 'packages': 'numpy scipy cython pytest pandas matplotlib', 'install': 'python -m pip install -v --no-use-pep517 --no-build-isolation -e .', 'pip_packages': ['cython', 'numpy==1.19.2', 'setuptools', 'scipy==1.5.2'], 'test_cmd': TEST_PYTEST}for k in ['0.20', '0.21', '0.22']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_FLASK","title":"SPECS_FLASK  <code>module-attribute</code>","text":"<pre><code>SPECS_FLASK = {'2.0': {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'pip_packages': ['setuptools==70.0.0', 'Werkzeug==2.3.7', 'Jinja2==3.0.1', 'itsdangerous==2.1.2', 'click==8.0.1', 'MarkupSafe==2.1.3'], 'test_cmd': TEST_PYTEST}, '2.1': {'python': '3.10', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'pip_packages': ['setuptools==70.0.0', 'click==8.1.3', 'itsdangerous==2.1.2', 'Jinja2==3.1.2', 'MarkupSafe==2.1.1', 'Werkzeug==2.3.7'], 'test_cmd': TEST_PYTEST}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_DJANGO","title":"SPECS_DJANGO  <code>module-attribute</code>","text":"<pre><code>SPECS_DJANGO = {k: {'python': '3.5', 'packages': 'requirements.txt', 'pre_install': ['apt-get update &amp;&amp; apt-get install -y locales', \"echo 'en_US UTF-8' &gt; /etc/locale.gen\", 'locale-gen en_US.UTF-8'], 'install': 'python setup.py install', 'pip_packages': ['setuptools'], 'eval_commands': ['export LANG=en_US.UTF-8', 'export LC_ALL=en_US.UTF-8', 'export PYTHONIOENCODING=utf8', 'export LANGUAGE=en_US:en'], 'test_cmd': TEST_DJANGO}for k in ['1.7', '1.8', '1.9', '1.10', '1.11', '2.0', '2.1', '2.2']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_REQUESTS","title":"SPECS_REQUESTS  <code>module-attribute</code>","text":"<pre><code>SPECS_REQUESTS = {k: {'python': '3.9', 'packages': 'pytest', 'install': 'python -m pip install .', 'test_cmd': TEST_PYTEST}for k in (['0.7', '0.8', '0.9', '0.11', '0.13', '0.14', '1.1', '1.2', '2.0', '2.2'] + ['2.3', '2.4', '2.5', '2.7', '2.8', '2.9', '2.10', '2.11', '2.12', '2.17'] + ['2.18', '2.19', '2.22', '2.26', '2.25', '2.27', '2.31', '3.0'])}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_SEABORN","title":"SPECS_SEABORN  <code>module-attribute</code>","text":"<pre><code>SPECS_SEABORN = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'pip_packages': ['contourpy==1.1.0', 'cycler==0.11.0', 'fonttools==4.42.1', 'importlib-resources==6.0.1', 'kiwisolver==1.4.5', 'matplotlib==3.7.2', 'numpy==1.25.2', 'packaging==23.1', 'pandas==1.3.5', 'pillow==10.0.0', 'pyparsing==3.0.9', 'pytest', 'python-dateutil==2.8.2', 'pytz==2023.3.post1', 'scipy==1.11.2', 'six==1.16.0', 'tzdata==2023.1', 'zipp==3.16.2'], 'test_cmd': TEST_SEABORN}for k in ['0.11']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PYTEST","title":"SPECS_PYTEST  <code>module-attribute</code>","text":"<pre><code>SPECS_PYTEST = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'test_cmd': TEST_PYTEST}for k in ['4.4', '4.5', '4.6', '5.0', '5.1', '5.2', '5.3', '5.4', '6.0', '6.2', '6.3', '7.0', '7.1', '7.2', '7.4', '8.0', '8.1', '8.2', '8.3', '8.4']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_MATPLOTLIB","title":"SPECS_MATPLOTLIB  <code>module-attribute</code>","text":"<pre><code>SPECS_MATPLOTLIB = {k: {'python': '3.11', 'packages': 'environment.yml', 'install': 'python -m pip install -e .', 'pre_install': ['apt-get -y update &amp;&amp; apt-get -y upgrade &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y imagemagick ffmpeg texlive texlive-latex-extra texlive-fonts-recommended texlive-xetex texlive-luatex cm-super dvipng', 'QHULL_URL=\"http://www.qhull.org/download/qhull-2020-src-8.0.2.tgz\"', 'QHULL_TAR=\"/tmp/qhull-2020-src-8.0.2.tgz\"', 'QHULL_BUILD_DIR=\"/testbed/build\"', 'wget -O \"$QHULL_TAR\" \"$QHULL_URL\"', 'mkdir -p \"$QHULL_BUILD_DIR\"', 'tar -xvzf \"$QHULL_TAR\" -C \"$QHULL_BUILD_DIR\"'], 'pip_packages': ['contourpy==1.1.0', 'cycler==0.11.0', 'fonttools==4.42.1', 'ghostscript', 'kiwisolver==1.4.5', 'numpy==1.25.2', 'packaging==23.1', 'pillow==10.0.0', 'pikepdf', 'pyparsing==3.0.9', 'python-dateutil==2.8.2', 'six==1.16.0', 'setuptools==68.1.2', 'setuptools-scm==7.1.0', 'typing-extensions==4.7.1'], 'test_cmd': TEST_PYTEST}for k in ['3.5', '3.6', '3.7', '3.8', '3.9']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_SPHINX","title":"SPECS_SPHINX  <code>module-attribute</code>","text":"<pre><code>SPECS_SPHINX = {k: {'python': '3.9', 'pip_packages': ['tox==4.16.0', 'tox-current-env==0.0.11', 'Jinja2==3.0.3'], 'install': 'python -m pip install -e .[test]', 'pre_install': [\"sed -i 's/pytest/pytest -rA/' tox.ini\"], 'test_cmd': TEST_SPHINX}for k in (['1.5', '1.6', '1.7', '1.8', '2.0', '2.1', '2.2', '2.3', '2.4', '3.0'] + ['3.1', '3.2', '3.3', '3.4', '3.5', '4.0', '4.1', '4.2', '4.3', '4.4'] + ['4.5', '5.0', '5.1', '5.2', '5.3', '6.0', '6.2', '7.0', '7.1', '7.2'] + ['7.3', '7.4', '8.0', '8.1'])}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_ASTROPY","title":"SPECS_ASTROPY  <code>module-attribute</code>","text":"<pre><code>SPECS_ASTROPY = {k: {'python': '3.9', 'install': 'python -m pip install -e .[test] --verbose', 'pip_packages': ['attrs==23.1.0', 'exceptiongroup==1.1.3', 'execnet==2.0.2', 'hypothesis==6.82.6', 'iniconfig==2.0.0', 'numpy==1.25.2', 'packaging==23.1', 'pluggy==1.3.0', 'psutil==5.9.5', 'pyerfa==2.0.0.3', 'pytest-arraydiff==0.5.0', 'pytest-astropy-header==0.2.2', 'pytest-astropy==0.10.0', 'pytest-cov==4.1.0', 'pytest-doctestplus==1.0.0', 'pytest-filter-subpackage==0.1.2', 'pytest-mock==3.11.1', 'pytest-openfiles==0.5.0', 'pytest-remotedata==0.4.0', 'pytest-xdist==3.3.1', 'pytest==7.4.0', 'PyYAML==6.0.1', 'setuptools==68.0.0', 'sortedcontainers==2.4.0', 'tomli==2.0.1'], 'test_cmd': TEST_PYTEST}for k in ['3.0', '3.1', '3.2', '4.1', '4.2', '4.3', '5.0', '5.1', '5.2', 'v5.3']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_SYMPY","title":"SPECS_SYMPY  <code>module-attribute</code>","text":"<pre><code>SPECS_SYMPY = {k: {'python': '3.9', 'packages': 'mpmath flake8', 'pip_packages': ['mpmath==1.3.0', 'flake8-comprehensions'], 'install': 'python -m pip install -e .', 'test_cmd': TEST_SYMPY}for k in (['0.7', '1.0', '1.1', '1.10', '1.11', '1.12', '1.2', '1.4', '1.5', '1.6'] + ['1.7', '1.8', '1.9'] + ['1.10', '1.11', '1.12', '1.13', '1.14'])}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PYLINT","title":"SPECS_PYLINT  <code>module-attribute</code>","text":"<pre><code>SPECS_PYLINT = {k: {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'test_cmd': TEST_PYTEST}for k in ['2.10', '2.11', '2.13', '2.14', '2.15', '2.16', '2.17', '2.8', '2.9', '3.0', '3.1', '3.2', '3.3', '4.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_XARRAY","title":"SPECS_XARRAY  <code>module-attribute</code>","text":"<pre><code>SPECS_XARRAY = {k: {'python': '3.10', 'packages': 'environment.yml', 'install': 'python -m pip install -e .', 'pip_packages': ['numpy==1.23.0', 'packaging==23.1', 'pandas==1.5.3', 'pytest==7.4.0', 'python-dateutil==2.8.2', 'pytz==2023.3', 'six==1.16.0', 'scipy==1.11.1', 'setuptools==68.0.0', 'dask==2022.8.1'], 'no_use_env': True, 'test_cmd': TEST_PYTEST}for k in ['0.12', '0.18', '0.19', '0.20', '2022.03', '2022.06', '2022.09', '2023.07', '2024.05']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_SQLFLUFF","title":"SPECS_SQLFLUFF  <code>module-attribute</code>","text":"<pre><code>SPECS_SQLFLUFF = {k: {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'test_cmd': TEST_PYTEST}for k in ['0.10', '0.11', '0.12', '0.13', '0.4', '0.5', '0.6', '0.8', '0.9', '1.0', '1.1', '1.2', '1.3', '1.4', '2.0', '2.1', '2.2']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_DBT_CORE","title":"SPECS_DBT_CORE  <code>module-attribute</code>","text":"<pre><code>SPECS_DBT_CORE = {k: {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .'}for k in ['0.13', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PYVISTA","title":"SPECS_PYVISTA  <code>module-attribute</code>","text":"<pre><code>SPECS_PYVISTA = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'pip_packages': ['pytest'], 'test_cmd': TEST_PYTEST}for k in ['0.20', '0.21', '0.22', '0.23']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_ASTROID","title":"SPECS_ASTROID  <code>module-attribute</code>","text":"<pre><code>SPECS_ASTROID = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'pip_packages': ['pytest'], 'test_cmd': TEST_PYTEST}for k in ['2.10', '2.12', '2.13', '2.14', '2.15', '2.16', '2.5', '2.6', '2.7', '2.8', '2.9', '3.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_MARSHMALLOW","title":"SPECS_MARSHMALLOW  <code>module-attribute</code>","text":"<pre><code>SPECS_MARSHMALLOW = {k: {'python': '3.9', 'install': \"python -m pip install -e '.[dev]'\", 'test_cmd': TEST_PYTEST}for k in ['2.18', '2.19', '2.20', '3.0', '3.1', '3.10', '3.11', '3.12', '3.13', '3.15', '3.16', '3.19', '3.2', '3.4', '3.8', '3.9']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PVLIB","title":"SPECS_PVLIB  <code>module-attribute</code>","text":"<pre><code>SPECS_PVLIB = {k: {'python': '3.9', 'install': 'python -m pip install -e .[all]', 'packages': 'pandas scipy', 'pip_packages': ['jupyter', 'ipython', 'matplotlib', 'pytest', 'flake8'], 'test_cmd': TEST_PYTEST}for k in ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_PYDICOM","title":"SPECS_PYDICOM  <code>module-attribute</code>","text":"<pre><code>SPECS_PYDICOM = {k: {'python': '3.6', 'install': 'python -m pip install -e .', 'packages': 'numpy', 'pip_packages': ['pytest'], 'test_cmd': TEST_PYTEST}for k in ['1.0', '1.1', '1.2', '1.3', '1.4', '2.0', '2.1', '2.2', '2.3', '2.4', '3.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_HUMANEVAL","title":"SPECS_HUMANEVAL  <code>module-attribute</code>","text":"<pre><code>SPECS_HUMANEVAL = {k: {'python': '3.9', 'test_cmd': 'python'}for k in ['1.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_PY","title":"MAP_REPO_VERSION_TO_SPECS_PY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_PY = {'astropy/astropy': SPECS_ASTROPY, 'dbt-labs/dbt-core': SPECS_DBT_CORE, 'django/django': SPECS_DJANGO, 'matplotlib/matplotlib': SPECS_MATPLOTLIB, 'marshmallow-code/marshmallow': SPECS_MARSHMALLOW, 'mwaskom/seaborn': SPECS_SEABORN, 'pallets/flask': SPECS_FLASK, 'psf/requests': SPECS_REQUESTS, 'pvlib/pvlib-python': SPECS_PVLIB, 'pydata/xarray': SPECS_XARRAY, 'pydicom/pydicom': SPECS_PYDICOM, 'pylint-dev/astroid': SPECS_ASTROID, 'pylint-dev/pylint': SPECS_PYLINT, 'pytest-dev/pytest': SPECS_PYTEST, 'pyvista/pyvista': SPECS_PYVISTA, 'scikit-learn/scikit-learn': SPECS_SKLEARN, 'sphinx-doc/sphinx': SPECS_SPHINX, 'sqlfluff/sqlfluff': SPECS_SQLFLUFF, 'swe-bench/humaneval': SPECS_HUMANEVAL, 'sympy/sympy': SPECS_SYMPY}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_PY","title":"MAP_REPO_TO_INSTALL_PY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_PY = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_REQS_PATHS","title":"MAP_REPO_TO_REQS_PATHS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_REQS_PATHS = {'dbt-labs/dbt-core': ['dev-requirements.txt', 'dev_requirements.txt'], 'django/django': ['tests/requirements/py3.txt'], 'matplotlib/matplotlib': ['requirements/dev/dev-requirements.txt', 'requirements/testing/travis_all.txt'], 'pallets/flask': ['requirements/dev.txt'], 'pylint-dev/pylint': ['requirements_test.txt'], 'pyvista/pyvista': ['requirements_test.txt', 'requirements.txt'], 'sqlfluff/sqlfluff': ['requirements_dev.txt'], 'sympy/sympy': ['requirements-dev.txt', 'requirements-test.txt']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_ENV_YML_PATHS","title":"MAP_REPO_TO_ENV_YML_PATHS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_ENV_YML_PATHS = {'matplotlib/matplotlib': ['environment.yml'], 'pydata/xarray': ['ci/requirements/environment.yml', 'environment.yml']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.USE_X86_PY","title":"USE_X86_PY  <code>module-attribute</code>","text":"<pre><code>USE_X86_PY = {'astropy__astropy-7973', 'django__django-10087', 'django__django-10097', 'django__django-10213', 'django__django-10301', 'django__django-10316', 'django__django-10426', 'django__django-11383', 'django__django-12185', 'django__django-12497', 'django__django-13121', 'django__django-13417', 'django__django-13431', 'django__django-13447', 'django__django-14155', 'django__django-14164', 'django__django-14169', 'django__django-14170', 'django__django-15180', 'django__django-15199', 'django__django-15280', 'django__django-15292', 'django__django-15474', 'django__django-15682', 'django__django-15689', 'django__django-15695', 'django__django-15698', 'django__django-15781', 'django__django-15925', 'django__django-15930', 'django__django-5158', 'django__django-5470', 'django__django-7188', 'django__django-7475', 'django__django-7530', 'django__django-8326', 'django__django-8961', 'django__django-9003', 'django__django-9703', 'django__django-9871', 'matplotlib__matplotlib-13983', 'matplotlib__matplotlib-13984', 'matplotlib__matplotlib-13989', 'matplotlib__matplotlib-14043', 'matplotlib__matplotlib-14471', 'matplotlib__matplotlib-22711', 'matplotlib__matplotlib-22719', 'matplotlib__matplotlib-22734', 'matplotlib__matplotlib-22767', 'matplotlib__matplotlib-22815', 'matplotlib__matplotlib-22835', 'matplotlib__matplotlib-22865', 'matplotlib__matplotlib-22871', 'matplotlib__matplotlib-22883', 'matplotlib__matplotlib-22926', 'matplotlib__matplotlib-22929', 'matplotlib__matplotlib-22931', 'matplotlib__matplotlib-22945', 'matplotlib__matplotlib-22991', 'matplotlib__matplotlib-23031', 'matplotlib__matplotlib-23047', 'matplotlib__matplotlib-23049', 'matplotlib__matplotlib-23057', 'matplotlib__matplotlib-23088', 'matplotlib__matplotlib-23111', 'matplotlib__matplotlib-23140', 'matplotlib__matplotlib-23174', 'matplotlib__matplotlib-23188', 'matplotlib__matplotlib-23198', 'matplotlib__matplotlib-23203', 'matplotlib__matplotlib-23266', 'matplotlib__matplotlib-23267', 'matplotlib__matplotlib-23288', 'matplotlib__matplotlib-23299', 'matplotlib__matplotlib-23314', 'matplotlib__matplotlib-23332', 'matplotlib__matplotlib-23348', 'matplotlib__matplotlib-23412', 'matplotlib__matplotlib-23476', 'matplotlib__matplotlib-23516', 'matplotlib__matplotlib-23562', 'matplotlib__matplotlib-23563', 'matplotlib__matplotlib-23573', 'matplotlib__matplotlib-23740', 'matplotlib__matplotlib-23742', 'matplotlib__matplotlib-23913', 'matplotlib__matplotlib-23964', 'matplotlib__matplotlib-23987', 'matplotlib__matplotlib-24013', 'matplotlib__matplotlib-24026', 'matplotlib__matplotlib-24088', 'matplotlib__matplotlib-24111', 'matplotlib__matplotlib-24149', 'matplotlib__matplotlib-24177', 'matplotlib__matplotlib-24189', 'matplotlib__matplotlib-24224', 'matplotlib__matplotlib-24250', 'matplotlib__matplotlib-24257', 'matplotlib__matplotlib-24265', 'matplotlib__matplotlib-24334', 'matplotlib__matplotlib-24362', 'matplotlib__matplotlib-24403', 'matplotlib__matplotlib-24431', 'matplotlib__matplotlib-24538', 'matplotlib__matplotlib-24570', 'matplotlib__matplotlib-24604', 'matplotlib__matplotlib-24619', 'matplotlib__matplotlib-24627', 'matplotlib__matplotlib-24637', 'matplotlib__matplotlib-24691', 'matplotlib__matplotlib-24749', 'matplotlib__matplotlib-24768', 'matplotlib__matplotlib-24849', 'matplotlib__matplotlib-24870', 'matplotlib__matplotlib-24912', 'matplotlib__matplotlib-24924', 'matplotlib__matplotlib-24970', 'matplotlib__matplotlib-24971', 'matplotlib__matplotlib-25027', 'matplotlib__matplotlib-25052', 'matplotlib__matplotlib-25079', 'matplotlib__matplotlib-25085', 'matplotlib__matplotlib-25122', 'matplotlib__matplotlib-25126', 'matplotlib__matplotlib-25129', 'matplotlib__matplotlib-25238', 'matplotlib__matplotlib-25281', 'matplotlib__matplotlib-25287', 'matplotlib__matplotlib-25311', 'matplotlib__matplotlib-25332', 'matplotlib__matplotlib-25334', 'matplotlib__matplotlib-25340', 'matplotlib__matplotlib-25346', 'matplotlib__matplotlib-25404', 'matplotlib__matplotlib-25405', 'matplotlib__matplotlib-25425', 'matplotlib__matplotlib-25430', 'matplotlib__matplotlib-25433', 'matplotlib__matplotlib-25442', 'matplotlib__matplotlib-25479', 'matplotlib__matplotlib-25498', 'matplotlib__matplotlib-25499', 'matplotlib__matplotlib-25515', 'matplotlib__matplotlib-25547', 'matplotlib__matplotlib-25551', 'matplotlib__matplotlib-25565', 'matplotlib__matplotlib-25624', 'matplotlib__matplotlib-25631', 'matplotlib__matplotlib-25640', 'matplotlib__matplotlib-25651', 'matplotlib__matplotlib-25667', 'matplotlib__matplotlib-25712', 'matplotlib__matplotlib-25746', 'matplotlib__matplotlib-25772', 'matplotlib__matplotlib-25775', 'matplotlib__matplotlib-25779', 'matplotlib__matplotlib-25785', 'matplotlib__matplotlib-25794', 'matplotlib__matplotlib-25859', 'matplotlib__matplotlib-25960', 'matplotlib__matplotlib-26011', 'matplotlib__matplotlib-26020', 'matplotlib__matplotlib-26024', 'matplotlib__matplotlib-26078', 'matplotlib__matplotlib-26089', 'matplotlib__matplotlib-26101', 'matplotlib__matplotlib-26113', 'matplotlib__matplotlib-26122', 'matplotlib__matplotlib-26160', 'matplotlib__matplotlib-26184', 'matplotlib__matplotlib-26208', 'matplotlib__matplotlib-26223', 'matplotlib__matplotlib-26232', 'matplotlib__matplotlib-26249', 'matplotlib__matplotlib-26278', 'matplotlib__matplotlib-26285', 'matplotlib__matplotlib-26291', 'matplotlib__matplotlib-26300', 'matplotlib__matplotlib-26311', 'matplotlib__matplotlib-26341', 'matplotlib__matplotlib-26342', 'matplotlib__matplotlib-26399', 'matplotlib__matplotlib-26466', 'matplotlib__matplotlib-26469', 'matplotlib__matplotlib-26472', 'matplotlib__matplotlib-26479', 'matplotlib__matplotlib-26532', 'pydata__xarray-2905', 'pydata__xarray-2922', 'pydata__xarray-3095', 'pydata__xarray-3114', 'pydata__xarray-3151', 'pydata__xarray-3156', 'pydata__xarray-3159', 'pydata__xarray-3239', 'pydata__xarray-3302', 'pydata__xarray-3305', 'pydata__xarray-3338', 'pydata__xarray-3364', 'pydata__xarray-3406', 'pydata__xarray-3520', 'pydata__xarray-3527', 'pydata__xarray-3631', 'pydata__xarray-3635', 'pydata__xarray-3637', 'pydata__xarray-3649', 'pydata__xarray-3677', 'pydata__xarray-3733', 'pydata__xarray-3812', 'pydata__xarray-3905', 'pydata__xarray-3976', 'pydata__xarray-3979', 'pydata__xarray-3993', 'pydata__xarray-4075', 'pydata__xarray-4094', 'pydata__xarray-4098', 'pydata__xarray-4182', 'pydata__xarray-4184', 'pydata__xarray-4248', 'pydata__xarray-4339', 'pydata__xarray-4356', 'pydata__xarray-4419', 'pydata__xarray-4423', 'pydata__xarray-4442', 'pydata__xarray-4493', 'pydata__xarray-4510', 'pydata__xarray-4629', 'pydata__xarray-4683', 'pydata__xarray-4684', 'pydata__xarray-4687', 'pydata__xarray-4695', 'pydata__xarray-4750', 'pydata__xarray-4758', 'pydata__xarray-4759', 'pydata__xarray-4767', 'pydata__xarray-4802', 'pydata__xarray-4819', 'pydata__xarray-4827', 'pydata__xarray-4879', 'pydata__xarray-4911', 'pydata__xarray-4939', 'pydata__xarray-4940', 'pydata__xarray-4966', 'pydata__xarray-4994', 'pydata__xarray-5033', 'pydata__xarray-5126', 'pydata__xarray-5131', 'pydata__xarray-5180', 'pydata__xarray-5187', 'pydata__xarray-5233', 'pydata__xarray-5362', 'pydata__xarray-5365', 'pydata__xarray-5455', 'pydata__xarray-5580', 'pydata__xarray-5662', 'pydata__xarray-5682', 'pydata__xarray-5731', 'pydata__xarray-6135', 'pydata__xarray-6386', 'pydata__xarray-6394', 'pydata__xarray-6400', 'pydata__xarray-6461', 'pydata__xarray-6548', 'pydata__xarray-6598', 'pydata__xarray-6599', 'pydata__xarray-6601', 'pydata__xarray-6721', 'pydata__xarray-6744', 'pydata__xarray-6798', 'pydata__xarray-6804', 'pydata__xarray-6823', 'pydata__xarray-6857', 'pydata__xarray-6882', 'pydata__xarray-6889', 'pydata__xarray-6938', 'pydata__xarray-6971', 'pydata__xarray-6992', 'pydata__xarray-6999', 'pydata__xarray-7003', 'pydata__xarray-7019', 'pydata__xarray-7052', 'pydata__xarray-7089', 'pydata__xarray-7101', 'pydata__xarray-7105', 'pydata__xarray-7112', 'pydata__xarray-7120', 'pydata__xarray-7147', 'pydata__xarray-7150', 'pydata__xarray-7179', 'pydata__xarray-7203', 'pydata__xarray-7229', 'pydata__xarray-7233', 'pydata__xarray-7347', 'pydata__xarray-7391', 'pydata__xarray-7393', 'pydata__xarray-7400', 'pydata__xarray-7444', 'pytest-dev__pytest-10482', 'scikit-learn__scikit-learn-10198', 'scikit-learn__scikit-learn-10297', 'scikit-learn__scikit-learn-10306', 'scikit-learn__scikit-learn-10331', 'scikit-learn__scikit-learn-10377', 'scikit-learn__scikit-learn-10382', 'scikit-learn__scikit-learn-10397', 'scikit-learn__scikit-learn-10427', 'scikit-learn__scikit-learn-10428', 'scikit-learn__scikit-learn-10443', 'scikit-learn__scikit-learn-10452', 'scikit-learn__scikit-learn-10459', 'scikit-learn__scikit-learn-10471', 'scikit-learn__scikit-learn-10483', 'scikit-learn__scikit-learn-10495', 'scikit-learn__scikit-learn-10508', 'scikit-learn__scikit-learn-10558', 'scikit-learn__scikit-learn-10577', 'scikit-learn__scikit-learn-10581', 'scikit-learn__scikit-learn-10687', 'scikit-learn__scikit-learn-10774', 'scikit-learn__scikit-learn-10777', 'scikit-learn__scikit-learn-10803', 'scikit-learn__scikit-learn-10844', 'scikit-learn__scikit-learn-10870', 'scikit-learn__scikit-learn-10881', 'scikit-learn__scikit-learn-10899', 'scikit-learn__scikit-learn-10908', 'scikit-learn__scikit-learn-10913', 'scikit-learn__scikit-learn-10949', 'scikit-learn__scikit-learn-10982', 'scikit-learn__scikit-learn-10986', 'scikit-learn__scikit-learn-11040', 'scikit-learn__scikit-learn-11042', 'scikit-learn__scikit-learn-11043', 'scikit-learn__scikit-learn-11151', 'scikit-learn__scikit-learn-11160', 'scikit-learn__scikit-learn-11206', 'scikit-learn__scikit-learn-11235', 'scikit-learn__scikit-learn-11243', 'scikit-learn__scikit-learn-11264', 'scikit-learn__scikit-learn-11281', 'scikit-learn__scikit-learn-11310', 'scikit-learn__scikit-learn-11315', 'scikit-learn__scikit-learn-11333', 'scikit-learn__scikit-learn-11346', 'scikit-learn__scikit-learn-11391', 'scikit-learn__scikit-learn-11496', 'scikit-learn__scikit-learn-11542', 'scikit-learn__scikit-learn-11574', 'scikit-learn__scikit-learn-11578', 'scikit-learn__scikit-learn-11585', 'scikit-learn__scikit-learn-11596', 'scikit-learn__scikit-learn-11635', 'scikit-learn__scikit-learn-12258', 'scikit-learn__scikit-learn-12421', 'scikit-learn__scikit-learn-12443', 'scikit-learn__scikit-learn-12462', 'scikit-learn__scikit-learn-12471', 'scikit-learn__scikit-learn-12486', 'scikit-learn__scikit-learn-12557', 'scikit-learn__scikit-learn-12583', 'scikit-learn__scikit-learn-12585', 'scikit-learn__scikit-learn-12625', 'scikit-learn__scikit-learn-12626', 'scikit-learn__scikit-learn-12656', 'scikit-learn__scikit-learn-12682', 'scikit-learn__scikit-learn-12704', 'scikit-learn__scikit-learn-12733', 'scikit-learn__scikit-learn-12758', 'scikit-learn__scikit-learn-12760', 'scikit-learn__scikit-learn-12784', 'scikit-learn__scikit-learn-12827', 'scikit-learn__scikit-learn-12834', 'scikit-learn__scikit-learn-12860', 'scikit-learn__scikit-learn-12908', 'scikit-learn__scikit-learn-12938', 'scikit-learn__scikit-learn-12961', 'scikit-learn__scikit-learn-12973', 'scikit-learn__scikit-learn-12983', 'scikit-learn__scikit-learn-12989', 'scikit-learn__scikit-learn-13010', 'scikit-learn__scikit-learn-13013', 'scikit-learn__scikit-learn-13017', 'scikit-learn__scikit-learn-13046', 'scikit-learn__scikit-learn-13087', 'scikit-learn__scikit-learn-13124', 'scikit-learn__scikit-learn-13135', 'scikit-learn__scikit-learn-13142', 'scikit-learn__scikit-learn-13143', 'scikit-learn__scikit-learn-13157', 'scikit-learn__scikit-learn-13165', 'scikit-learn__scikit-learn-13174', 'scikit-learn__scikit-learn-13221', 'scikit-learn__scikit-learn-13241', 'scikit-learn__scikit-learn-13253', 'scikit-learn__scikit-learn-13280', 'scikit-learn__scikit-learn-13283', 'scikit-learn__scikit-learn-13302', 'scikit-learn__scikit-learn-13313', 'scikit-learn__scikit-learn-13328', 'scikit-learn__scikit-learn-13333', 'scikit-learn__scikit-learn-13363', 'scikit-learn__scikit-learn-13368', 'scikit-learn__scikit-learn-13392', 'scikit-learn__scikit-learn-13436', 'scikit-learn__scikit-learn-13439', 'scikit-learn__scikit-learn-13447', 'scikit-learn__scikit-learn-13454', 'scikit-learn__scikit-learn-13467', 'scikit-learn__scikit-learn-13472', 'scikit-learn__scikit-learn-13485', 'scikit-learn__scikit-learn-13496', 'scikit-learn__scikit-learn-13497', 'scikit-learn__scikit-learn-13536', 'scikit-learn__scikit-learn-13549', 'scikit-learn__scikit-learn-13554', 'scikit-learn__scikit-learn-13584', 'scikit-learn__scikit-learn-13618', 'scikit-learn__scikit-learn-13620', 'scikit-learn__scikit-learn-13628', 'scikit-learn__scikit-learn-13641', 'scikit-learn__scikit-learn-13704', 'scikit-learn__scikit-learn-13726', 'scikit-learn__scikit-learn-13779', 'scikit-learn__scikit-learn-13780', 'scikit-learn__scikit-learn-13828', 'scikit-learn__scikit-learn-13864', 'scikit-learn__scikit-learn-13877', 'scikit-learn__scikit-learn-13910', 'scikit-learn__scikit-learn-13915', 'scikit-learn__scikit-learn-13933', 'scikit-learn__scikit-learn-13960', 'scikit-learn__scikit-learn-13974', 'scikit-learn__scikit-learn-13983', 'scikit-learn__scikit-learn-14012', 'scikit-learn__scikit-learn-14024', 'scikit-learn__scikit-learn-14053', 'scikit-learn__scikit-learn-14067', 'scikit-learn__scikit-learn-14087', 'scikit-learn__scikit-learn-14092', 'scikit-learn__scikit-learn-14114', 'scikit-learn__scikit-learn-14125', 'scikit-learn__scikit-learn-14141', 'scikit-learn__scikit-learn-14237', 'scikit-learn__scikit-learn-14309', 'scikit-learn__scikit-learn-14430', 'scikit-learn__scikit-learn-14450', 'scikit-learn__scikit-learn-14458', 'scikit-learn__scikit-learn-14464', 'scikit-learn__scikit-learn-14496', 'scikit-learn__scikit-learn-14520', 'scikit-learn__scikit-learn-14544', 'scikit-learn__scikit-learn-14591', 'scikit-learn__scikit-learn-14629', 'scikit-learn__scikit-learn-14704', 'scikit-learn__scikit-learn-14706', 'scikit-learn__scikit-learn-14710', 'scikit-learn__scikit-learn-14732', 'scikit-learn__scikit-learn-14764', 'scikit-learn__scikit-learn-14806', 'scikit-learn__scikit-learn-14869', 'scikit-learn__scikit-learn-14878', 'scikit-learn__scikit-learn-14890', 'scikit-learn__scikit-learn-14894', 'scikit-learn__scikit-learn-14898', 'scikit-learn__scikit-learn-14908', 'scikit-learn__scikit-learn-14983', 'scikit-learn__scikit-learn-14999', 'scikit-learn__scikit-learn-15028', 'scikit-learn__scikit-learn-15084', 'scikit-learn__scikit-learn-15086', 'scikit-learn__scikit-learn-15094', 'scikit-learn__scikit-learn-15096', 'scikit-learn__scikit-learn-15100', 'scikit-learn__scikit-learn-15119', 'scikit-learn__scikit-learn-15120', 'scikit-learn__scikit-learn-15138', 'scikit-learn__scikit-learn-15393', 'scikit-learn__scikit-learn-15495', 'scikit-learn__scikit-learn-15512', 'scikit-learn__scikit-learn-15524', 'scikit-learn__scikit-learn-15535', 'scikit-learn__scikit-learn-15625', 'scikit-learn__scikit-learn-3840', 'scikit-learn__scikit-learn-7760', 'scikit-learn__scikit-learn-8554', 'scikit-learn__scikit-learn-9274', 'scikit-learn__scikit-learn-9288', 'scikit-learn__scikit-learn-9304', 'scikit-learn__scikit-learn-9775', 'scikit-learn__scikit-learn-9939', 'sphinx-doc__sphinx-11311', 'sphinx-doc__sphinx-7910', 'sympy__sympy-12812', 'sympy__sympy-14248', 'sympy__sympy-15222', 'sympy__sympy-19201'}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.FASTLANE_RSPEC_JQ_TRANSFORM","title":"FASTLANE_RSPEC_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>FASTLANE_RSPEC_JQ_TRANSFORM = 'tail -n +2 | jq -r \\'.examples[] | \"\\\\(.description) - \\\\(.id) - \\\\(.status)\"\\''\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.FPM_RSPEC_JQ_TRANSFORM","title":"FPM_RSPEC_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>FPM_RSPEC_JQ_TRANSFORM = 'sed -n \\'/^{/,$p\\' | jq -r \\'.examples[] | \"\\\\(.description) - \\\\(.status)\"\\''\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.RUBOCOP_RSPEC_JQ_TRANSFORM","title":"RUBOCOP_RSPEC_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>RUBOCOP_RSPEC_JQ_TRANSFORM = strip()\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_JEKYLL","title":"SPECS_JEKYLL  <code>module-attribute</code>","text":"<pre><code>SPECS_JEKYLL = {'9141': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap'], 'test_cmd': ['bundle exec ruby -I test test/test_site.rb -v -n \"/static files/\"']}, '8761': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap'], 'test_cmd': ['bundle exec cucumber --publish-quiet --format progress --no-color features/post_data.feature:6 features/post_data.feature:30']}, '8047': {'docker_specs': {'ruby_version': '3.3'}, 'pre_install': [\"sed -i '/^[[:space:]]*install_if.*mingw/,/^[[:space:]]*end/d' Gemfile\"], 'install': ['script/bootstrap', 'bundle add webrick'], 'test_cmd': ['bundle exec ruby -I test test/test_filters.rb -v -n \"/where_exp filter/\"']}, '8167': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap', 'bundle add webrick'], 'test_cmd': ['bundle exec ruby -I test test/test_utils.rb -v -n \"/Utils.slugify/\"']}, '8771': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap'], 'test_cmd': ['bundle exec cucumber --publish-quiet --format progress --no-color features/incremental_rebuild.feature:27 features/incremental_rebuild.feature:70']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_FLUENTD","title":"SPECS_FLUENTD  <code>module-attribute</code>","text":"<pre><code>SPECS_FLUENTD = {'4598': {'docker_specs': {'ruby_version': '3.3'}, 'pre_install': ['echo \"gem \\'console\\', \\'1.29\\'\" &gt;&gt; Gemfile'], 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin_helper/test_http_server_helper.rb -v -n '/mount/'\"]}, '4311': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/config/test_system_config.rb -v -n '/rotate_age/'\"]}, '4655': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_http.rb -v -n '/test_add/'\"]}, '4030': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': ['bundle exec ruby test/plugin/out_forward/test_ack_handler.rb -v']}, '3917': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': ['bundle exec ruby test/test_config.rb -v']}, '3640': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin_helper/test_retry_state.rb -v -n '/exponential backoff/'\"]}, '3641': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': ['bundle exec ruby test/test_supervisor.rb -v']}, '3616': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_http.rb -v -n '/test_application/'\"]}, '3631': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/test_event_router.rb -v -n '/handle_emits_error/'\"]}, '3466': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_tail.rb -v -n '/test_should_replace_target_info/'\"]}, '3328': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_tail.rb -v -n '/test_ENOENT_error_after_setup_watcher/'\"]}, '3608': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_output_as_buffered_retries.rb -v -n '/retry_max_times/'\"]}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_FASTLANE","title":"SPECS_FASTLANE  <code>module-attribute</code>","text":"<pre><code>SPECS_FASTLANE = {'21857': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/lane_manager_base_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '20958': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/import_from_git_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '20642': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./frameit/spec/device_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '19765': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/download_dsyms_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '20975': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./match/spec/storage/s3_storage_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '19304': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/zip_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '19207': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/zip_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_FPM","title":"SPECS_FPM  <code>module-attribute</code>","text":"<pre><code>SPECS_FPM = {'1850': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/fpm/package/empty_spec.rb --no-color --format json | {FPM_RSPEC_JQ_TRANSFORM}']}, '1829': {'docker_specs': {'ruby_version': '3.1'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/fpm/package/deb_spec.rb --no-color --format json | {FPM_RSPEC_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_FAKER","title":"SPECS_FAKER  <code>module-attribute</code>","text":"<pre><code>SPECS_FAKER = {'2970': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/faker/default/test_faker_internet.rb -v -n '/email/'\"]}, '2705': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/faker/default/test_faker_internet.rb -v -n '/password/'\"]}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_RUBOCOP","title":"SPECS_RUBOCOP  <code>module-attribute</code>","text":"<pre><code>SPECS_RUBOCOP = {'13705': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/lint/out_of_range_regexp_ref_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13687': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/lint/safe_navigation_chain_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13680': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/redundant_line_continuation_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13668': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/sole_nested_conditional_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13627': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/multiple_comparison_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13653': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/access_modifier_declarations_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13579': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/layout/line_continuation_spacing_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13560': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/file_null_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13503': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/dig_chain_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13479': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/layout/leading_comment_space_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13431': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/layout/empty_lines_around_method_body_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13424': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/safe_navigation_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13393': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/guard_clause_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13396': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/redundant_parentheses_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13375': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cli_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13362': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/redundant_freeze_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_RUBY","title":"MAP_REPO_VERSION_TO_SPECS_RUBY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_RUBY = {'jekyll/jekyll': SPECS_JEKYLL, 'fluent/fluentd': SPECS_FLUENTD, 'fastlane/fastlane': SPECS_FASTLANE, 'jordansissel/fpm': SPECS_FPM, 'faker-ruby/faker': SPECS_FAKER, 'rubocop/rubocop': SPECS_RUBOCOP}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_RUBY","title":"MAP_REPO_TO_INSTALL_RUBY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_RUBY = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_RIPGREP","title":"SPECS_RIPGREP  <code>module-attribute</code>","text":"<pre><code>SPECS_RIPGREP = {'2576': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration -- regression']}, '2209': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration -- regression::r2208 --exact']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_BAT","title":"SPECS_BAT  <code>module-attribute</code>","text":"<pre><code>SPECS_BAT = {'3108': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag']}, '2835': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests header --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests header']}, '2650': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests map_syntax --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests map_syntax']}, '2393': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests cache_ --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests cache_']}, '2201': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag']}, '2260': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests syntax --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests syntax']}, '1892': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests ignored_suffix_arg --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests ignored_suffix_arg']}, '562': {'docker_specs': {'rust_version': '1.81'}, 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests cache']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SPECS_RUFF","title":"SPECS_RUFF  <code>module-attribute</code>","text":"<pre><code>SPECS_RUFF = {'15626': {'docker_specs': {'rust_version': '1.84'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::flake8_simplify::tests --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::flake8_simplify::tests']}, '15543': {'docker_specs': {'rust_version': '1.84'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::pyupgrade --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::pyupgrade']}, '15443': {'docker_specs': {'rust_version': '1.84'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::flake8_bandit --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::flake8_bandit']}, '15394': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::flake8_pie --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::flake8_pie']}, '15356': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::pycodestyle --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::pycodestyle']}, '15330': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::eradicate --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::eradicate']}, '15309': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --no-run'], 'test_cmd': [\"cargo test --package ruff_linter 'f52'\"]}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TOKIO_SPECS","title":"TOKIO_SPECS  <code>module-attribute</code>","text":"<pre><code>TOKIO_SPECS = {'6724': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test io_write_all_buf --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test io_write_all_buf --no-fail-fast']}, '6838': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test uds_stream --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test uds_stream --no-fail-fast']}, '6752': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test time_delay_queue --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test time_delay_queue --no-fail-fast']}, '4867': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test sync_broadcast --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test sync_broadcast --no-fail-fast']}, '4898': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics --no-run'], 'test_cmd': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics']}, '6603': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test sync_mpsc --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test sync_mpsc --no-fail-fast']}, '6551': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics --no-fail-fast --no-run'], 'test_cmd': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics --no-fail-fast']}, '4384': {'docker_specs': {'rust_version': '1.81'}, 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package tokio --test net_types_unwind --features full --no-fail-fast']}, '7139': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --test fs_file --no-fail-fast --no-run'], 'test_cmd': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --test fs_file --no-fail-fast']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.COREUTILS_SPECS","title":"COREUTILS_SPECS  <code>module-attribute</code>","text":"<pre><code>COREUTILS_SPECS = {'6690': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --no-run -- test_cp_cp test_cp_same_file test_cp_multiple_files test_cp_single_file test_cp_no_file'], 'test_cmd': ['cargo test --no-fail-fast -- test_cp_cp test_cp_same_file test_cp_multiple_files test_cp_single_file test_cp_no_file']}, '6731': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test backslash --no-run'], 'test_cmd': ['cargo test backslash --no-fail-fast']}, '6575': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test cksum --no-run'], 'test_cmd': ['cargo test cksum --no-fail-fast']}, '6682': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test mkdir --no-run'], 'test_cmd': ['cargo test mkdir --no-fail-fast']}, '6377': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test test_env --no-run'], 'test_cmd': ['cargo test test_env --no-fail-fast']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.NUSHELL_SPECS","title":"NUSHELL_SPECS  <code>module-attribute</code>","text":"<pre><code>NUSHELL_SPECS = {'13246': {'docker_specs': {'rust_version': '1.77'}, 'install': ['cargo test -p nu-command --no-run --test main find::'], 'build': ['cargo build'], 'test_cmd': ['cargo test -p nu-command --no-fail-fast --test main find::']}, '12950': {'docker_specs': {'rust_version': '1.77'}, 'install': ['cargo test external_arguments --no-run'], 'test_cmd': ['cargo test external_arguments --no-fail-fast']}, '12901': {'docker_specs': {'rust_version': '1.77'}, 'install': ['cargo test --no-run shell::env'], 'test_cmd': ['cargo test --no-fail-fast shell::env']}, '13831': {'docker_specs': {'rust_version': '1.79'}, 'install': ['cargo test -p nu-command --no-run split_column'], 'build': ['cargo build'], 'test_cmd': ['cargo test -p nu-command --no-fail-fast split_column']}, '13605': {'docker_specs': {'rust_version': '1.78'}, 'install': ['cargo test -p nu-command --no-run ls::'], 'build': ['cargo build'], 'test_cmd': ['cargo test -p nu-command --no-fail-fast ls::']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.AXUM_SPECS","title":"AXUM_SPECS  <code>module-attribute</code>","text":"<pre><code>AXUM_SPECS = {'2096': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::fallback']}, '1934': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::fallback']}, '1730': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::mod state']}, '1119': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib slash --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib slash']}, '734': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::head']}, '691': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::nest::nesting_router_at_root --exact']}, '682': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib trailing --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib trailing -- with_trailing_slash_post without_trailing_slash_post']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS_RUST","title":"MAP_REPO_VERSION_TO_SPECS_RUST  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_RUST = {'burntsushi/ripgrep': SPECS_RIPGREP, 'sharkdp/bat': SPECS_BAT, 'astral-sh/ruff': SPECS_RUFF, 'tokio-rs/tokio': TOKIO_SPECS, 'uutils/coreutils': COREUTILS_SPECS, 'nushell/nushell': NUSHELL_SPECS, 'tokio-rs/axum': AXUM_SPECS}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL_RUST","title":"MAP_REPO_TO_INSTALL_RUST  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_RUST = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.BASE_IMAGE_BUILD_DIR","title":"BASE_IMAGE_BUILD_DIR  <code>module-attribute</code>","text":"<pre><code>BASE_IMAGE_BUILD_DIR = Path('logs/build_images/base')\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ENV_IMAGE_BUILD_DIR","title":"ENV_IMAGE_BUILD_DIR  <code>module-attribute</code>","text":"<pre><code>ENV_IMAGE_BUILD_DIR = Path('logs/build_images/env')\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.INSTANCE_IMAGE_BUILD_DIR","title":"INSTANCE_IMAGE_BUILD_DIR  <code>module-attribute</code>","text":"<pre><code>INSTANCE_IMAGE_BUILD_DIR = Path('logs/build_images/instances')\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.RUN_EVALUATION_LOG_DIR","title":"RUN_EVALUATION_LOG_DIR  <code>module-attribute</code>","text":"<pre><code>RUN_EVALUATION_LOG_DIR = Path('logs/run_evaluation')\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.RUN_VALIDATION_LOG_DIR","title":"RUN_VALIDATION_LOG_DIR  <code>module-attribute</code>","text":"<pre><code>RUN_VALIDATION_LOG_DIR = Path('logs/run_validation')\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.FAIL_TO_PASS","title":"FAIL_TO_PASS  <code>module-attribute</code>","text":"<pre><code>FAIL_TO_PASS = 'FAIL_TO_PASS'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.FAIL_TO_FAIL","title":"FAIL_TO_FAIL  <code>module-attribute</code>","text":"<pre><code>FAIL_TO_FAIL = 'FAIL_TO_FAIL'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PASS_TO_PASS","title":"PASS_TO_PASS  <code>module-attribute</code>","text":"<pre><code>PASS_TO_PASS = 'PASS_TO_PASS'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PASS_TO_FAIL","title":"PASS_TO_FAIL  <code>module-attribute</code>","text":"<pre><code>PASS_TO_FAIL = 'PASS_TO_FAIL'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.KEY_INSTANCE_ID","title":"KEY_INSTANCE_ID  <code>module-attribute</code>","text":"<pre><code>KEY_INSTANCE_ID = 'instance_id'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.KEY_MODEL","title":"KEY_MODEL  <code>module-attribute</code>","text":"<pre><code>KEY_MODEL = 'model_name_or_path'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.KEY_PREDICTION","title":"KEY_PREDICTION  <code>module-attribute</code>","text":"<pre><code>KEY_PREDICTION = 'model_patch'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.DOCKER_PATCH","title":"DOCKER_PATCH  <code>module-attribute</code>","text":"<pre><code>DOCKER_PATCH = '/tmp/patch.diff'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.DOCKER_USER","title":"DOCKER_USER  <code>module-attribute</code>","text":"<pre><code>DOCKER_USER = 'root'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.DOCKER_WORKDIR","title":"DOCKER_WORKDIR  <code>module-attribute</code>","text":"<pre><code>DOCKER_WORKDIR = '/testbed'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.LOG_REPORT","title":"LOG_REPORT  <code>module-attribute</code>","text":"<pre><code>LOG_REPORT = 'report.json'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.LOG_INSTANCE","title":"LOG_INSTANCE  <code>module-attribute</code>","text":"<pre><code>LOG_INSTANCE = 'run_instance.log'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.LOG_TEST_OUTPUT","title":"LOG_TEST_OUTPUT  <code>module-attribute</code>","text":"<pre><code>LOG_TEST_OUTPUT = 'test_output.txt'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.UTF8","title":"UTF8  <code>module-attribute</code>","text":"<pre><code>UTF8 = 'utf-8'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.APPLY_PATCH_FAIL","title":"APPLY_PATCH_FAIL  <code>module-attribute</code>","text":"<pre><code>APPLY_PATCH_FAIL = '&gt;&gt;&gt;&gt;&gt; Patch Apply Failed'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.APPLY_PATCH_PASS","title":"APPLY_PATCH_PASS  <code>module-attribute</code>","text":"<pre><code>APPLY_PATCH_PASS = '&gt;&gt;&gt;&gt;&gt; Applied Patch'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.INSTALL_FAIL","title":"INSTALL_FAIL  <code>module-attribute</code>","text":"<pre><code>INSTALL_FAIL = '&gt;&gt;&gt;&gt;&gt; Init Failed'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.INSTALL_PASS","title":"INSTALL_PASS  <code>module-attribute</code>","text":"<pre><code>INSTALL_PASS = '&gt;&gt;&gt;&gt;&gt; Init Succeeded'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.INSTALL_TIMEOUT","title":"INSTALL_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>INSTALL_TIMEOUT = '&gt;&gt;&gt;&gt;&gt; Init Timed Out'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.RESET_FAILED","title":"RESET_FAILED  <code>module-attribute</code>","text":"<pre><code>RESET_FAILED = '&gt;&gt;&gt;&gt;&gt; Reset Failed'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TESTS_ERROR","title":"TESTS_ERROR  <code>module-attribute</code>","text":"<pre><code>TESTS_ERROR = '&gt;&gt;&gt;&gt;&gt; Tests Errored'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TESTS_FAILED","title":"TESTS_FAILED  <code>module-attribute</code>","text":"<pre><code>TESTS_FAILED = '&gt;&gt;&gt;&gt;&gt; Some Tests Failed'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TESTS_PASSED","title":"TESTS_PASSED  <code>module-attribute</code>","text":"<pre><code>TESTS_PASSED = '&gt;&gt;&gt;&gt;&gt; All Tests Passed'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TESTS_TIMEOUT","title":"TESTS_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>TESTS_TIMEOUT = '&gt;&gt;&gt;&gt;&gt; Tests Timed Out'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.START_TEST_OUTPUT","title":"START_TEST_OUTPUT  <code>module-attribute</code>","text":"<pre><code>START_TEST_OUTPUT = '&gt;&gt;&gt;&gt;&gt; Start Test Output'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.END_TEST_OUTPUT","title":"END_TEST_OUTPUT  <code>module-attribute</code>","text":"<pre><code>END_TEST_OUTPUT = '&gt;&gt;&gt;&gt;&gt; End Test Output'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.NON_TEST_EXTS","title":"NON_TEST_EXTS  <code>module-attribute</code>","text":"<pre><code>NON_TEST_EXTS = ['.json', '.png', 'csv', '.txt', '.md', '.jpg', '.jpeg', '.pkl', '.yml', '.yaml', '.toml']\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWE_BENCH_URL_RAW","title":"SWE_BENCH_URL_RAW  <code>module-attribute</code>","text":"<pre><code>SWE_BENCH_URL_RAW = 'https://raw.githubusercontent.com/'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.DEFAULT_DOCKER_SPECS","title":"DEFAULT_DOCKER_SPECS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DOCKER_SPECS = {'conda_version': 'py311_23.11.0-2', 'node_version': '21.6.2', 'pnpm_version': '9.5.0', 'python_version': '3.9', 'ubuntu_version': '22.04'}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.FAIL_ONLY_REPOS","title":"FAIL_ONLY_REPOS  <code>module-attribute</code>","text":"<pre><code>FAIL_ONLY_REPOS = {'chartjs/Chart.js', 'processing/p5.js', 'markedjs/marked'}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_VERSION_TO_SPECS","title":"MAP_REPO_VERSION_TO_SPECS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS = {None: MAP_REPO_VERSION_TO_SPECS_C, None: MAP_REPO_VERSION_TO_SPECS_GO, None: MAP_REPO_VERSION_TO_SPECS_JAVA, None: MAP_REPO_VERSION_TO_SPECS_JS, None: MAP_REPO_VERSION_TO_SPECS_PHP, None: MAP_REPO_VERSION_TO_SPECS_PY, None: MAP_REPO_VERSION_TO_SPECS_RUBY, None: MAP_REPO_VERSION_TO_SPECS_RUST}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_INSTALL","title":"MAP_REPO_TO_INSTALL  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL = {None: MAP_REPO_TO_INSTALL_C, None: MAP_REPO_TO_INSTALL_GO, None: MAP_REPO_TO_INSTALL_JAVA, None: MAP_REPO_TO_INSTALL_JS, None: MAP_REPO_TO_INSTALL_PHP, None: MAP_REPO_TO_INSTALL_PY, None: MAP_REPO_TO_INSTALL_RUBY, None: MAP_REPO_TO_INSTALL_RUST}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.MAP_REPO_TO_EXT","title":"MAP_REPO_TO_EXT  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_EXT = {None: {k: 'c'for k in (keys())}, None: {k: 'go'for k in (keys())}, None: {k: 'java'for k in (keys())}, None: {k: 'js'for k in (keys())}, None: {k: 'php'for k in (keys())}, None: {k: 'py'for k in (keys())}, None: {k: 'rb'for k in (keys())}, None: {k: 'rs'for k in (keys())}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.LATEST","title":"LATEST  <code>module-attribute</code>","text":"<pre><code>LATEST = 'latest'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.USE_X86","title":"USE_X86  <code>module-attribute</code>","text":"<pre><code>USE_X86 = USE_X86_PY\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance","title":"SWEbenchInstance","text":"<p>               Bases: <code>TypedDict</code></p>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.repo","title":"repo  <code>instance-attribute</code>","text":"<pre><code>repo: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.instance_id","title":"instance_id  <code>instance-attribute</code>","text":"<pre><code>instance_id: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.base_commit","title":"base_commit  <code>instance-attribute</code>","text":"<pre><code>base_commit: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.patch","title":"patch  <code>instance-attribute</code>","text":"<pre><code>patch: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.test_patch","title":"test_patch  <code>instance-attribute</code>","text":"<pre><code>test_patch: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.problem_statement","title":"problem_statement  <code>instance-attribute</code>","text":"<pre><code>problem_statement: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.hints_text","title":"hints_text  <code>instance-attribute</code>","text":"<pre><code>hints_text: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.FAIL_TO_PASS","title":"FAIL_TO_PASS  <code>instance-attribute</code>","text":"<pre><code>FAIL_TO_PASS: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.PASS_TO_PASS","title":"PASS_TO_PASS  <code>instance-attribute</code>","text":"<pre><code>PASS_TO_PASS: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.SWEbenchInstance.environment_setup_commit","title":"environment_setup_commit  <code>instance-attribute</code>","text":"<pre><code>environment_setup_commit: str\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ResolvedStatus","title":"ResolvedStatus","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/harness/#swebench.harness.constants.ResolvedStatus.NO","title":"NO  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NO = 'RESOLVED_NO'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ResolvedStatus.PARTIAL","title":"PARTIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PARTIAL = 'RESOLVED_PARTIAL'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ResolvedStatus.FULL","title":"FULL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FULL = 'RESOLVED_FULL'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TestStatus","title":"TestStatus","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/harness/#swebench.harness.constants.TestStatus.FAILED","title":"FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAILED = 'FAILED'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TestStatus.PASSED","title":"PASSED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PASSED = 'PASSED'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TestStatus.SKIPPED","title":"SKIPPED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKIPPED = 'SKIPPED'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TestStatus.ERROR","title":"ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR = 'ERROR'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.TestStatus.XFAIL","title":"XFAIL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XFAIL = 'XFAIL'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.EvalType","title":"EvalType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/harness/#swebench.harness.constants.EvalType.PASS_AND_FAIL","title":"PASS_AND_FAIL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PASS_AND_FAIL = 'pass_and_fail'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.EvalType.FAIL_ONLY","title":"FAIL_ONLY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAIL_ONLY = 'fail_only'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType","title":"PatchType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/harness/#swebench.harness.constants.PatchType.PATCH_GOLD","title":"PATCH_GOLD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PATCH_GOLD = 'gold'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType.PATCH_PRED","title":"PATCH_PRED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PATCH_PRED = 'pred'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType.PATCH_PRED_TRY","title":"PATCH_PRED_TRY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PATCH_PRED_TRY = 'pred_try'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType.PATCH_PRED_MINIMAL","title":"PATCH_PRED_MINIMAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PATCH_PRED_MINIMAL = 'pred_minimal'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType.PATCH_PRED_MINIMAL_TRY","title":"PATCH_PRED_MINIMAL_TRY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PATCH_PRED_MINIMAL_TRY = 'pred_minimal_try'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType.PATCH_TEST","title":"PATCH_TEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PATCH_TEST = 'test'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.PatchType.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>swebench/harness/constants/__init__.py</code> <pre><code>def __str__(self):\n    return self.value\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.make_lombok_pre_install_script","title":"make_lombok_pre_install_script","text":"<pre><code>make_lombok_pre_install_script(tests: List[str]) -&gt; List[str]\n</code></pre> <p>There's no way to run individual tests out of the box, so this script modifies the xml file that defines test scripts to run individual tests with <code>ant test.instance</code>.</p> Source code in <code>swebench/harness/constants/java.py</code> <pre><code>def make_lombok_pre_install_script(tests: List[str]) -&gt; List[str]:\n    \"\"\"\n    There's no way to run individual tests out of the box, so this script\n    modifies the xml file that defines test scripts to run individual tests with\n    `ant test.instance`.\n    \"\"\"\n    tests_xml = \"\\n\".join(rf'&lt;test name=\"{test}\" /&gt;' for test in tests)\n    xml = rf\"\"\"\n    &lt;target name=\"test.instance\" depends=\"test.compile, test.formatter.compile\" description=\"Runs test cases for the swe-bench instance\"&gt;\n      &lt;junit printsummary=\"yes\" fork=\"true\" forkmode=\"once\" haltonfailure=\"no\"&gt;\n        &lt;formatter classname=\"lombok.ant.SimpleTestFormatter\" usefile=\"false\" unless=\"tests.quiet\" /&gt;\n        &lt;classpath location=\"build/ant\" /&gt;\n        &lt;classpath refid=\"cp.test\" /&gt;\n        &lt;classpath refid=\"cp.stripe\" /&gt;\n        &lt;classpath refid=\"packing.basedirs.path\" /&gt;\n        &lt;classpath location=\"build/tests\" /&gt;\n        &lt;classpath location=\"build/teststubs\" /&gt;\n        {tests_xml}\n      &lt;/junit&gt;\n    &lt;/target&gt;\n    \"\"\"\n    build_file = \"buildScripts/tests.ant.xml\"\n    escaped_xml = shlex.quote(xml.strip())\n\n    return [\n        f\"{{ head -n -1 {build_file}; echo {escaped_xml}; tail -n 1 {build_file}; }} &gt; temp_file &amp;&amp; mv temp_file {build_file}\"\n    ]\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.make_lucene_pre_install_script","title":"make_lucene_pre_install_script","text":"<pre><code>make_lucene_pre_install_script() -&gt; List[str]\n</code></pre> <p>This script modifies the gradle config to print all test results, including passing tests.</p> Source code in <code>swebench/harness/constants/java.py</code> <pre><code>def make_lucene_pre_install_script() -&gt; List[str]:\n    \"\"\"\n    This script modifies the gradle config to print all test results, including\n    passing tests.\n    \"\"\"\n    gradle_file = \"gradle/testing/defaults-tests.gradle\"\n\n    new_content = \"\"\"testLogging {\n  showStandardStreams = true\n  // set options for log level LIFECYCLE\n  events TestLogEvent.FAILED,\n         TestLogEvent.PASSED,\n         TestLogEvent.SKIPPED,\n         TestLogEvent.STANDARD_OUT\n  exceptionFormat TestExceptionFormat.FULL\n  showExceptions true\n  showCauses true\n  showStackTraces true\n\n  // set options for log level DEBUG and INFO\n  debug {\n      events TestLogEvent.STARTED,\n             TestLogEvent.FAILED,\n             TestLogEvent.PASSED,\n             TestLogEvent.SKIPPED,\n             TestLogEvent.STANDARD_ERROR,\n             TestLogEvent.STANDARD_OUT\n      exceptionFormat TestExceptionFormat.FULL\n  }\n  info.events = debug.events\n  info.exceptionFormat = debug.exceptionFormat\n\n  afterSuite { desc, result -&gt;\n      if (!desc.parent) { // will match the outermost suite\n          def output = \"Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} passed, ${result.failedTestCount} failed, ${result.skippedTestCount} skipped)\"\n          def startItem = '|  ', endItem = '  |'\n          def repeatLength = startItem.length() + output.length() + endItem.length()\n          println('\\\\n' + ('-' * repeatLength) + '\\\\n' + startItem + output + endItem + '\\\\n' + ('-' * repeatLength))\n      }\n  }\n}\"\"\"\n\n    return [\n        f\"\"\"\nsed -i '\n/testLogging {{/,/}}/{{\n  /testLogging {{/r /dev/stdin\n  d\n}}\n' {gradle_file} &lt;&lt; 'EOF'\n{new_content}\nEOF\n\"\"\".strip()\n    ]\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.make_rxjava_pre_install_script","title":"make_rxjava_pre_install_script","text":"<pre><code>make_rxjava_pre_install_script() -&gt; List[str]\n</code></pre> <p>This script modifies the gradle config to print all test results, including passing tests.</p> Source code in <code>swebench/harness/constants/java.py</code> <pre><code>def make_rxjava_pre_install_script() -&gt; List[str]:\n    \"\"\"\n    This script modifies the gradle config to print all test results, including\n    passing tests.\n    \"\"\"\n    gradle_file = \"build.gradle\"\n\n    new_content = \"\"\"testLogging {\n    outputs.upToDateWhen { false }\n    showStandardStreams = true\n    showStackTraces = true\n\n    // Show output for all logging levels\n    events = ['passed', 'skipped', 'failed', 'standardOut', 'standardError']\n\n    // set options for log level LIFECYCLE\n    events org.gradle.api.tasks.testing.logging.TestLogEvent.FAILED,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.PASSED,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.SKIPPED,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_OUT,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_ERROR\n    exceptionFormat org.gradle.api.tasks.testing.logging.TestExceptionFormat.FULL\n    showExceptions true\n    showCauses true\n    showStackTraces true\n\n    // set options for log level DEBUG and INFO\n    debug {\n        events org.gradle.api.tasks.testing.logging.TestLogEvent.STARTED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.FAILED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.PASSED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.SKIPPED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_ERROR,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_OUT\n        exceptionFormat org.gradle.api.tasks.testing.logging.TestExceptionFormat.FULL\n    }\n    info.events = debug.events\n    info.exceptionFormat = debug.exceptionFormat\n\n    afterSuite { desc, result -&gt;\n        if (!desc.parent) { // will match the outermost suite\n            def output = \"Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} passed, ${result.failedTestCount} failed, ${result.skippedTestCount} skipped)\"\n            def startItem = '|  ', endItem = '  |'\n            def repeatLength = startItem.length() + output.length() + endItem.length()\n            println('\\\\n' + ('-' * repeatLength) + '\\\\n' + startItem + output + endItem + '\\\\n' + ('-' * repeatLength))\n        }\n    }\n}\"\"\"\n\n    return [\n        f\"\"\"\nsed -i '\n/testLogging {{/,/}}/{{\n  /testLogging {{/r /dev/stdin\n  d\n}}\n' {gradle_file} &lt;&lt; 'EOF'\n{new_content}\nEOF\n\"\"\".strip()\n    ]\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c","title":"c","text":""},{"location":"api/harness/#swebench.harness.constants.c.SPECS_REDIS","title":"SPECS_REDIS  <code>module-attribute</code>","text":"<pre><code>SPECS_REDIS = {'13115': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/scripting']}, '12472': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/acl --only \"/.*ACL GETUSER.*\"']}, '12272': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/string --only \"/.*(GETRANGE|SETRANGE).*\"']}, '11734': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/bitops']}, '10764': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/zset --only \"BZMPOP\"']}, '10095': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/list --only \"/.*(LPOP|RPOP)\"']}, '9733': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/introspection-2']}, '10068': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/stream --only \"/*XTRIM*\"']}, '11631': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/geo --only \"/.*GEOSEARCH .*\"']}, '11510': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/introspection --only \"/.*MONITOR.*\"']}, '11279': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/acl']}, '13338': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/type/stream-cgroups']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.SPECS_JQ","title":"SPECS_JQ  <code>module-attribute</code>","text":"<pre><code>SPECS_JQ = {None: {k: {'build': ['git submodule update --init', 'autoreconf -fi', './configure --with-oniguruma=builtin', 'make clean', 'touch src/parser.y src/lexer.l', 'make -j$(nproc)'], 'test_cmd': ['make check']}for k in ['2839', '2650', '2235', '2658', '2750', '2681', '2919', '2598', '2728']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.SPECS_JSON","title":"SPECS_JSON  <code>module-attribute</code>","text":"<pre><code>SPECS_JSON = {'4237': {'build': ['mkdir -p build', 'cd build', 'cmake ..', 'make test-udt_cpp11', 'cd ..'], 'test_cmd': ['./build/tests/test-udt_cpp11 -s -r=xml']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.SPECS_MICROPYTHON","title":"SPECS_MICROPYTHON  <code>module-attribute</code>","text":"<pre><code>SPECS_MICROPYTHON = {'15898': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/ports/unix/ffi_lib.so tests/ports/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i string_format']}, '13569': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/ports/unix/ffi_lib.so tests/ports/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i try']}, '13039': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/unix/ffi_lib.so tests/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i slice']}, '12158': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate'], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard', 'gcc -shared -o tests/unix/ffi_lib.so tests/unix/ffi_lib.c'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -d thread']}, '10095': {'pre_install': ['python -m venv .venv', 'source .venv/bin/activate', \"sed -i 's/uint mp_import_stat/mp_import_stat_t mp_import_stat/' mpy-cross/main.c\"], 'build': ['source ./tools/ci.sh', 'ci_unix_build_helper VARIANT=standard'], 'test_cmd': ['cd tests', 'MICROPY_CPYTHON3=python3 MICROPY_MICROPYTHON=../ports/unix/build-standard/micropython ./run-tests.py -i basics/fun']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.SPECS_VALKEY","title":"SPECS_VALKEY  <code>module-attribute</code>","text":"<pre><code>SPECS_VALKEY = {'928': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/cluster/replica-migration --only \"/.*NOREPLICAS.*\"']}, '790': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/cluster/cluster-shards']}, '1499': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/introspection-2']}, '1842': {'build': ['make distclean', 'make'], 'test_cmd': ['TERM=dumb ./runtest --durable --single unit/acl --only \"/.*ACL LOAD.*\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.SPECS_FMT","title":"SPECS_FMT  <code>module-attribute</code>","text":"<pre><code>SPECS_FMT = {None: {k: {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target ranges-test'], 'test_cmd': ['ctest --test-dir build -V -R ranges-test']}for k in ['3863', '3158', '2457']}, None: {k: {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target format-test'], 'test_cmd': ['ctest --test-dir build -V -R format-test']}for k in ['3901', '3750', '3248', '2317', '2310']}, '3272': {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target xchar-test'], 'test_cmd': ['ctest --test-dir build -V -R xchar-test']}, '3729': {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target std-test'], 'test_cmd': ['ctest --test-dir build -V -R std-test']}, '1683': {'build': ['mkdir -p build', 'cmake -B build -S .', 'cmake --build build --parallel $(nproc) --target printf-test'], 'test_cmd': ['ctest --test-dir build -V -R printf-test']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.MAP_REPO_VERSION_TO_SPECS_C","title":"MAP_REPO_VERSION_TO_SPECS_C  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_C = {'redis/redis': SPECS_REDIS, 'jqlang/jq': SPECS_JQ, 'nlohmann/json': SPECS_JSON, 'micropython/micropython': SPECS_MICROPYTHON, 'valkey-io/valkey': SPECS_VALKEY, 'fmtlib/fmt': SPECS_FMT}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.c.MAP_REPO_TO_INSTALL_C","title":"MAP_REPO_TO_INSTALL_C  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_C = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go","title":"go","text":""},{"location":"api/harness/#swebench.harness.constants.go.SPECS_CADDY","title":"SPECS_CADDY  <code>module-attribute</code>","text":"<pre><code>SPECS_CADDY = {'6411': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go mod tidy'], 'test_cmd': ['go test -v . -run \"TestReplacerNew*\"']}, '6345': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration'], 'test_cmd': ['go test -v ./caddytest/integration']}, '6115': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./modules/caddyhttp/reverseproxy'], 'test_cmd': ['go test -v ./modules/caddyhttp/reverseproxy']}, '6051': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddyconfig/caddyfile'], 'test_cmd': ['go test -v ./caddyconfig/caddyfile']}, '5404': {'docker_specs': {'go_version': '1.20.14'}, 'install': ['go test -c ./caddyconfig/caddyfile'], 'test_cmd': ['go test -v ./caddyconfig/caddyfile']}, '6370': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./cmd'], 'test_cmd': ['go test -v ./cmd']}, '6350': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration -run \"TestCaddyfileAdapt*\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"TestCaddyfileAdapt*\"']}, '6288': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration -run \"TestCaddyfileAdapt*\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"TestCaddyfileAdapt*\"']}, '5995': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddytest/integration -run \"^TestUriReplace\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"^TestUriReplace\"']}, '4943': {'docker_specs': {'go_version': '1.18.10'}, 'install': ['go test -c ./modules/logging'], 'test_cmd': ['go test -v ./modules/logging']}, '5626': {'docker_specs': {'go_version': '1.19.13'}, 'install': ['go test -c ./caddyconfig/httpcaddyfile -run \"Test.*Import\"'], 'test_cmd': ['go test -v ./caddyconfig/httpcaddyfile -run \"Test.*Import\"']}, '5761': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./caddyconfig/caddyfile -run \"TestLexer.*\"'], 'test_cmd': ['go test -v ./caddyconfig/caddyfile -run \"TestLexer.*\"']}, '5870': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c . -run \"TestUnsyncedConfigAccess\"'], 'test_cmd': ['go test -v . -run \"TestUnsyncedConfigAccess\"']}, '4774': {'docker_specs': {'go_version': '1.18.10'}, 'install': ['go test -c ./caddytest/integration -run \"TestCaddyfileAdapt*\"'], 'test_cmd': ['go test -v ./caddytest/integration -run \"TestCaddyfileAdapt*\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go.SPECS_TERRAFORM","title":"SPECS_TERRAFORM  <code>module-attribute</code>","text":"<pre><code>SPECS_TERRAFORM = {'35611': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./internal/terraform'], 'test_cmd': ['go test -v ./internal/terraform -run \"^TestContext2Apply_provisioner\"']}, '35543': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./internal/terraform'], 'test_cmd': ['go test -v ./internal/terraform -run \"^TestContext2Plan_import\"']}, '34900': {'docker_specs': {'go_version': '1.22.12'}, 'install': ['go test -c ./internal/terraform'], 'test_cmd': ['go test -v ./internal/terraform -run \"(^TestContext2Apply|^TestContext2Plan).*[Ss]ensitive\"']}, '34580': {'docker_specs': {'go_version': '1.21.13'}, 'install': ['go test -c ./internal/command'], 'test_cmd': ['go test -v ./internal/command -run \"^TestFmt\"']}, '34814': {'docker_specs': {'go_version': '1.22.12'}, 'install': ['go test -c ./internal/builtin/provisioners/remote-exec'], 'test_cmd': ['go test -v ./internal/builtin/provisioners/remote-exec']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go.SPECS_PROMETHEUS","title":"SPECS_PROMETHEUS  <code>module-attribute</code>","text":"<pre><code>SPECS_PROMETHEUS = {'14861': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql'], 'test_cmd': ['go test -v ./promql -run \"^TestEngine\"']}, '13845': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql ./model/labels'], 'test_cmd': ['go test -v ./promql ./model/labels -run \"^(TestRangeQuery|TestLabels)\"']}, '12874': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tsdb'], 'test_cmd': ['go test -v ./tsdb -run \"^TestHead\"']}, '11859': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tsdb'], 'test_cmd': ['go test -v ./tsdb -run \"^TestSnapshot\"']}, '10720': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql'], 'test_cmd': ['go test -v ./promql -run \"^TestEvaluations\"']}, '10633': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./discovery/puppetdb'], 'test_cmd': ['go test -v ./discovery/puppetdb -run \"TestPuppetDBRefreshWithParameters\"']}, '9248': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./promql'], 'test_cmd': ['go test -v ./promql -run \"^TestEvaluations\"']}, '15142': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tsdb'], 'test_cmd': ['go test -v ./tsdb -run \"^TestHead\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go.SPECS_HUGO","title":"SPECS_HUGO  <code>module-attribute</code>","text":"<pre><code>SPECS_HUGO = {'12768': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./markup/goldmark/blockquotes/...'], 'test_cmd': ['go test -v ./markup/goldmark/blockquotes/...']}, '12579': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./resources/page'], 'test_cmd': ['go test -v ./resources/page -run \"^TestGroupBy\"']}, '12562': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./hugolib/...'], 'test_cmd': ['go test -v ./hugolib/... -run \"^TestGetPage[^/]\"']}, '12448': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./hugolib/...'], 'test_cmd': ['go test -v ./hugolib/... -run \"^TestRebuild\"']}, '12343': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./resources/page/...'], 'test_cmd': ['go test -v ./resources/page/... -run \"^Test.*Permalink\"']}, '12204': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./tpl/tplimpl'], 'test_cmd': ['go test -v ./tpl/tplimpl -run \"^TestEmbedded\"']}, '12171': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./hugolib'], 'test_cmd': ['go test -v ./hugolib -run \"^Test.*Pages\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go.SPECS_GIN","title":"SPECS_GIN  <code>module-attribute</code>","text":"<pre><code>SPECS_GIN = {'4003': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test . -v -run \"TestMethodNotAllowedNoRoute\"']}, '3820': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./binding'], 'test_cmd': ['go test -v ./binding -run \"^TestMapping\"']}, '3741': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestColor\"']}, '2755': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestTree\"']}, '3227': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestRedirect\"']}, '2121': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c ./...'], 'test_cmd': ['go test -v ./... -run \"^Test.*Reader\"']}, '1957': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^TestContext.*Bind\"']}, '1805': {'docker_specs': {'go_version': '1.23.8'}, 'install': ['go test -c .'], 'test_cmd': ['go test -v . -run \"^Test.*Router\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go.MAP_REPO_VERSION_TO_SPECS_GO","title":"MAP_REPO_VERSION_TO_SPECS_GO  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_GO = {'caddyserver/caddy': SPECS_CADDY, 'hashicorp/terraform': SPECS_TERRAFORM, 'prometheus/prometheus': SPECS_PROMETHEUS, 'gohugoio/hugo': SPECS_HUGO, 'gin-gonic/gin': SPECS_GIN}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.go.MAP_REPO_TO_INSTALL_GO","title":"MAP_REPO_TO_INSTALL_GO  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_GO = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java","title":"java","text":""},{"location":"api/harness/#swebench.harness.constants.java.SPECS_GSON","title":"SPECS_GSON  <code>module-attribute</code>","text":"<pre><code>SPECS_GSON = {'2158': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testByteSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testShortSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testIntSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testLongSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testFloatSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testDoubleSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testPrimitiveIntegerAutoboxedSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testPrimitiveIntegerAutoboxedInASingleElementArraySerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testReallyLongValuesSerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.PrimitiveTest#testPrimitiveLongAutoboxedSerialization']}, '2024': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.FieldNamingTest#testUpperCaseWithUnderscores', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.NamingPolicyTest#testGsonWithUpperCaseUnderscorePolicySerialization', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.functional.NamingPolicyTest#testGsonWithUpperCaseUnderscorePolicyDeserialiation']}, '2479': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.GsonBuilderTest#testRegisterTypeAdapterForObjectAndJsonElements', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.GsonBuilderTest#testRegisterTypeHierarchyAdapterJsonElements', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.GsonBuilderTest#testModificationAfterCreate']}, '2134': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.util.ISO8601UtilsTest#testDateParseInvalidDay', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.util.ISO8601UtilsTest#testDateParseInvalidMonth', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.util.ISO8601UtilsTest#testDateParseWithDefaultTimezone']}, '2061': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonReaderTest#testHasNextEndOfDocument', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testHasNext_endOfDocument', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonReaderTest#testReadEmptyObject', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonReaderTest#testReadEmptyArray', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_emptyJsonObject', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_filledJsonObject']}, '2311': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.JsonPrimitiveTest#testEqualsIntegerAndBigInteger', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.JsonPrimitiveTest#testLongEqualsBigInteger', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.JsonPrimitiveTest#testEqualsAcrossTypes']}, '1100': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.DefaultDateTypeAdapterTest#testNullValue', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.DefaultDateTypeAdapterTest#testDatePattern', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.DefaultDateTypeAdapterTest#testInvalidDatePattern']}, '1093': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteDoublesWhenLenient', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteBoxedDoublesWhenLenient', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteDoubles', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testNonFiniteBoxedDoubles', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.stream.JsonWriterTest#testDoubles']}, '1014': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl gson -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_emptyJsonObject', 'mvnd test -B -T 1C -pl gson -Dtest=com.google.gson.internal.bind.JsonTreeReaderTest#testSkipValue_filledJsonObject']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.SPECS_DRUID","title":"SPECS_DRUID  <code>module-attribute</code>","text":"<pre><code>SPECS_DRUID = {'15402': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl processing -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.query.groupby.GroupByQueryQueryToolChestTest#testCacheStrategy', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.query.groupby.GroupByQueryQueryToolChestTest#testResultLevelCacheKeyWithSubTotalsSpec', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.query.groupby.GroupByQueryQueryToolChestTest#testMultiColumnCacheStrategy']}, '14092': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl processing,cloud/aws-common,cloud/gcp-common -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl server -Dtest=org.apache.druid.discovery.DruidLeaderClientTest#test503ResponseFromServerAndCacheRefresh', 'mvnd test -B -T 1C -pl server -Dtest=org.apache.druid.discovery.DruidLeaderClientTest#testServerFailureAndRedirect']}, '14136': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl processing -DskipTests -am'], 'test_cmd': ['mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval2', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval3', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirstZeroLengthInterval4', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapFirstContainsSecond', 'mvnd test -B -T 1C -pl processing -Dtest=org.apache.druid.timeline.VersionedIntervalTimelineTest#testOverlapSecondContainsFirst']}, '13704': {'docker_specs': {'java_version': '11'}, 'install': [\"sed -i 's/&lt;resourceBundle&gt;org.apache.apache.resources:apache-jar-resource-bundle:1.5-SNAPSHOT&lt;\\\\/resourceBundle&gt;/&lt;resourceBundle&gt;org.apache.apache.resources:apache-jar-resource-bundle:1.5&lt;\\\\/resourceBundle&gt;/' pom.xml\", 'mvn clean install -B -pl processing -DskipTests -am'], 'test_cmd': ['mvnd test -B -pl processing -Dtest=org.apache.druid.query.aggregation.post.ArithmeticPostAggregatorTest#testPow', 'mvnd test -B -pl processing -Dtest=org.apache.druid.query.aggregation.post.ArithmeticPostAggregatorTest#testDiv', 'mvnd test -B -pl processing -Dtest=org.apache.druid.query.aggregation.post.ArithmeticPostAggregatorTest#testQuotient']}, '16875': {'docker_specs': {'java_version': '11'}, 'install': ['mvn clean install -B -pl server -DskipTests -am'], 'test_cmd': ['mvnd test -B -pl server -Dtest=org.apache.druid.server.metrics.WorkerTaskCountStatsMonitorTest#testMonitorWithPeon', 'mvnd test -B -pl server -Dtest=org.apache.druid.server.metrics.WorkerTaskCountStatsMonitorTest#testMonitorWithNulls', 'mvnd test -B -pl server -Dtest=org.apache.druid.server.metrics.WorkerTaskCountStatsMonitorTest#testMonitorIndexer']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.SPECS_JAVAPARSER","title":"SPECS_JAVAPARSER  <code>module-attribute</code>","text":"<pre><code>SPECS_JAVAPARSER = {'4561': {'docker_specs': {'java_version': '17'}, 'build': ['./mvnw clean install -B -pl javaparser-symbol-solver-testing -DskipTests -am'], 'test_cmd': ['./mvnw test -B -pl javaparser-symbol-solver-testing -Dtest=Issue4560Test', './mvnw test -B -pl javaparser-symbol-solver-testing -Dtest=JavaSymbolSolverTest']}, '4538': {'docker_specs': {'java_version': '17'}, 'build': ['./mvnw clean install -B -pl javaparser-core-testing -DskipTests -am'], 'test_cmd': ['./mvnw test -B -pl javaparser-core-testing -Dtest=NodeTest', './mvnw test -B -pl javaparser-core-testing -Dtest=NodePositionTest']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.SPECS_LOMBOK","title":"SPECS_LOMBOK  <code>module-attribute</code>","text":"<pre><code>SPECS_LOMBOK = {'3602': {'docker_specs': {'java_version': '11'}, 'pre_install': make_lombok_pre_install_script(['lombok.bytecode.TestPostCompiler']), 'build': ['ant test.compile'], 'test_cmd': ['ant test.instance']}, None: {k: {'docker_specs': {'java_version': '11'}, 'pre_install': make_lombok_pre_install_script(['lombok.transform.TestWithDelombok']), 'build': ['ant test.compile'], 'test_cmd': ['ant test.instance']}for k in ['3312', '3697', '3326', '3674', '3594', '3422', '3215', '3486', '3042', '3052', '2792']}, None: {k: {'docker_specs': {'java_version': '17'}, 'pre_install': make_lombok_pre_install_script(['lombok.transform.TestWithDelombok']), 'build': ['ant test.compile'], 'test_cmd': ['ant test.instance']}for k in ['3571', '3479', '3371', '3350', '3009']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.SPECS_LUCENE","title":"SPECS_LUCENE  <code>module-attribute</code>","text":"<pre><code>SPECS_LUCENE = {'13494': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.facet.TestStringValueFacetCounts']}, '13704': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.search.TestLatLonDocValuesQueries']}, '13301': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests TestXYPoint.testEqualsAndHashCode -Dtests.seed=3ABEFE4D876DD310 -Dtests.nightly=true -Dtests.locale=es-419 -Dtests.timezone=Asia/Ulaanbaatar -Dtests.asserts=true -Dtests.file.encoding=UTF-8']}, '12626': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.index.TestIndexWriter']}, '12212': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.facet.TestDrillSideways']}, '13170': {'docker_specs': {'java_version': '21'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.analysis.opennlp.TestOpenNLPSentenceBreakIterator']}, '12196': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.queryparser.classic.TestMultiFieldQueryParser']}, '12022': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.document.TestLatLonShape']}, '11760': {'docker_specs': {'java_version': '17'}, 'pre_install': make_lucene_pre_install_script(), 'test_cmd': ['./gradlew test --tests org.apache.lucene.queries.intervals.TestIntervalBuilder']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.SPECS_RXJAVA","title":"SPECS_RXJAVA  <code>module-attribute</code>","text":"<pre><code>SPECS_RXJAVA = {'7597': {'docker_specs': {'java_version': '11'}, 'pre_install': make_rxjava_pre_install_script(), 'test_cmd': ['./gradlew test --tests io.reactivex.rxjava3.internal.operators.observable.ObservableSwitchTest']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.MAP_REPO_VERSION_TO_SPECS_JAVA","title":"MAP_REPO_VERSION_TO_SPECS_JAVA  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_JAVA = {'google/gson': SPECS_GSON, 'apache/druid': SPECS_DRUID, 'javaparser/javaparser': SPECS_JAVAPARSER, 'projectlombok/lombok': SPECS_LOMBOK, 'apache/lucene': SPECS_LUCENE, 'reactivex/rxjava': SPECS_RXJAVA}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.MAP_REPO_TO_INSTALL_JAVA","title":"MAP_REPO_TO_INSTALL_JAVA  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_JAVA = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.make_lombok_pre_install_script","title":"make_lombok_pre_install_script","text":"<pre><code>make_lombok_pre_install_script(tests: List[str]) -&gt; List[str]\n</code></pre> <p>There's no way to run individual tests out of the box, so this script modifies the xml file that defines test scripts to run individual tests with <code>ant test.instance</code>.</p> Source code in <code>swebench/harness/constants/java.py</code> <pre><code>def make_lombok_pre_install_script(tests: List[str]) -&gt; List[str]:\n    \"\"\"\n    There's no way to run individual tests out of the box, so this script\n    modifies the xml file that defines test scripts to run individual tests with\n    `ant test.instance`.\n    \"\"\"\n    tests_xml = \"\\n\".join(rf'&lt;test name=\"{test}\" /&gt;' for test in tests)\n    xml = rf\"\"\"\n    &lt;target name=\"test.instance\" depends=\"test.compile, test.formatter.compile\" description=\"Runs test cases for the swe-bench instance\"&gt;\n      &lt;junit printsummary=\"yes\" fork=\"true\" forkmode=\"once\" haltonfailure=\"no\"&gt;\n        &lt;formatter classname=\"lombok.ant.SimpleTestFormatter\" usefile=\"false\" unless=\"tests.quiet\" /&gt;\n        &lt;classpath location=\"build/ant\" /&gt;\n        &lt;classpath refid=\"cp.test\" /&gt;\n        &lt;classpath refid=\"cp.stripe\" /&gt;\n        &lt;classpath refid=\"packing.basedirs.path\" /&gt;\n        &lt;classpath location=\"build/tests\" /&gt;\n        &lt;classpath location=\"build/teststubs\" /&gt;\n        {tests_xml}\n      &lt;/junit&gt;\n    &lt;/target&gt;\n    \"\"\"\n    build_file = \"buildScripts/tests.ant.xml\"\n    escaped_xml = shlex.quote(xml.strip())\n\n    return [\n        f\"{{ head -n -1 {build_file}; echo {escaped_xml}; tail -n 1 {build_file}; }} &gt; temp_file &amp;&amp; mv temp_file {build_file}\"\n    ]\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.make_lucene_pre_install_script","title":"make_lucene_pre_install_script","text":"<pre><code>make_lucene_pre_install_script() -&gt; List[str]\n</code></pre> <p>This script modifies the gradle config to print all test results, including passing tests.</p> Source code in <code>swebench/harness/constants/java.py</code> <pre><code>def make_lucene_pre_install_script() -&gt; List[str]:\n    \"\"\"\n    This script modifies the gradle config to print all test results, including\n    passing tests.\n    \"\"\"\n    gradle_file = \"gradle/testing/defaults-tests.gradle\"\n\n    new_content = \"\"\"testLogging {\n  showStandardStreams = true\n  // set options for log level LIFECYCLE\n  events TestLogEvent.FAILED,\n         TestLogEvent.PASSED,\n         TestLogEvent.SKIPPED,\n         TestLogEvent.STANDARD_OUT\n  exceptionFormat TestExceptionFormat.FULL\n  showExceptions true\n  showCauses true\n  showStackTraces true\n\n  // set options for log level DEBUG and INFO\n  debug {\n      events TestLogEvent.STARTED,\n             TestLogEvent.FAILED,\n             TestLogEvent.PASSED,\n             TestLogEvent.SKIPPED,\n             TestLogEvent.STANDARD_ERROR,\n             TestLogEvent.STANDARD_OUT\n      exceptionFormat TestExceptionFormat.FULL\n  }\n  info.events = debug.events\n  info.exceptionFormat = debug.exceptionFormat\n\n  afterSuite { desc, result -&gt;\n      if (!desc.parent) { // will match the outermost suite\n          def output = \"Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} passed, ${result.failedTestCount} failed, ${result.skippedTestCount} skipped)\"\n          def startItem = '|  ', endItem = '  |'\n          def repeatLength = startItem.length() + output.length() + endItem.length()\n          println('\\\\n' + ('-' * repeatLength) + '\\\\n' + startItem + output + endItem + '\\\\n' + ('-' * repeatLength))\n      }\n  }\n}\"\"\"\n\n    return [\n        f\"\"\"\nsed -i '\n/testLogging {{/,/}}/{{\n  /testLogging {{/r /dev/stdin\n  d\n}}\n' {gradle_file} &lt;&lt; 'EOF'\n{new_content}\nEOF\n\"\"\".strip()\n    ]\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.java.make_rxjava_pre_install_script","title":"make_rxjava_pre_install_script","text":"<pre><code>make_rxjava_pre_install_script() -&gt; List[str]\n</code></pre> <p>This script modifies the gradle config to print all test results, including passing tests.</p> Source code in <code>swebench/harness/constants/java.py</code> <pre><code>def make_rxjava_pre_install_script() -&gt; List[str]:\n    \"\"\"\n    This script modifies the gradle config to print all test results, including\n    passing tests.\n    \"\"\"\n    gradle_file = \"build.gradle\"\n\n    new_content = \"\"\"testLogging {\n    outputs.upToDateWhen { false }\n    showStandardStreams = true\n    showStackTraces = true\n\n    // Show output for all logging levels\n    events = ['passed', 'skipped', 'failed', 'standardOut', 'standardError']\n\n    // set options for log level LIFECYCLE\n    events org.gradle.api.tasks.testing.logging.TestLogEvent.FAILED,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.PASSED,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.SKIPPED,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_OUT,\n           org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_ERROR\n    exceptionFormat org.gradle.api.tasks.testing.logging.TestExceptionFormat.FULL\n    showExceptions true\n    showCauses true\n    showStackTraces true\n\n    // set options for log level DEBUG and INFO\n    debug {\n        events org.gradle.api.tasks.testing.logging.TestLogEvent.STARTED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.FAILED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.PASSED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.SKIPPED,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_ERROR,\n               org.gradle.api.tasks.testing.logging.TestLogEvent.STANDARD_OUT\n        exceptionFormat org.gradle.api.tasks.testing.logging.TestExceptionFormat.FULL\n    }\n    info.events = debug.events\n    info.exceptionFormat = debug.exceptionFormat\n\n    afterSuite { desc, result -&gt;\n        if (!desc.parent) { // will match the outermost suite\n            def output = \"Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} passed, ${result.failedTestCount} failed, ${result.skippedTestCount} skipped)\"\n            def startItem = '|  ', endItem = '  |'\n            def repeatLength = startItem.length() + output.length() + endItem.length()\n            println('\\\\n' + ('-' * repeatLength) + '\\\\n' + startItem + output + endItem + '\\\\n' + ('-' * repeatLength))\n        }\n    }\n}\"\"\"\n\n    return [\n        f\"\"\"\nsed -i '\n/testLogging {{/,/}}/{{\n  /testLogging {{/r /dev/stdin\n  d\n}}\n' {gradle_file} &lt;&lt; 'EOF'\n{new_content}\nEOF\n\"\"\".strip()\n    ]\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript","title":"javascript","text":""},{"location":"api/harness/#swebench.harness.constants.javascript.TEST_XVFB_PREFIX","title":"TEST_XVFB_PREFIX  <code>module-attribute</code>","text":"<pre><code>TEST_XVFB_PREFIX = 'xvfb-run --server-args=\"-screen 0 1280x1024x24 -ac :99\"'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.XVFB_DEPS","title":"XVFB_DEPS  <code>module-attribute</code>","text":"<pre><code>XVFB_DEPS = ['python3', 'python3-pip', 'xvfb', 'x11-xkb-utils', 'xfonts-100dpi', 'xfonts-75dpi', 'xfonts-scalable', 'xfonts-cyrillic', 'x11-apps', 'firefox']\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.X11_DEPS","title":"X11_DEPS  <code>module-attribute</code>","text":"<pre><code>X11_DEPS = ['libx11-xcb1', 'libxcomposite1', 'libxcursor1', 'libxdamage1', 'libxi6', 'libxtst6', 'libnss3', 'libcups2', 'libxss1', 'libxrandr2', 'libasound2', 'libatk1.0-0', 'libgtk-3-0', 'x11-utils']\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_CALYPSO","title":"SPECS_CALYPSO  <code>module-attribute</code>","text":"<pre><code>SPECS_CALYPSO = {None: {k: {'apt-pkgs': ['libsass-dev', 'sassc'], 'install': ['npm install --unsafe-perm'], 'test_cmd': 'npm run test-client', 'docker_specs': {'node_version': k}}for k in ['0.8', '4.2.3', '4.3.0', '5.10.1', '5.11.1', '6.1.0', '6.7.0', '6.9.0', '6.9.1', '6.9.4', '6.10.0', '6.10.2', '6.10.3', '6.11.1', '6.11.2', '6.11.5', '8.9.1', '8.9.3', '8.9.4', '8.11.0', '8.11.2', '10.4.1', '10.5.0', '10.6.0', '10.9.0', '10.10.0', '10.12.0', '10.13.0', '10.14.0', '10.15.2', '10.16.3']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.TEST_CHART_JS_TEMPLATE","title":"TEST_CHART_JS_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>TEST_CHART_JS_TEMPLATE = './node_modules/.bin/cross-env NODE_ENV=test ./node_modules/.bin/karma start {} --single-run --coverage --grep --auto-watch false'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_CHART_JS","title":"SPECS_CHART_JS  <code>module-attribute</code>","text":"<pre><code>SPECS_CHART_JS = {None: {k: {'install': ['pnpm install', 'pnpm run build'], 'test_cmd': ['pnpm install', 'pnpm run build', f'{TEST_XVFB_PREFIX} su chromeuser -c \"{format('./karma.conf.cjs')}\"'], 'docker_specs': {'node_version': '21.6.2', 'pnpm_version': '7.9.0', 'run_args': {'cap_add': ['SYS_ADMIN']}}}for k in ['4.0', '4.1', '4.2', '4.3', '4.4']}, None: {k: {'install': ['npm install'], 'test_cmd': ['npm install', 'npm run build', f'{TEST_XVFB_PREFIX} su chromeuser -c \"{format('./karma.conf.js')}\"'], 'docker_specs': {'node_version': '21.6.2', 'run_args': {'cap_add': ['SYS_ADMIN']}}}for k in ['3.0', '3.1', '3.2', '3.3', '3.4', '3.5', '3.6', '3.7', '3.8']}, None: {k: {'install': ['npm install', 'npm install -g gulp-cli'], 'test_cmd': ['npm install', 'gulp build', TEST_XVFB_PREFIX + ' su chromeuser -c \"gulp test\"'], 'docker_specs': {'node_version': '21.6.2', 'run_args': {'cap_add': ['SYS_ADMIN']}}}for k in ['2.0', '2.1', '2.2', '2.3', '2.4', '2.5', '2.6', '2.7', '2.8', '2.9']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_MARKED","title":"SPECS_MARKED  <code>module-attribute</code>","text":"<pre><code>SPECS_MARKED = {None: {k: {'install': ['npm install'], 'test_cmd': './node_modules/.bin/jasmine --no-color --config=jasmine.json', 'docker_specs': {'node_version': '12.22.12'}}for k in ['0.3', '0.5', '0.6', '0.7', '1.0', '1.1', '1.2', '2.0', '3.9', '4.0', '4.1', '5.0']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_P5_JS","title":"SPECS_P5_JS  <code>module-attribute</code>","text":"<pre><code>SPECS_P5_JS = {None: {k: {'apt-pkgs': X11_DEPS, 'install': ['npm install', \"PUPPETEER_SKIP_CHROMIUM_DOWNLOAD='' node node_modules/puppeteer/install.js\", './node_modules/.bin/grunt yui'], 'test_cmd': \"sed -i 's/concurrency:[[:space:]]*[0-9][0-9]*/concurrency: 1/g' Gruntfile.js\\nstdbuf -o 1M ./node_modules/.bin/grunt test --quiet --force\", 'docker_specs': {'node_version': '14.17.3'}}for k in ['0.10', '0.2', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', '1.9']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_REACT_PDF","title":"SPECS_REACT_PDF  <code>module-attribute</code>","text":"<pre><code>SPECS_REACT_PDF = {None: {k: {'apt-pkgs': ['pkg-config', 'build-essential', 'libpixman-1-0', 'libpixman-1-dev', 'libcairo2-dev', 'libpango1.0-dev', 'libjpeg-dev', 'libgif-dev', 'librsvg2-dev'] + X11_DEPS, 'install': ['npm i -g yarn', 'yarn install'], 'test_cmd': 'NODE_OPTIONS=\"--experimental-vm-modules\" ./node_modules/.bin/jest --no-color', 'docker_specs': {'node_version': '18.20.4'}}for k in ['1.0', '1.1', '1.2', '2.0']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.JEST_JSON_JQ_TRANSFORM","title":"JEST_JSON_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>JEST_JSON_JQ_TRANSFORM = 'jq -r \\'.testResults[].assertionResults[] | \"[\" + (.status | ascii_upcase) + \"] \" + ((.ancestorTitles | join(\" &gt; \")) + (if .ancestorTitles | length &gt; 0 then \" &gt; \" else \"\" end) + .title)\\''\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_BABEL","title":"SPECS_BABEL  <code>module-attribute</code>","text":"<pre><code>SPECS_BABEL = {'14532': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest babel-generator --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '13928': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest babel-parser -t \"arrow\" --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '15649': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest packages/babel-traverse/test/scope.js --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '15445': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest packages/babel-generator/test/index.js -t \"generation \" --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}, '16130': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['yarn jest babel-helpers --verbose'], 'install': ['make bootstrap'], 'build': ['make build']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_VUEJS","title":"SPECS_VUEJS  <code>module-attribute</code>","text":"<pre><code>SPECS_VUEJS = {'11899': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/compiler-sfc/__tests__/compileStyle.spec.ts --no-watch --reporter=verbose'], 'install': ['pnpm i'], 'build': ['pnpm run build compiler-sfc']}, '11870': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/runtime-core/__tests__/helpers/renderList.spec.ts --no-watch --reporter=verbose'], 'install': ['pnpm i']}, '11739': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/runtime-core/__tests__/hydration.spec.ts --no-watch --reporter=verbose -t \"mismatch handling\"'], 'install': ['pnpm i']}, '11915': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/compiler-core/__tests__/parse.spec.ts --no-watch --reporter=verbose -t \"Element\"'], 'install': ['pnpm i']}, '11589': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'test_cmd': ['pnpm run test packages/runtime-core/__tests__/apiWatch.spec.ts --no-watch --reporter=verbose'], 'install': ['pnpm i']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_DOCUSAURUS","title":"SPECS_DOCUSAURUS  <code>module-attribute</code>","text":"<pre><code>SPECS_DOCUSAURUS = {'10309': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-plugin-content-docs/src/client/__tests__/docsClientUtils.test.ts --verbose']}, '10130': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus/src/server/__tests__/brokenLinks.test.ts --verbose']}, '9897': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-utils/src/__tests__/markdownUtils.test.ts --verbose']}, '9183': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-theme-classic/src/__tests__/options.test.ts --verbose']}, '8927': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['yarn install'], 'test_cmd': ['yarn test packages/docusaurus-utils/src/__tests__/markdownLinks.test.ts --verbose']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_IMMUTABLEJS","title":"SPECS_IMMUTABLEJS  <code>module-attribute</code>","text":"<pre><code>SPECS_IMMUTABLEJS = {'2006': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'build': ['npm run build'], 'test_cmd': ['npx jest __tests__/Range.ts --verbose']}, '2005': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'build': ['npm run build'], 'test_cmd': [f'npx jest __tests__/OrderedMap.ts __tests__/OrderedSet.ts --silent --json | {JEST_JSON_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_THREEJS","title":"SPECS_THREEJS  <code>module-attribute</code>","text":"<pre><code>SPECS_THREEJS = {'27395': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install --ignore-scripts'], 'test_cmd': ['npx qunit test/unit/src/math/Sphere.tests.js']}, '26589': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install --ignore-scripts'], 'test_cmd': ['npx qunit test/unit/src/objects/Line.tests.js test/unit/src/objects/Mesh.tests.js test/unit/src/objects/Points.tests.js']}, '25687': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install --ignore-scripts'], 'test_cmd': ['npx qunit test/unit/src/core/Object3D.tests.js -f \"/json|clone|copy/i\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_PREACT","title":"SPECS_PREACT  <code>module-attribute</code>","text":"<pre><code>SPECS_PREACT = {'4152': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/components.test.js\"']}, '4316': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/events.test.js\"']}, '4245': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useId.test.js\"']}, '4182': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/errorBoundary.test.js\"']}, '4436': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/refs.test.js\"']}, '3763': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/lifecycles/componentDidMount.test.js\"']}, '3739': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useState.test.js\"']}, '3689': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/errorBoundary.test.js\"']}, '3567': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useEffect.test.js\"']}, '3562': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"compat/test/browser/render.test.js\"']}, '3454': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/svg.test.js\"']}, '3345': {'docker_specs': {'node_version': '18', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"hooks/test/browser/useEffect.test.js\"']}, '3062': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}, '3010': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}, '2927': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}, '2896': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"compat/test/browser/memo.test.js\"']}, '2757': {'docker_specs': {'node_version': '16', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['COVERAGE=false BABEL_NO_MODULES=true npx karma start karma.conf.js --single-run --grep=\"test/browser/render.test.js\"']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.SPECS_AXIOS","title":"SPECS_AXIOS  <code>module-attribute</code>","text":"<pre><code>SPECS_AXIOS = {'5892': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': [\"npx mocha test/unit/adapters/http.js -R tap -g 'compression'\"]}, '5316': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'build': ['npm install'], 'test_cmd': [\"npx mocha test/unit/adapters/http.js -R tap -g 'FormData'\"]}, '4738': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': [\"timeout 10s npx mocha -R tap test/unit/adapters/http.js -g 'timeout'\"]}, '4731': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': [\"npx mocha -R tap test/unit/adapters/http.js -g 'body length'\"]}, '6539': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['npx mocha -R tap test/unit/regression/SNYK-JS-AXIOS-7361793.js']}, '5085': {'docker_specs': {'node_version': '20', '_variant': 'js_2'}, 'install': ['npm install'], 'test_cmd': ['npx mocha -R tap test/unit/regression/bugs.js']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.MAP_REPO_VERSION_TO_SPECS_JS","title":"MAP_REPO_VERSION_TO_SPECS_JS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_JS = {'Automattic/wp-calypso': SPECS_CALYPSO, 'chartjs/Chart.js': SPECS_CHART_JS, 'markedjs/marked': SPECS_MARKED, 'processing/p5.js': SPECS_P5_JS, 'diegomura/react-pdf': SPECS_REACT_PDF, 'babel/babel': SPECS_BABEL, 'vuejs/core': SPECS_VUEJS, 'facebook/docusaurus': SPECS_DOCUSAURUS, 'immutable-js/immutable-js': SPECS_IMMUTABLEJS, 'mrdoob/three.js': SPECS_THREEJS, 'preactjs/preact': SPECS_PREACT, 'axios/axios': SPECS_AXIOS}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.javascript.MAP_REPO_TO_INSTALL_JS","title":"MAP_REPO_TO_INSTALL_JS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_JS = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.php","title":"php","text":""},{"location":"api/harness/#swebench.harness.constants.php.SPECS_PHPSPREADSHEET","title":"SPECS_PHPSPREADSHEET  <code>module-attribute</code>","text":"<pre><code>SPECS_PHPSPREADSHEET = {'4313': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Reader/Ods/FormulaTranslatorTest.php']}, '4214': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Calculation/Functions/MathTrig/RoundDownTest.php']}, '4186': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Writer/Xlsx/FunctionPrefixTest.php']}, '4114': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Worksheet/Issue4112Test.php']}, '3940': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Worksheet/WorksheetTest.php']}, '3903': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Shared/StringHelperTest.php']}, '3570': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Calculation/Functions/LookupRef/VLookupTest.php']}, '3463': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Writer/Xlsx/FunctionPrefixTest.php']}, '3469': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Style/StyleTest.php']}, '3659': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['./vendor/bin/phpunit --testdox --colors=never tests/PhpSpreadsheetTests/Worksheet/Table/Issue3635Test.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.php.SPECS_LARAVEL_FRAMEWORK","title":"SPECS_LARAVEL_FRAMEWORK  <code>module-attribute</code>","text":"<pre><code>SPECS_LARAVEL_FRAMEWORK = {'53914': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Integration/Database/DatabaseConnectionsTest.php']}, '53206': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Support/SupportJsTest.php']}, '52866': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Container/ContextualAttributeBindingTest.php']}, '52684': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Support/SupportStrTest.php']}, '52680': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Database/DatabaseEloquentInverseRelationTest.php']}, '52451': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': [\"vendor/bin/phpunit --testdox --colors=never tests/Validation/ValidationValidatorTest.php --filter 'custom'\"]}, '53949': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Support/OnceTest.php']}, '51890': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': [\"vendor/bin/phpunit --testdox --colors=never tests/Validation/ValidationValidatorTest.php --filter 'attribute'\"]}, '51195': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/View/Blade/BladeVerbatimTest.php']}, '48636': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Database/DatabaseEloquentModelTest.php']}, '48573': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Cache/CacheArrayStoreTest.php']}, '46234': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require laravel/prompts --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Routing/RoutingUrlGeneratorTest.php']}, '53696': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer require orchestra/testbench-core --no-update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Database/DatabaseSchemaBlueprintTest.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.php.SPECS_PHP_CS_FIXER","title":"SPECS_PHP_CS_FIXER  <code>module-attribute</code>","text":"<pre><code>SPECS_PHP_CS_FIXER = {'8367': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Import/FullyQualifiedStrictTypesFixerTest.php']}, '8331': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/LanguageConstruct/NullableTypeDeclarationFixerTest.php']}, '8075': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/PhpUnit/PhpUnitAttributesFixerTest.php']}, '8064': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/StringNotation/SimpleToComplexStringVariableFixerTest.php']}, '7998': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Casing/ConstantCaseFixerTest.php']}, '7875': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Whitespace/StatementIndentationFixerTest.php']}, '7635': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Import/FullyQualifiedStrictTypesFixerTest.php']}, '7523': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Operator/BinaryOperatorSpacesFixerTest.php']}, '8256': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/PhpTag/BlankLineAfterOpeningTagFixerTest.php']}, '7663': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Fixer/Whitespace/StatementIndentationFixerTest.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.php.SPECS_CARBON","title":"SPECS_CARBON  <code>module-attribute</code>","text":"<pre><code>SPECS_CARBON = {'3103': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonImmutable/SettersTest.php']}, '3098': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/ConstructTest.php']}, '3073': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/TotalTest.php']}, '3041': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonPeriod/CreateTest.php']}, '3005': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/ConstructTest.php']}, '2981': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/TotalTest.php']}, '2813': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'build': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Factory/FactoryTest.php']}, '2752': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonImmutable/IsTest.php']}, '2665': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/Carbon/RoundTest.php']}, '2762': {'docker_specs': {'php_version': '8.3.16'}, 'install': ['composer update', 'composer install'], 'test_cmd': ['vendor/bin/phpunit --testdox --colors=never tests/CarbonInterval/RoundingTest.php']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.php.MAP_REPO_VERSION_TO_SPECS_PHP","title":"MAP_REPO_VERSION_TO_SPECS_PHP  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_PHP = {'phpoffice/phpspreadsheet': SPECS_PHPSPREADSHEET, 'laravel/framework': SPECS_LARAVEL_FRAMEWORK, 'php-cs-fixer/php-cs-fixer': SPECS_PHP_CS_FIXER, 'briannesbitt/carbon': SPECS_CARBON}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.php.MAP_REPO_TO_INSTALL_PHP","title":"MAP_REPO_TO_INSTALL_PHP  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_PHP = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python","title":"python","text":""},{"location":"api/harness/#swebench.harness.constants.python.TEST_ASTROPY_PYTEST","title":"TEST_ASTROPY_PYTEST  <code>module-attribute</code>","text":"<pre><code>TEST_ASTROPY_PYTEST = 'pytest -rA -vv -o console_output_style=classic --tb=no'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_DJANGO","title":"TEST_DJANGO  <code>module-attribute</code>","text":"<pre><code>TEST_DJANGO = './tests/runtests.py --verbosity 2 --settings=test_sqlite --parallel 1'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_DJANGO_NO_PARALLEL","title":"TEST_DJANGO_NO_PARALLEL  <code>module-attribute</code>","text":"<pre><code>TEST_DJANGO_NO_PARALLEL = './tests/runtests.py --verbosity 2'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_SEABORN","title":"TEST_SEABORN  <code>module-attribute</code>","text":"<pre><code>TEST_SEABORN = 'pytest --no-header -rA'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_SEABORN_VERBOSE","title":"TEST_SEABORN_VERBOSE  <code>module-attribute</code>","text":"<pre><code>TEST_SEABORN_VERBOSE = 'pytest -rA --tb=long'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_PYTEST","title":"TEST_PYTEST  <code>module-attribute</code>","text":"<pre><code>TEST_PYTEST = 'pytest -rA'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_PYTEST_VERBOSE","title":"TEST_PYTEST_VERBOSE  <code>module-attribute</code>","text":"<pre><code>TEST_PYTEST_VERBOSE = 'pytest -rA --tb=long'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_SPHINX","title":"TEST_SPHINX  <code>module-attribute</code>","text":"<pre><code>TEST_SPHINX = 'tox --current-env -epy39 -v --'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_SYMPY","title":"TEST_SYMPY  <code>module-attribute</code>","text":"<pre><code>TEST_SYMPY = \"PYTHONWARNINGS='ignore::UserWarning,ignore::SyntaxWarning' bin/test -C --verbose\"\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.TEST_SYMPY_VERBOSE","title":"TEST_SYMPY_VERBOSE  <code>module-attribute</code>","text":"<pre><code>TEST_SYMPY_VERBOSE = 'bin/test -C --verbose'\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_SKLEARN","title":"SPECS_SKLEARN  <code>module-attribute</code>","text":"<pre><code>SPECS_SKLEARN = {k: {'python': '3.6', 'packages': 'numpy scipy cython pytest pandas matplotlib', 'install': 'python -m pip install -v --no-use-pep517 --no-build-isolation -e .', 'pip_packages': ['cython', 'numpy==1.19.2', 'setuptools', 'scipy==1.5.2'], 'test_cmd': TEST_PYTEST}for k in ['0.20', '0.21', '0.22']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_FLASK","title":"SPECS_FLASK  <code>module-attribute</code>","text":"<pre><code>SPECS_FLASK = {'2.0': {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'pip_packages': ['setuptools==70.0.0', 'Werkzeug==2.3.7', 'Jinja2==3.0.1', 'itsdangerous==2.1.2', 'click==8.0.1', 'MarkupSafe==2.1.3'], 'test_cmd': TEST_PYTEST}, '2.1': {'python': '3.10', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'pip_packages': ['setuptools==70.0.0', 'click==8.1.3', 'itsdangerous==2.1.2', 'Jinja2==3.1.2', 'MarkupSafe==2.1.1', 'Werkzeug==2.3.7'], 'test_cmd': TEST_PYTEST}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_DJANGO","title":"SPECS_DJANGO  <code>module-attribute</code>","text":"<pre><code>SPECS_DJANGO = {k: {'python': '3.5', 'packages': 'requirements.txt', 'pre_install': ['apt-get update &amp;&amp; apt-get install -y locales', \"echo 'en_US UTF-8' &gt; /etc/locale.gen\", 'locale-gen en_US.UTF-8'], 'install': 'python setup.py install', 'pip_packages': ['setuptools'], 'eval_commands': ['export LANG=en_US.UTF-8', 'export LC_ALL=en_US.UTF-8', 'export PYTHONIOENCODING=utf8', 'export LANGUAGE=en_US:en'], 'test_cmd': TEST_DJANGO}for k in ['1.7', '1.8', '1.9', '1.10', '1.11', '2.0', '2.1', '2.2']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_REQUESTS","title":"SPECS_REQUESTS  <code>module-attribute</code>","text":"<pre><code>SPECS_REQUESTS = {k: {'python': '3.9', 'packages': 'pytest', 'install': 'python -m pip install .', 'test_cmd': TEST_PYTEST}for k in (['0.7', '0.8', '0.9', '0.11', '0.13', '0.14', '1.1', '1.2', '2.0', '2.2'] + ['2.3', '2.4', '2.5', '2.7', '2.8', '2.9', '2.10', '2.11', '2.12', '2.17'] + ['2.18', '2.19', '2.22', '2.26', '2.25', '2.27', '2.31', '3.0'])}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_SEABORN","title":"SPECS_SEABORN  <code>module-attribute</code>","text":"<pre><code>SPECS_SEABORN = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'pip_packages': ['contourpy==1.1.0', 'cycler==0.11.0', 'fonttools==4.42.1', 'importlib-resources==6.0.1', 'kiwisolver==1.4.5', 'matplotlib==3.7.2', 'numpy==1.25.2', 'packaging==23.1', 'pandas==1.3.5', 'pillow==10.0.0', 'pyparsing==3.0.9', 'pytest', 'python-dateutil==2.8.2', 'pytz==2023.3.post1', 'scipy==1.11.2', 'six==1.16.0', 'tzdata==2023.1', 'zipp==3.16.2'], 'test_cmd': TEST_SEABORN}for k in ['0.11']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_PYTEST","title":"SPECS_PYTEST  <code>module-attribute</code>","text":"<pre><code>SPECS_PYTEST = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'test_cmd': TEST_PYTEST}for k in ['4.4', '4.5', '4.6', '5.0', '5.1', '5.2', '5.3', '5.4', '6.0', '6.2', '6.3', '7.0', '7.1', '7.2', '7.4', '8.0', '8.1', '8.2', '8.3', '8.4']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_MATPLOTLIB","title":"SPECS_MATPLOTLIB  <code>module-attribute</code>","text":"<pre><code>SPECS_MATPLOTLIB = {k: {'python': '3.11', 'packages': 'environment.yml', 'install': 'python -m pip install -e .', 'pre_install': ['apt-get -y update &amp;&amp; apt-get -y upgrade &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y imagemagick ffmpeg texlive texlive-latex-extra texlive-fonts-recommended texlive-xetex texlive-luatex cm-super dvipng', 'QHULL_URL=\"http://www.qhull.org/download/qhull-2020-src-8.0.2.tgz\"', 'QHULL_TAR=\"/tmp/qhull-2020-src-8.0.2.tgz\"', 'QHULL_BUILD_DIR=\"/testbed/build\"', 'wget -O \"$QHULL_TAR\" \"$QHULL_URL\"', 'mkdir -p \"$QHULL_BUILD_DIR\"', 'tar -xvzf \"$QHULL_TAR\" -C \"$QHULL_BUILD_DIR\"'], 'pip_packages': ['contourpy==1.1.0', 'cycler==0.11.0', 'fonttools==4.42.1', 'ghostscript', 'kiwisolver==1.4.5', 'numpy==1.25.2', 'packaging==23.1', 'pillow==10.0.0', 'pikepdf', 'pyparsing==3.0.9', 'python-dateutil==2.8.2', 'six==1.16.0', 'setuptools==68.1.2', 'setuptools-scm==7.1.0', 'typing-extensions==4.7.1'], 'test_cmd': TEST_PYTEST}for k in ['3.5', '3.6', '3.7', '3.8', '3.9']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_SPHINX","title":"SPECS_SPHINX  <code>module-attribute</code>","text":"<pre><code>SPECS_SPHINX = {k: {'python': '3.9', 'pip_packages': ['tox==4.16.0', 'tox-current-env==0.0.11', 'Jinja2==3.0.3'], 'install': 'python -m pip install -e .[test]', 'pre_install': [\"sed -i 's/pytest/pytest -rA/' tox.ini\"], 'test_cmd': TEST_SPHINX}for k in (['1.5', '1.6', '1.7', '1.8', '2.0', '2.1', '2.2', '2.3', '2.4', '3.0'] + ['3.1', '3.2', '3.3', '3.4', '3.5', '4.0', '4.1', '4.2', '4.3', '4.4'] + ['4.5', '5.0', '5.1', '5.2', '5.3', '6.0', '6.2', '7.0', '7.1', '7.2'] + ['7.3', '7.4', '8.0', '8.1'])}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_ASTROPY","title":"SPECS_ASTROPY  <code>module-attribute</code>","text":"<pre><code>SPECS_ASTROPY = {k: {'python': '3.9', 'install': 'python -m pip install -e .[test] --verbose', 'pip_packages': ['attrs==23.1.0', 'exceptiongroup==1.1.3', 'execnet==2.0.2', 'hypothesis==6.82.6', 'iniconfig==2.0.0', 'numpy==1.25.2', 'packaging==23.1', 'pluggy==1.3.0', 'psutil==5.9.5', 'pyerfa==2.0.0.3', 'pytest-arraydiff==0.5.0', 'pytest-astropy-header==0.2.2', 'pytest-astropy==0.10.0', 'pytest-cov==4.1.0', 'pytest-doctestplus==1.0.0', 'pytest-filter-subpackage==0.1.2', 'pytest-mock==3.11.1', 'pytest-openfiles==0.5.0', 'pytest-remotedata==0.4.0', 'pytest-xdist==3.3.1', 'pytest==7.4.0', 'PyYAML==6.0.1', 'setuptools==68.0.0', 'sortedcontainers==2.4.0', 'tomli==2.0.1'], 'test_cmd': TEST_PYTEST}for k in ['3.0', '3.1', '3.2', '4.1', '4.2', '4.3', '5.0', '5.1', '5.2', 'v5.3']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_SYMPY","title":"SPECS_SYMPY  <code>module-attribute</code>","text":"<pre><code>SPECS_SYMPY = {k: {'python': '3.9', 'packages': 'mpmath flake8', 'pip_packages': ['mpmath==1.3.0', 'flake8-comprehensions'], 'install': 'python -m pip install -e .', 'test_cmd': TEST_SYMPY}for k in (['0.7', '1.0', '1.1', '1.10', '1.11', '1.12', '1.2', '1.4', '1.5', '1.6'] + ['1.7', '1.8', '1.9'] + ['1.10', '1.11', '1.12', '1.13', '1.14'])}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_PYLINT","title":"SPECS_PYLINT  <code>module-attribute</code>","text":"<pre><code>SPECS_PYLINT = {k: {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'test_cmd': TEST_PYTEST}for k in ['2.10', '2.11', '2.13', '2.14', '2.15', '2.16', '2.17', '2.8', '2.9', '3.0', '3.1', '3.2', '3.3', '4.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_XARRAY","title":"SPECS_XARRAY  <code>module-attribute</code>","text":"<pre><code>SPECS_XARRAY = {k: {'python': '3.10', 'packages': 'environment.yml', 'install': 'python -m pip install -e .', 'pip_packages': ['numpy==1.23.0', 'packaging==23.1', 'pandas==1.5.3', 'pytest==7.4.0', 'python-dateutil==2.8.2', 'pytz==2023.3', 'six==1.16.0', 'scipy==1.11.1', 'setuptools==68.0.0', 'dask==2022.8.1'], 'no_use_env': True, 'test_cmd': TEST_PYTEST}for k in ['0.12', '0.18', '0.19', '0.20', '2022.03', '2022.06', '2022.09', '2023.07', '2024.05']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_SQLFLUFF","title":"SPECS_SQLFLUFF  <code>module-attribute</code>","text":"<pre><code>SPECS_SQLFLUFF = {k: {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .', 'test_cmd': TEST_PYTEST}for k in ['0.10', '0.11', '0.12', '0.13', '0.4', '0.5', '0.6', '0.8', '0.9', '1.0', '1.1', '1.2', '1.3', '1.4', '2.0', '2.1', '2.2']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_DBT_CORE","title":"SPECS_DBT_CORE  <code>module-attribute</code>","text":"<pre><code>SPECS_DBT_CORE = {k: {'python': '3.9', 'packages': 'requirements.txt', 'install': 'python -m pip install -e .'}for k in ['0.13', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_PYVISTA","title":"SPECS_PYVISTA  <code>module-attribute</code>","text":"<pre><code>SPECS_PYVISTA = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'pip_packages': ['pytest'], 'test_cmd': TEST_PYTEST}for k in ['0.20', '0.21', '0.22', '0.23']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_ASTROID","title":"SPECS_ASTROID  <code>module-attribute</code>","text":"<pre><code>SPECS_ASTROID = {k: {'python': '3.9', 'install': 'python -m pip install -e .', 'pip_packages': ['pytest'], 'test_cmd': TEST_PYTEST}for k in ['2.10', '2.12', '2.13', '2.14', '2.15', '2.16', '2.5', '2.6', '2.7', '2.8', '2.9', '3.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_MARSHMALLOW","title":"SPECS_MARSHMALLOW  <code>module-attribute</code>","text":"<pre><code>SPECS_MARSHMALLOW = {k: {'python': '3.9', 'install': \"python -m pip install -e '.[dev]'\", 'test_cmd': TEST_PYTEST}for k in ['2.18', '2.19', '2.20', '3.0', '3.1', '3.10', '3.11', '3.12', '3.13', '3.15', '3.16', '3.19', '3.2', '3.4', '3.8', '3.9']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_PVLIB","title":"SPECS_PVLIB  <code>module-attribute</code>","text":"<pre><code>SPECS_PVLIB = {k: {'python': '3.9', 'install': 'python -m pip install -e .[all]', 'packages': 'pandas scipy', 'pip_packages': ['jupyter', 'ipython', 'matplotlib', 'pytest', 'flake8'], 'test_cmd': TEST_PYTEST}for k in ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_PYDICOM","title":"SPECS_PYDICOM  <code>module-attribute</code>","text":"<pre><code>SPECS_PYDICOM = {k: {'python': '3.6', 'install': 'python -m pip install -e .', 'packages': 'numpy', 'pip_packages': ['pytest'], 'test_cmd': TEST_PYTEST}for k in ['1.0', '1.1', '1.2', '1.3', '1.4', '2.0', '2.1', '2.2', '2.3', '2.4', '3.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.SPECS_HUMANEVAL","title":"SPECS_HUMANEVAL  <code>module-attribute</code>","text":"<pre><code>SPECS_HUMANEVAL = {k: {'python': '3.9', 'test_cmd': 'python'}for k in ['1.0']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.MAP_REPO_VERSION_TO_SPECS_PY","title":"MAP_REPO_VERSION_TO_SPECS_PY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_PY = {'astropy/astropy': SPECS_ASTROPY, 'dbt-labs/dbt-core': SPECS_DBT_CORE, 'django/django': SPECS_DJANGO, 'matplotlib/matplotlib': SPECS_MATPLOTLIB, 'marshmallow-code/marshmallow': SPECS_MARSHMALLOW, 'mwaskom/seaborn': SPECS_SEABORN, 'pallets/flask': SPECS_FLASK, 'psf/requests': SPECS_REQUESTS, 'pvlib/pvlib-python': SPECS_PVLIB, 'pydata/xarray': SPECS_XARRAY, 'pydicom/pydicom': SPECS_PYDICOM, 'pylint-dev/astroid': SPECS_ASTROID, 'pylint-dev/pylint': SPECS_PYLINT, 'pytest-dev/pytest': SPECS_PYTEST, 'pyvista/pyvista': SPECS_PYVISTA, 'scikit-learn/scikit-learn': SPECS_SKLEARN, 'sphinx-doc/sphinx': SPECS_SPHINX, 'sqlfluff/sqlfluff': SPECS_SQLFLUFF, 'swe-bench/humaneval': SPECS_HUMANEVAL, 'sympy/sympy': SPECS_SYMPY}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.MAP_REPO_TO_INSTALL_PY","title":"MAP_REPO_TO_INSTALL_PY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_PY = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.MAP_REPO_TO_REQS_PATHS","title":"MAP_REPO_TO_REQS_PATHS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_REQS_PATHS = {'dbt-labs/dbt-core': ['dev-requirements.txt', 'dev_requirements.txt'], 'django/django': ['tests/requirements/py3.txt'], 'matplotlib/matplotlib': ['requirements/dev/dev-requirements.txt', 'requirements/testing/travis_all.txt'], 'pallets/flask': ['requirements/dev.txt'], 'pylint-dev/pylint': ['requirements_test.txt'], 'pyvista/pyvista': ['requirements_test.txt', 'requirements.txt'], 'sqlfluff/sqlfluff': ['requirements_dev.txt'], 'sympy/sympy': ['requirements-dev.txt', 'requirements-test.txt']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.MAP_REPO_TO_ENV_YML_PATHS","title":"MAP_REPO_TO_ENV_YML_PATHS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_ENV_YML_PATHS = {'matplotlib/matplotlib': ['environment.yml'], 'pydata/xarray': ['ci/requirements/environment.yml', 'environment.yml']}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.python.USE_X86_PY","title":"USE_X86_PY  <code>module-attribute</code>","text":"<pre><code>USE_X86_PY = {'astropy__astropy-7973', 'django__django-10087', 'django__django-10097', 'django__django-10213', 'django__django-10301', 'django__django-10316', 'django__django-10426', 'django__django-11383', 'django__django-12185', 'django__django-12497', 'django__django-13121', 'django__django-13417', 'django__django-13431', 'django__django-13447', 'django__django-14155', 'django__django-14164', 'django__django-14169', 'django__django-14170', 'django__django-15180', 'django__django-15199', 'django__django-15280', 'django__django-15292', 'django__django-15474', 'django__django-15682', 'django__django-15689', 'django__django-15695', 'django__django-15698', 'django__django-15781', 'django__django-15925', 'django__django-15930', 'django__django-5158', 'django__django-5470', 'django__django-7188', 'django__django-7475', 'django__django-7530', 'django__django-8326', 'django__django-8961', 'django__django-9003', 'django__django-9703', 'django__django-9871', 'matplotlib__matplotlib-13983', 'matplotlib__matplotlib-13984', 'matplotlib__matplotlib-13989', 'matplotlib__matplotlib-14043', 'matplotlib__matplotlib-14471', 'matplotlib__matplotlib-22711', 'matplotlib__matplotlib-22719', 'matplotlib__matplotlib-22734', 'matplotlib__matplotlib-22767', 'matplotlib__matplotlib-22815', 'matplotlib__matplotlib-22835', 'matplotlib__matplotlib-22865', 'matplotlib__matplotlib-22871', 'matplotlib__matplotlib-22883', 'matplotlib__matplotlib-22926', 'matplotlib__matplotlib-22929', 'matplotlib__matplotlib-22931', 'matplotlib__matplotlib-22945', 'matplotlib__matplotlib-22991', 'matplotlib__matplotlib-23031', 'matplotlib__matplotlib-23047', 'matplotlib__matplotlib-23049', 'matplotlib__matplotlib-23057', 'matplotlib__matplotlib-23088', 'matplotlib__matplotlib-23111', 'matplotlib__matplotlib-23140', 'matplotlib__matplotlib-23174', 'matplotlib__matplotlib-23188', 'matplotlib__matplotlib-23198', 'matplotlib__matplotlib-23203', 'matplotlib__matplotlib-23266', 'matplotlib__matplotlib-23267', 'matplotlib__matplotlib-23288', 'matplotlib__matplotlib-23299', 'matplotlib__matplotlib-23314', 'matplotlib__matplotlib-23332', 'matplotlib__matplotlib-23348', 'matplotlib__matplotlib-23412', 'matplotlib__matplotlib-23476', 'matplotlib__matplotlib-23516', 'matplotlib__matplotlib-23562', 'matplotlib__matplotlib-23563', 'matplotlib__matplotlib-23573', 'matplotlib__matplotlib-23740', 'matplotlib__matplotlib-23742', 'matplotlib__matplotlib-23913', 'matplotlib__matplotlib-23964', 'matplotlib__matplotlib-23987', 'matplotlib__matplotlib-24013', 'matplotlib__matplotlib-24026', 'matplotlib__matplotlib-24088', 'matplotlib__matplotlib-24111', 'matplotlib__matplotlib-24149', 'matplotlib__matplotlib-24177', 'matplotlib__matplotlib-24189', 'matplotlib__matplotlib-24224', 'matplotlib__matplotlib-24250', 'matplotlib__matplotlib-24257', 'matplotlib__matplotlib-24265', 'matplotlib__matplotlib-24334', 'matplotlib__matplotlib-24362', 'matplotlib__matplotlib-24403', 'matplotlib__matplotlib-24431', 'matplotlib__matplotlib-24538', 'matplotlib__matplotlib-24570', 'matplotlib__matplotlib-24604', 'matplotlib__matplotlib-24619', 'matplotlib__matplotlib-24627', 'matplotlib__matplotlib-24637', 'matplotlib__matplotlib-24691', 'matplotlib__matplotlib-24749', 'matplotlib__matplotlib-24768', 'matplotlib__matplotlib-24849', 'matplotlib__matplotlib-24870', 'matplotlib__matplotlib-24912', 'matplotlib__matplotlib-24924', 'matplotlib__matplotlib-24970', 'matplotlib__matplotlib-24971', 'matplotlib__matplotlib-25027', 'matplotlib__matplotlib-25052', 'matplotlib__matplotlib-25079', 'matplotlib__matplotlib-25085', 'matplotlib__matplotlib-25122', 'matplotlib__matplotlib-25126', 'matplotlib__matplotlib-25129', 'matplotlib__matplotlib-25238', 'matplotlib__matplotlib-25281', 'matplotlib__matplotlib-25287', 'matplotlib__matplotlib-25311', 'matplotlib__matplotlib-25332', 'matplotlib__matplotlib-25334', 'matplotlib__matplotlib-25340', 'matplotlib__matplotlib-25346', 'matplotlib__matplotlib-25404', 'matplotlib__matplotlib-25405', 'matplotlib__matplotlib-25425', 'matplotlib__matplotlib-25430', 'matplotlib__matplotlib-25433', 'matplotlib__matplotlib-25442', 'matplotlib__matplotlib-25479', 'matplotlib__matplotlib-25498', 'matplotlib__matplotlib-25499', 'matplotlib__matplotlib-25515', 'matplotlib__matplotlib-25547', 'matplotlib__matplotlib-25551', 'matplotlib__matplotlib-25565', 'matplotlib__matplotlib-25624', 'matplotlib__matplotlib-25631', 'matplotlib__matplotlib-25640', 'matplotlib__matplotlib-25651', 'matplotlib__matplotlib-25667', 'matplotlib__matplotlib-25712', 'matplotlib__matplotlib-25746', 'matplotlib__matplotlib-25772', 'matplotlib__matplotlib-25775', 'matplotlib__matplotlib-25779', 'matplotlib__matplotlib-25785', 'matplotlib__matplotlib-25794', 'matplotlib__matplotlib-25859', 'matplotlib__matplotlib-25960', 'matplotlib__matplotlib-26011', 'matplotlib__matplotlib-26020', 'matplotlib__matplotlib-26024', 'matplotlib__matplotlib-26078', 'matplotlib__matplotlib-26089', 'matplotlib__matplotlib-26101', 'matplotlib__matplotlib-26113', 'matplotlib__matplotlib-26122', 'matplotlib__matplotlib-26160', 'matplotlib__matplotlib-26184', 'matplotlib__matplotlib-26208', 'matplotlib__matplotlib-26223', 'matplotlib__matplotlib-26232', 'matplotlib__matplotlib-26249', 'matplotlib__matplotlib-26278', 'matplotlib__matplotlib-26285', 'matplotlib__matplotlib-26291', 'matplotlib__matplotlib-26300', 'matplotlib__matplotlib-26311', 'matplotlib__matplotlib-26341', 'matplotlib__matplotlib-26342', 'matplotlib__matplotlib-26399', 'matplotlib__matplotlib-26466', 'matplotlib__matplotlib-26469', 'matplotlib__matplotlib-26472', 'matplotlib__matplotlib-26479', 'matplotlib__matplotlib-26532', 'pydata__xarray-2905', 'pydata__xarray-2922', 'pydata__xarray-3095', 'pydata__xarray-3114', 'pydata__xarray-3151', 'pydata__xarray-3156', 'pydata__xarray-3159', 'pydata__xarray-3239', 'pydata__xarray-3302', 'pydata__xarray-3305', 'pydata__xarray-3338', 'pydata__xarray-3364', 'pydata__xarray-3406', 'pydata__xarray-3520', 'pydata__xarray-3527', 'pydata__xarray-3631', 'pydata__xarray-3635', 'pydata__xarray-3637', 'pydata__xarray-3649', 'pydata__xarray-3677', 'pydata__xarray-3733', 'pydata__xarray-3812', 'pydata__xarray-3905', 'pydata__xarray-3976', 'pydata__xarray-3979', 'pydata__xarray-3993', 'pydata__xarray-4075', 'pydata__xarray-4094', 'pydata__xarray-4098', 'pydata__xarray-4182', 'pydata__xarray-4184', 'pydata__xarray-4248', 'pydata__xarray-4339', 'pydata__xarray-4356', 'pydata__xarray-4419', 'pydata__xarray-4423', 'pydata__xarray-4442', 'pydata__xarray-4493', 'pydata__xarray-4510', 'pydata__xarray-4629', 'pydata__xarray-4683', 'pydata__xarray-4684', 'pydata__xarray-4687', 'pydata__xarray-4695', 'pydata__xarray-4750', 'pydata__xarray-4758', 'pydata__xarray-4759', 'pydata__xarray-4767', 'pydata__xarray-4802', 'pydata__xarray-4819', 'pydata__xarray-4827', 'pydata__xarray-4879', 'pydata__xarray-4911', 'pydata__xarray-4939', 'pydata__xarray-4940', 'pydata__xarray-4966', 'pydata__xarray-4994', 'pydata__xarray-5033', 'pydata__xarray-5126', 'pydata__xarray-5131', 'pydata__xarray-5180', 'pydata__xarray-5187', 'pydata__xarray-5233', 'pydata__xarray-5362', 'pydata__xarray-5365', 'pydata__xarray-5455', 'pydata__xarray-5580', 'pydata__xarray-5662', 'pydata__xarray-5682', 'pydata__xarray-5731', 'pydata__xarray-6135', 'pydata__xarray-6386', 'pydata__xarray-6394', 'pydata__xarray-6400', 'pydata__xarray-6461', 'pydata__xarray-6548', 'pydata__xarray-6598', 'pydata__xarray-6599', 'pydata__xarray-6601', 'pydata__xarray-6721', 'pydata__xarray-6744', 'pydata__xarray-6798', 'pydata__xarray-6804', 'pydata__xarray-6823', 'pydata__xarray-6857', 'pydata__xarray-6882', 'pydata__xarray-6889', 'pydata__xarray-6938', 'pydata__xarray-6971', 'pydata__xarray-6992', 'pydata__xarray-6999', 'pydata__xarray-7003', 'pydata__xarray-7019', 'pydata__xarray-7052', 'pydata__xarray-7089', 'pydata__xarray-7101', 'pydata__xarray-7105', 'pydata__xarray-7112', 'pydata__xarray-7120', 'pydata__xarray-7147', 'pydata__xarray-7150', 'pydata__xarray-7179', 'pydata__xarray-7203', 'pydata__xarray-7229', 'pydata__xarray-7233', 'pydata__xarray-7347', 'pydata__xarray-7391', 'pydata__xarray-7393', 'pydata__xarray-7400', 'pydata__xarray-7444', 'pytest-dev__pytest-10482', 'scikit-learn__scikit-learn-10198', 'scikit-learn__scikit-learn-10297', 'scikit-learn__scikit-learn-10306', 'scikit-learn__scikit-learn-10331', 'scikit-learn__scikit-learn-10377', 'scikit-learn__scikit-learn-10382', 'scikit-learn__scikit-learn-10397', 'scikit-learn__scikit-learn-10427', 'scikit-learn__scikit-learn-10428', 'scikit-learn__scikit-learn-10443', 'scikit-learn__scikit-learn-10452', 'scikit-learn__scikit-learn-10459', 'scikit-learn__scikit-learn-10471', 'scikit-learn__scikit-learn-10483', 'scikit-learn__scikit-learn-10495', 'scikit-learn__scikit-learn-10508', 'scikit-learn__scikit-learn-10558', 'scikit-learn__scikit-learn-10577', 'scikit-learn__scikit-learn-10581', 'scikit-learn__scikit-learn-10687', 'scikit-learn__scikit-learn-10774', 'scikit-learn__scikit-learn-10777', 'scikit-learn__scikit-learn-10803', 'scikit-learn__scikit-learn-10844', 'scikit-learn__scikit-learn-10870', 'scikit-learn__scikit-learn-10881', 'scikit-learn__scikit-learn-10899', 'scikit-learn__scikit-learn-10908', 'scikit-learn__scikit-learn-10913', 'scikit-learn__scikit-learn-10949', 'scikit-learn__scikit-learn-10982', 'scikit-learn__scikit-learn-10986', 'scikit-learn__scikit-learn-11040', 'scikit-learn__scikit-learn-11042', 'scikit-learn__scikit-learn-11043', 'scikit-learn__scikit-learn-11151', 'scikit-learn__scikit-learn-11160', 'scikit-learn__scikit-learn-11206', 'scikit-learn__scikit-learn-11235', 'scikit-learn__scikit-learn-11243', 'scikit-learn__scikit-learn-11264', 'scikit-learn__scikit-learn-11281', 'scikit-learn__scikit-learn-11310', 'scikit-learn__scikit-learn-11315', 'scikit-learn__scikit-learn-11333', 'scikit-learn__scikit-learn-11346', 'scikit-learn__scikit-learn-11391', 'scikit-learn__scikit-learn-11496', 'scikit-learn__scikit-learn-11542', 'scikit-learn__scikit-learn-11574', 'scikit-learn__scikit-learn-11578', 'scikit-learn__scikit-learn-11585', 'scikit-learn__scikit-learn-11596', 'scikit-learn__scikit-learn-11635', 'scikit-learn__scikit-learn-12258', 'scikit-learn__scikit-learn-12421', 'scikit-learn__scikit-learn-12443', 'scikit-learn__scikit-learn-12462', 'scikit-learn__scikit-learn-12471', 'scikit-learn__scikit-learn-12486', 'scikit-learn__scikit-learn-12557', 'scikit-learn__scikit-learn-12583', 'scikit-learn__scikit-learn-12585', 'scikit-learn__scikit-learn-12625', 'scikit-learn__scikit-learn-12626', 'scikit-learn__scikit-learn-12656', 'scikit-learn__scikit-learn-12682', 'scikit-learn__scikit-learn-12704', 'scikit-learn__scikit-learn-12733', 'scikit-learn__scikit-learn-12758', 'scikit-learn__scikit-learn-12760', 'scikit-learn__scikit-learn-12784', 'scikit-learn__scikit-learn-12827', 'scikit-learn__scikit-learn-12834', 'scikit-learn__scikit-learn-12860', 'scikit-learn__scikit-learn-12908', 'scikit-learn__scikit-learn-12938', 'scikit-learn__scikit-learn-12961', 'scikit-learn__scikit-learn-12973', 'scikit-learn__scikit-learn-12983', 'scikit-learn__scikit-learn-12989', 'scikit-learn__scikit-learn-13010', 'scikit-learn__scikit-learn-13013', 'scikit-learn__scikit-learn-13017', 'scikit-learn__scikit-learn-13046', 'scikit-learn__scikit-learn-13087', 'scikit-learn__scikit-learn-13124', 'scikit-learn__scikit-learn-13135', 'scikit-learn__scikit-learn-13142', 'scikit-learn__scikit-learn-13143', 'scikit-learn__scikit-learn-13157', 'scikit-learn__scikit-learn-13165', 'scikit-learn__scikit-learn-13174', 'scikit-learn__scikit-learn-13221', 'scikit-learn__scikit-learn-13241', 'scikit-learn__scikit-learn-13253', 'scikit-learn__scikit-learn-13280', 'scikit-learn__scikit-learn-13283', 'scikit-learn__scikit-learn-13302', 'scikit-learn__scikit-learn-13313', 'scikit-learn__scikit-learn-13328', 'scikit-learn__scikit-learn-13333', 'scikit-learn__scikit-learn-13363', 'scikit-learn__scikit-learn-13368', 'scikit-learn__scikit-learn-13392', 'scikit-learn__scikit-learn-13436', 'scikit-learn__scikit-learn-13439', 'scikit-learn__scikit-learn-13447', 'scikit-learn__scikit-learn-13454', 'scikit-learn__scikit-learn-13467', 'scikit-learn__scikit-learn-13472', 'scikit-learn__scikit-learn-13485', 'scikit-learn__scikit-learn-13496', 'scikit-learn__scikit-learn-13497', 'scikit-learn__scikit-learn-13536', 'scikit-learn__scikit-learn-13549', 'scikit-learn__scikit-learn-13554', 'scikit-learn__scikit-learn-13584', 'scikit-learn__scikit-learn-13618', 'scikit-learn__scikit-learn-13620', 'scikit-learn__scikit-learn-13628', 'scikit-learn__scikit-learn-13641', 'scikit-learn__scikit-learn-13704', 'scikit-learn__scikit-learn-13726', 'scikit-learn__scikit-learn-13779', 'scikit-learn__scikit-learn-13780', 'scikit-learn__scikit-learn-13828', 'scikit-learn__scikit-learn-13864', 'scikit-learn__scikit-learn-13877', 'scikit-learn__scikit-learn-13910', 'scikit-learn__scikit-learn-13915', 'scikit-learn__scikit-learn-13933', 'scikit-learn__scikit-learn-13960', 'scikit-learn__scikit-learn-13974', 'scikit-learn__scikit-learn-13983', 'scikit-learn__scikit-learn-14012', 'scikit-learn__scikit-learn-14024', 'scikit-learn__scikit-learn-14053', 'scikit-learn__scikit-learn-14067', 'scikit-learn__scikit-learn-14087', 'scikit-learn__scikit-learn-14092', 'scikit-learn__scikit-learn-14114', 'scikit-learn__scikit-learn-14125', 'scikit-learn__scikit-learn-14141', 'scikit-learn__scikit-learn-14237', 'scikit-learn__scikit-learn-14309', 'scikit-learn__scikit-learn-14430', 'scikit-learn__scikit-learn-14450', 'scikit-learn__scikit-learn-14458', 'scikit-learn__scikit-learn-14464', 'scikit-learn__scikit-learn-14496', 'scikit-learn__scikit-learn-14520', 'scikit-learn__scikit-learn-14544', 'scikit-learn__scikit-learn-14591', 'scikit-learn__scikit-learn-14629', 'scikit-learn__scikit-learn-14704', 'scikit-learn__scikit-learn-14706', 'scikit-learn__scikit-learn-14710', 'scikit-learn__scikit-learn-14732', 'scikit-learn__scikit-learn-14764', 'scikit-learn__scikit-learn-14806', 'scikit-learn__scikit-learn-14869', 'scikit-learn__scikit-learn-14878', 'scikit-learn__scikit-learn-14890', 'scikit-learn__scikit-learn-14894', 'scikit-learn__scikit-learn-14898', 'scikit-learn__scikit-learn-14908', 'scikit-learn__scikit-learn-14983', 'scikit-learn__scikit-learn-14999', 'scikit-learn__scikit-learn-15028', 'scikit-learn__scikit-learn-15084', 'scikit-learn__scikit-learn-15086', 'scikit-learn__scikit-learn-15094', 'scikit-learn__scikit-learn-15096', 'scikit-learn__scikit-learn-15100', 'scikit-learn__scikit-learn-15119', 'scikit-learn__scikit-learn-15120', 'scikit-learn__scikit-learn-15138', 'scikit-learn__scikit-learn-15393', 'scikit-learn__scikit-learn-15495', 'scikit-learn__scikit-learn-15512', 'scikit-learn__scikit-learn-15524', 'scikit-learn__scikit-learn-15535', 'scikit-learn__scikit-learn-15625', 'scikit-learn__scikit-learn-3840', 'scikit-learn__scikit-learn-7760', 'scikit-learn__scikit-learn-8554', 'scikit-learn__scikit-learn-9274', 'scikit-learn__scikit-learn-9288', 'scikit-learn__scikit-learn-9304', 'scikit-learn__scikit-learn-9775', 'scikit-learn__scikit-learn-9939', 'sphinx-doc__sphinx-11311', 'sphinx-doc__sphinx-7910', 'sympy__sympy-12812', 'sympy__sympy-14248', 'sympy__sympy-15222', 'sympy__sympy-19201'}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby","title":"ruby","text":""},{"location":"api/harness/#swebench.harness.constants.ruby.FASTLANE_RSPEC_JQ_TRANSFORM","title":"FASTLANE_RSPEC_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>FASTLANE_RSPEC_JQ_TRANSFORM = 'tail -n +2 | jq -r \\'.examples[] | \"\\\\(.description) - \\\\(.id) - \\\\(.status)\"\\''\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.FPM_RSPEC_JQ_TRANSFORM","title":"FPM_RSPEC_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>FPM_RSPEC_JQ_TRANSFORM = 'sed -n \\'/^{/,$p\\' | jq -r \\'.examples[] | \"\\\\(.description) - \\\\(.status)\"\\''\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.RUBOCOP_RSPEC_JQ_TRANSFORM","title":"RUBOCOP_RSPEC_JQ_TRANSFORM  <code>module-attribute</code>","text":"<pre><code>RUBOCOP_RSPEC_JQ_TRANSFORM = strip()\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.SPECS_JEKYLL","title":"SPECS_JEKYLL  <code>module-attribute</code>","text":"<pre><code>SPECS_JEKYLL = {'9141': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap'], 'test_cmd': ['bundle exec ruby -I test test/test_site.rb -v -n \"/static files/\"']}, '8761': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap'], 'test_cmd': ['bundle exec cucumber --publish-quiet --format progress --no-color features/post_data.feature:6 features/post_data.feature:30']}, '8047': {'docker_specs': {'ruby_version': '3.3'}, 'pre_install': [\"sed -i '/^[[:space:]]*install_if.*mingw/,/^[[:space:]]*end/d' Gemfile\"], 'install': ['script/bootstrap', 'bundle add webrick'], 'test_cmd': ['bundle exec ruby -I test test/test_filters.rb -v -n \"/where_exp filter/\"']}, '8167': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap', 'bundle add webrick'], 'test_cmd': ['bundle exec ruby -I test test/test_utils.rb -v -n \"/Utils.slugify/\"']}, '8771': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['script/bootstrap'], 'test_cmd': ['bundle exec cucumber --publish-quiet --format progress --no-color features/incremental_rebuild.feature:27 features/incremental_rebuild.feature:70']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.SPECS_FLUENTD","title":"SPECS_FLUENTD  <code>module-attribute</code>","text":"<pre><code>SPECS_FLUENTD = {'4598': {'docker_specs': {'ruby_version': '3.3'}, 'pre_install': ['echo \"gem \\'console\\', \\'1.29\\'\" &gt;&gt; Gemfile'], 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin_helper/test_http_server_helper.rb -v -n '/mount/'\"]}, '4311': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/config/test_system_config.rb -v -n '/rotate_age/'\"]}, '4655': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_http.rb -v -n '/test_add/'\"]}, '4030': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': ['bundle exec ruby test/plugin/out_forward/test_ack_handler.rb -v']}, '3917': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': ['bundle exec ruby test/test_config.rb -v']}, '3640': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin_helper/test_retry_state.rb -v -n '/exponential backoff/'\"]}, '3641': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': ['bundle exec ruby test/test_supervisor.rb -v']}, '3616': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_http.rb -v -n '/test_application/'\"]}, '3631': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/test_event_router.rb -v -n '/handle_emits_error/'\"]}, '3466': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_tail.rb -v -n '/test_should_replace_target_info/'\"]}, '3328': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_in_tail.rb -v -n '/test_ENOENT_error_after_setup_watcher/'\"]}, '3608': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/plugin/test_output_as_buffered_retries.rb -v -n '/retry_max_times/'\"]}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.SPECS_FASTLANE","title":"SPECS_FASTLANE  <code>module-attribute</code>","text":"<pre><code>SPECS_FASTLANE = {'21857': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/lane_manager_base_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '20958': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/import_from_git_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '20642': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./frameit/spec/device_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '19765': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/download_dsyms_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '20975': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./match/spec/storage/s3_storage_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '19304': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/zip_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}, '19207': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install --jobs=$(nproc)'], 'test_cmd': [f'FASTLANE_SKIP_UPDATE_CHECK=1 bundle exec rspec ./fastlane/spec/actions_specs/zip_spec.rb --no-color --format json | {FASTLANE_RSPEC_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.SPECS_FPM","title":"SPECS_FPM  <code>module-attribute</code>","text":"<pre><code>SPECS_FPM = {'1850': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/fpm/package/empty_spec.rb --no-color --format json | {FPM_RSPEC_JQ_TRANSFORM}']}, '1829': {'docker_specs': {'ruby_version': '3.1'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/fpm/package/deb_spec.rb --no-color --format json | {FPM_RSPEC_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.SPECS_FAKER","title":"SPECS_FAKER  <code>module-attribute</code>","text":"<pre><code>SPECS_FAKER = {'2970': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/faker/default/test_faker_internet.rb -v -n '/email/'\"]}, '2705': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [\"bundle exec ruby test/faker/default/test_faker_internet.rb -v -n '/password/'\"]}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.SPECS_RUBOCOP","title":"SPECS_RUBOCOP  <code>module-attribute</code>","text":"<pre><code>SPECS_RUBOCOP = {'13705': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/lint/out_of_range_regexp_ref_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13687': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/lint/safe_navigation_chain_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13680': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/redundant_line_continuation_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13668': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/sole_nested_conditional_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13627': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/multiple_comparison_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13653': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/access_modifier_declarations_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13579': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/layout/line_continuation_spacing_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13560': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/file_null_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13503': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/dig_chain_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13479': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/layout/leading_comment_space_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13431': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/layout/empty_lines_around_method_body_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13424': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/safe_navigation_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13393': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/guard_clause_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13396': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/redundant_parentheses_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13375': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cli_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}, '13362': {'docker_specs': {'ruby_version': '3.3'}, 'install': ['bundle install'], 'test_cmd': [f'bundle exec rspec spec/rubocop/cop/style/redundant_freeze_spec.rb --no-color --format json | {RUBOCOP_RSPEC_JQ_TRANSFORM}']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.MAP_REPO_VERSION_TO_SPECS_RUBY","title":"MAP_REPO_VERSION_TO_SPECS_RUBY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_RUBY = {'jekyll/jekyll': SPECS_JEKYLL, 'fluent/fluentd': SPECS_FLUENTD, 'fastlane/fastlane': SPECS_FASTLANE, 'jordansissel/fpm': SPECS_FPM, 'faker-ruby/faker': SPECS_FAKER, 'rubocop/rubocop': SPECS_RUBOCOP}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.ruby.MAP_REPO_TO_INSTALL_RUBY","title":"MAP_REPO_TO_INSTALL_RUBY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_RUBY = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust","title":"rust","text":""},{"location":"api/harness/#swebench.harness.constants.rust.SPECS_RIPGREP","title":"SPECS_RIPGREP  <code>module-attribute</code>","text":"<pre><code>SPECS_RIPGREP = {'2576': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration -- regression']}, '2209': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package ripgrep --test integration -- regression::r2208 --exact']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.SPECS_BAT","title":"SPECS_BAT  <code>module-attribute</code>","text":"<pre><code>SPECS_BAT = {'3108': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag']}, '2835': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests header --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests header']}, '2650': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests map_syntax --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests map_syntax']}, '2393': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests cache_ --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests cache_']}, '2201': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests pag']}, '2260': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests syntax --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests syntax']}, '1892': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests ignored_suffix_arg --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests ignored_suffix_arg']}, '562': {'docker_specs': {'rust_version': '1.81'}, 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package bat --test integration_tests cache']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.SPECS_RUFF","title":"SPECS_RUFF  <code>module-attribute</code>","text":"<pre><code>SPECS_RUFF = {'15626': {'docker_specs': {'rust_version': '1.84'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::flake8_simplify::tests --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::flake8_simplify::tests']}, '15543': {'docker_specs': {'rust_version': '1.84'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::pyupgrade --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::pyupgrade']}, '15443': {'docker_specs': {'rust_version': '1.84'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::flake8_bandit --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::flake8_bandit']}, '15394': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::flake8_pie --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::flake8_pie']}, '15356': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::pycodestyle --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::pycodestyle']}, '15330': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --lib rules::eradicate --no-run'], 'test_cmd': ['cargo test --package ruff_linter --lib rules::eradicate']}, '15309': {'docker_specs': {'rust_version': '1.83'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package ruff_linter --no-run'], 'test_cmd': [\"cargo test --package ruff_linter 'f52'\"]}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.TOKIO_SPECS","title":"TOKIO_SPECS  <code>module-attribute</code>","text":"<pre><code>TOKIO_SPECS = {'6724': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test io_write_all_buf --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test io_write_all_buf --no-fail-fast']}, '6838': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test uds_stream --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test uds_stream --no-fail-fast']}, '6752': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test time_delay_queue --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test time_delay_queue --no-fail-fast']}, '4867': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test sync_broadcast --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test sync_broadcast --no-fail-fast']}, '4898': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics --no-run'], 'test_cmd': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics']}, '6603': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --test sync_mpsc --no-fail-fast --no-run'], 'test_cmd': ['cargo test --test sync_mpsc --no-fail-fast']}, '6551': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics --no-fail-fast --no-run'], 'test_cmd': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --features full --test rt_metrics --no-fail-fast']}, '4384': {'docker_specs': {'rust_version': '1.81'}, 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package tokio --test net_types_unwind --features full --no-fail-fast']}, '7139': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --test fs_file --no-fail-fast --no-run'], 'test_cmd': ['RUSTFLAGS=\"--cfg tokio_unstable\" cargo test --test fs_file --no-fail-fast']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.COREUTILS_SPECS","title":"COREUTILS_SPECS  <code>module-attribute</code>","text":"<pre><code>COREUTILS_SPECS = {'6690': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test --no-run -- test_cp_cp test_cp_same_file test_cp_multiple_files test_cp_single_file test_cp_no_file'], 'test_cmd': ['cargo test --no-fail-fast -- test_cp_cp test_cp_same_file test_cp_multiple_files test_cp_single_file test_cp_no_file']}, '6731': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test backslash --no-run'], 'test_cmd': ['cargo test backslash --no-fail-fast']}, '6575': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test cksum --no-run'], 'test_cmd': ['cargo test cksum --no-fail-fast']}, '6682': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test mkdir --no-run'], 'test_cmd': ['cargo test mkdir --no-fail-fast']}, '6377': {'docker_specs': {'rust_version': '1.81'}, 'install': ['cargo test test_env --no-run'], 'test_cmd': ['cargo test test_env --no-fail-fast']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.NUSHELL_SPECS","title":"NUSHELL_SPECS  <code>module-attribute</code>","text":"<pre><code>NUSHELL_SPECS = {'13246': {'docker_specs': {'rust_version': '1.77'}, 'install': ['cargo test -p nu-command --no-run --test main find::'], 'build': ['cargo build'], 'test_cmd': ['cargo test -p nu-command --no-fail-fast --test main find::']}, '12950': {'docker_specs': {'rust_version': '1.77'}, 'install': ['cargo test external_arguments --no-run'], 'test_cmd': ['cargo test external_arguments --no-fail-fast']}, '12901': {'docker_specs': {'rust_version': '1.77'}, 'install': ['cargo test --no-run shell::env'], 'test_cmd': ['cargo test --no-fail-fast shell::env']}, '13831': {'docker_specs': {'rust_version': '1.79'}, 'install': ['cargo test -p nu-command --no-run split_column'], 'build': ['cargo build'], 'test_cmd': ['cargo test -p nu-command --no-fail-fast split_column']}, '13605': {'docker_specs': {'rust_version': '1.78'}, 'install': ['cargo test -p nu-command --no-run ls::'], 'build': ['cargo build'], 'test_cmd': ['cargo test -p nu-command --no-fail-fast ls::']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.AXUM_SPECS","title":"AXUM_SPECS  <code>module-attribute</code>","text":"<pre><code>AXUM_SPECS = {'2096': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::fallback']}, '1934': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::fallback']}, '1730': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::mod state']}, '1119': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib slash --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib slash']}, '734': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::head']}, '691': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib -- routing::tests::nest::nesting_router_at_root --exact']}, '682': {'docker_specs': {'rust_version': '1.81'}, 'install': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib trailing --no-run'], 'test_cmd': ['RUSTFLAGS=-Awarnings cargo test --package axum --lib trailing -- with_trailing_slash_post without_trailing_slash_post']}}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.MAP_REPO_VERSION_TO_SPECS_RUST","title":"MAP_REPO_VERSION_TO_SPECS_RUST  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_VERSION_TO_SPECS_RUST = {'burntsushi/ripgrep': SPECS_RIPGREP, 'sharkdp/bat': SPECS_BAT, 'astral-sh/ruff': SPECS_RUFF, 'tokio-rs/tokio': TOKIO_SPECS, 'uutils/coreutils': COREUTILS_SPECS, 'nushell/nushell': NUSHELL_SPECS, 'tokio-rs/axum': AXUM_SPECS}\n</code></pre>"},{"location":"api/harness/#swebench.harness.constants.rust.MAP_REPO_TO_INSTALL_RUST","title":"MAP_REPO_TO_INSTALL_RUST  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_INSTALL_RUST = {}\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build","title":"docker_build","text":""},{"location":"api/harness/#swebench.harness.docker_build.BuildImageError","title":"BuildImageError","text":"<pre><code>BuildImageError(image_name, message, logger)\n</code></pre> <p>               Bases: <code>Exception</code></p> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def __init__(self, image_name, message, logger):\n    super().__init__(message)\n    self.super_str = super().__str__()\n    self.image_name = image_name\n    self.log_path = logger.log_file\n    self.logger = logger\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.BuildImageError.super_str","title":"super_str  <code>instance-attribute</code>","text":"<pre><code>super_str = __str__()\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.BuildImageError.image_name","title":"image_name  <code>instance-attribute</code>","text":"<pre><code>image_name = image_name\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.BuildImageError.log_path","title":"log_path  <code>instance-attribute</code>","text":"<pre><code>log_path = log_file\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.BuildImageError.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.BuildImageError.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def __str__(self):\n    return (\n        f\"Error building image {self.image_name}: {self.super_str}\\n\"\n        f\"Check ({self.log_path}) for more information.\"\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.setup_logger","title":"setup_logger","text":"<pre><code>setup_logger(instance_id: str, log_file: Path, mode='w', add_stdout: bool = False)\n</code></pre> <p>This logger is used for logging the build process of images and containers. It writes logs to the log file.</p> <p>If <code>add_stdout</code> is True, logs will also be sent to stdout, which can be used for streaming ephemeral output from Modal containers.</p> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def setup_logger(instance_id: str, log_file: Path, mode=\"w\", add_stdout: bool = False):\n    \"\"\"\n    This logger is used for logging the build process of images and containers.\n    It writes logs to the log file.\n\n    If `add_stdout` is True, logs will also be sent to stdout, which can be used for\n    streaming ephemeral output from Modal containers.\n    \"\"\"\n    log_file.parent.mkdir(parents=True, exist_ok=True)\n    logger = logging.getLogger(f\"{instance_id}.{log_file.name}\")\n    handler = logging.FileHandler(log_file, mode=mode, encoding=UTF8)\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n    setattr(logger, \"log_file\", log_file)\n    if add_stdout:\n        handler = logging.StreamHandler(sys.stdout)\n        formatter = logging.Formatter(\n            f\"%(asctime)s - {instance_id} - %(levelname)s - %(message)s\"\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.close_logger","title":"close_logger","text":"<pre><code>close_logger(logger)\n</code></pre> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def close_logger(logger):\n    # To avoid too many open files\n    for handler in logger.handlers:\n        handler.close()\n        logger.removeHandler(handler)\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.build_image","title":"build_image","text":"<pre><code>build_image(image_name: str, setup_scripts: dict, dockerfile: str, platform: str, client: DockerClient, build_dir: Path, nocache: bool = False)\n</code></pre> <p>Builds a docker image with the given name, setup scripts, dockerfile, and platform.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of the image to build</p> required <code>setup_scripts</code> <code>dict</code> <p>Dictionary of setup script names to setup script contents</p> required <code>dockerfile</code> <code>str</code> <p>Contents of the Dockerfile</p> required <code>platform</code> <code>str</code> <p>Platform to build the image for</p> required <code>client</code> <code>DockerClient</code> <p>Docker client to use for building the image</p> required <code>build_dir</code> <code>Path</code> <p>Directory for the build context (will also contain logs, scripts, and artifacts)</p> required <code>nocache</code> <code>bool</code> <p>Whether to use the cache when building</p> <code>False</code> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def build_image(\n    image_name: str,\n    setup_scripts: dict,\n    dockerfile: str,\n    platform: str,\n    client: docker.DockerClient,\n    build_dir: Path,\n    nocache: bool = False,\n):\n    \"\"\"\n    Builds a docker image with the given name, setup scripts, dockerfile, and platform.\n\n    Args:\n        image_name (str): Name of the image to build\n        setup_scripts (dict): Dictionary of setup script names to setup script contents\n        dockerfile (str): Contents of the Dockerfile\n        platform (str): Platform to build the image for\n        client (docker.DockerClient): Docker client to use for building the image\n        build_dir (Path): Directory for the build context (will also contain logs, scripts, and artifacts)\n        nocache (bool): Whether to use the cache when building\n    \"\"\"\n    # Create a logger for the build process\n    logger = setup_logger(image_name, build_dir / \"build_image.log\")\n    logger.info(\n        f\"Building image {image_name}\\n\"\n        f\"Using dockerfile:\\n{dockerfile}\\n\"\n        f\"Adding ({len(setup_scripts)}) setup scripts to image build repo\"\n    )\n\n    for setup_script_name, setup_script in setup_scripts.items():\n        logger.info(f\"[SETUP SCRIPT] {setup_script_name}:\\n{setup_script}\")\n    try:\n        # Write the setup scripts to the build directory\n        for setup_script_name, setup_script in setup_scripts.items():\n            setup_script_path = build_dir / setup_script_name\n            with open(setup_script_path, \"w\") as f:\n                f.write(setup_script)\n            if setup_script_name not in dockerfile:\n                logger.warning(\n                    f\"Setup script {setup_script_name} may not be used in Dockerfile\"\n                )\n\n        # Write the dockerfile to the build directory\n        dockerfile_path = build_dir / \"Dockerfile\"\n        with open(dockerfile_path, \"w\") as f:\n            f.write(dockerfile)\n\n        # Build the image\n        logger.info(\n            f\"Building docker image {image_name} in {build_dir} with platform {platform}\"\n        )\n        response = client.api.build(\n            path=str(build_dir),\n            tag=image_name,\n            rm=True,\n            forcerm=True,\n            decode=True,\n            platform=platform,\n            nocache=nocache,\n        )\n\n        # Log the build process continuously\n        buildlog = \"\"\n        for chunk in response:\n            if \"stream\" in chunk:\n                # Remove ANSI escape sequences from the log\n                chunk_stream = ansi_escape(chunk[\"stream\"])\n                logger.info(chunk_stream.strip())\n                buildlog += chunk_stream\n            elif \"errorDetail\" in chunk:\n                # Decode error message, raise BuildError\n                logger.error(\n                    f\"Error: {ansi_escape(chunk['errorDetail']['message'])}\"\n                )\n                raise docker.errors.BuildError(\n                    chunk[\"errorDetail\"][\"message\"], buildlog\n                )\n        logger.info(\"Image built successfully!\")\n    except docker.errors.BuildError as e:\n        logger.error(f\"docker.errors.BuildError during {image_name}: {e}\")\n        raise BuildImageError(image_name, str(e), logger) from e\n    except Exception as e:\n        logger.error(f\"Error building image {image_name}: {e}\")\n        raise BuildImageError(image_name, str(e), logger) from e\n    finally:\n        close_logger(logger)  # functions that create loggers should close them\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.build_base_images","title":"build_base_images","text":"<pre><code>build_base_images(client: DockerClient, dataset: list, force_rebuild: bool = False)\n</code></pre> <p>Builds the base images required for the dataset if they do not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client to use for building the images</p> required <code>dataset</code> <code>list</code> <p>List of test specs or dataset to build images for</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild the images even if they already exist</p> <code>False</code> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def build_base_images(\n    client: docker.DockerClient, dataset: list, force_rebuild: bool = False\n):\n    \"\"\"\n    Builds the base images required for the dataset if they do not already exist.\n\n    Args:\n        client (docker.DockerClient): Docker client to use for building the images\n        dataset (list): List of test specs or dataset to build images for\n        force_rebuild (bool): Whether to force rebuild the images even if they already exist\n    \"\"\"\n    # Get the base images to build from the dataset\n    test_specs = get_test_specs_from_dataset(dataset)\n    base_images = {\n        x.base_image_key: (x.base_dockerfile, x.platform) for x in test_specs\n    }\n\n    # Build the base images\n    for image_name, (dockerfile, platform) in base_images.items():\n        try:\n            # Check if the base image already exists\n            client.images.get(image_name)\n            if force_rebuild:\n                # Remove the base image if it exists and force rebuild is enabled\n                remove_image(client, image_name, \"quiet\")\n            else:\n                print(f\"Base image {image_name} already exists, skipping build.\")\n                continue\n        except docker.errors.ImageNotFound:\n            pass\n        # Build the base image (if it does not exist or force rebuild is enabled)\n        print(f\"Building base image ({image_name})\")\n        build_image(\n            image_name=image_name,\n            setup_scripts={},\n            dockerfile=dockerfile,\n            platform=platform,\n            client=client,\n            build_dir=BASE_IMAGE_BUILD_DIR / image_name.replace(\":\", \"__\"),\n        )\n    print(\"Base images built successfully.\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.get_env_configs_to_build","title":"get_env_configs_to_build","text":"<pre><code>get_env_configs_to_build(client: DockerClient, dataset: list)\n</code></pre> <p>Returns a dictionary of image names to build scripts and dockerfiles for environment images. Returns only the environment images that need to be built.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client to use for building the images</p> required <code>dataset</code> <code>list</code> <p>List of test specs or dataset to build images for</p> required Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def get_env_configs_to_build(\n    client: docker.DockerClient,\n    dataset: list,\n):\n    \"\"\"\n    Returns a dictionary of image names to build scripts and dockerfiles for environment images.\n    Returns only the environment images that need to be built.\n\n    Args:\n        client (docker.DockerClient): Docker client to use for building the images\n        dataset (list): List of test specs or dataset to build images for\n    \"\"\"\n    image_scripts = dict()\n    base_images = dict()\n    test_specs = get_test_specs_from_dataset(dataset)\n\n    for test_spec in test_specs:\n        # Check if the base image exists\n        try:\n            if test_spec.base_image_key not in base_images:\n                base_images[test_spec.base_image_key] = client.images.get(\n                    test_spec.base_image_key\n                )\n            base_image = base_images[test_spec.base_image_key]\n        except docker.errors.ImageNotFound:\n            raise Exception(\n                f\"Base image {test_spec.base_image_key} not found for {test_spec.env_image_key}\\n.\"\n                \"Please build the base images first.\"\n            )\n\n        # Check if the environment image exists\n        image_exists = False\n        try:\n            env_image = client.images.get(test_spec.env_image_key)\n            image_exists = True\n        except docker.errors.ImageNotFound:\n            pass\n        if not image_exists:\n            # Add the environment image to the list of images to build\n            image_scripts[test_spec.env_image_key] = {\n                \"setup_script\": test_spec.setup_env_script,\n                \"dockerfile\": test_spec.env_dockerfile,\n                \"platform\": test_spec.platform,\n            }\n    return image_scripts\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.build_env_images","title":"build_env_images","text":"<pre><code>build_env_images(client: DockerClient, dataset: list, force_rebuild: bool = False, max_workers: int = 4)\n</code></pre> <p>Builds the environment images required for the dataset if they do not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client to use for building the images</p> required <code>dataset</code> <code>list</code> <p>List of test specs or dataset to build images for</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild the images even if they already exist</p> <code>False</code> <code>max_workers</code> <code>int</code> <p>Maximum number of workers to use for building images</p> <code>4</code> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def build_env_images(\n    client: docker.DockerClient,\n    dataset: list,\n    force_rebuild: bool = False,\n    max_workers: int = 4,\n):\n    \"\"\"\n    Builds the environment images required for the dataset if they do not already exist.\n\n    Args:\n        client (docker.DockerClient): Docker client to use for building the images\n        dataset (list): List of test specs or dataset to build images for\n        force_rebuild (bool): Whether to force rebuild the images even if they already exist\n        max_workers (int): Maximum number of workers to use for building images\n    \"\"\"\n    # Get the environment images to build from the dataset\n    if force_rebuild:\n        env_image_keys = {x.env_image_key for x in get_test_specs_from_dataset(dataset)}\n        for key in env_image_keys:\n            remove_image(client, key, \"quiet\")\n    build_base_images(client, dataset, force_rebuild)\n    configs_to_build = get_env_configs_to_build(client, dataset)\n    if len(configs_to_build) == 0:\n        print(\"No environment images need to be built.\")\n        return [], []\n    print(f\"Total environment images to build: {len(configs_to_build)}\")\n\n    args_list = list()\n    for image_name, config in configs_to_build.items():\n        args_list.append(\n            (\n                image_name,\n                {\"setup_env.sh\": config[\"setup_script\"]},\n                config[\"dockerfile\"],\n                config[\"platform\"],\n                client,\n                ENV_IMAGE_BUILD_DIR / image_name.replace(\":\", \"__\"),\n            )\n        )\n\n    successful, failed = run_threadpool(build_image, args_list, max_workers)\n    # Show how many images failed to build\n    if len(failed) == 0:\n        print(\"All environment images built successfully.\")\n    else:\n        print(f\"{len(failed)} environment images failed to build.\")\n\n    # Return the list of (un)successfuly built images\n    return successful, failed\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.build_instance_images","title":"build_instance_images","text":"<pre><code>build_instance_images(client: DockerClient, dataset: list, force_rebuild: bool = False, max_workers: int = 4, namespace: str = None, tag: str = None)\n</code></pre> <p>Builds the instance images required for the dataset if they do not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>List of test specs or dataset to build images for</p> required <code>client</code> <code>DockerClient</code> <p>Docker client to use for building the images</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild the images even if they already exist</p> <code>False</code> <code>max_workers</code> <code>int</code> <p>Maximum number of workers to use for building images</p> <code>4</code> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def build_instance_images(\n    client: docker.DockerClient,\n    dataset: list,\n    force_rebuild: bool = False,\n    max_workers: int = 4,\n    namespace: str = None,\n    tag: str = None,\n):\n    \"\"\"\n    Builds the instance images required for the dataset if they do not already exist.\n\n    Args:\n        dataset (list): List of test specs or dataset to build images for\n        client (docker.DockerClient): Docker client to use for building the images\n        force_rebuild (bool): Whether to force rebuild the images even if they already exist\n        max_workers (int): Maximum number of workers to use for building images\n    \"\"\"\n    # Build environment images (and base images as needed) first\n    test_specs = list(\n        map(\n            lambda x: make_test_spec(x, namespace=namespace, instance_image_tag=tag),\n            dataset,\n        )\n    )\n    if force_rebuild:\n        for spec in test_specs:\n            remove_image(client, spec.instance_image_key, \"quiet\")\n    _, env_failed = build_env_images(client, test_specs, force_rebuild, max_workers)\n\n    if len(env_failed) &gt; 0:\n        # Don't build images for instances that depend on failed-to-build env images\n        dont_run_specs = [\n            spec for spec in test_specs if spec.env_image_key in env_failed\n        ]\n        test_specs = [\n            spec for spec in test_specs if spec.env_image_key not in env_failed\n        ]\n        print(\n            f\"Skipping {len(dont_run_specs)} instances - due to failed env image builds\"\n        )\n    print(f\"Building instance images for {len(test_specs)} instances\")\n    successful, failed = list(), list()\n\n    # `logger` is set to None b/c logger is created in build-instage_image\n    payloads = [(spec, client, None, False) for spec in test_specs]\n    # Build the instance images\n    successful, failed = run_threadpool(build_instance_image, payloads, max_workers)\n    # Show how many images failed to build\n    if len(failed) == 0:\n        print(\"All instance images built successfully.\")\n    else:\n        print(f\"{len(failed)} instance images failed to build.\")\n\n    # Return the list of (un)successfuly built images\n    return successful, failed\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.build_instance_image","title":"build_instance_image","text":"<pre><code>build_instance_image(test_spec: TestSpec, client: DockerClient, logger: Logger | None, nocache: bool)\n</code></pre> <p>Builds the instance image for the given test spec if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>test_spec</code> <code>TestSpec</code> <p>Test spec to build the instance image for</p> required <code>client</code> <code>DockerClient</code> <p>Docker client to use for building the image</p> required <code>logger</code> <code>Logger</code> <p>Logger to use for logging the build process</p> required <code>nocache</code> <code>bool</code> <p>Whether to use the cache when building</p> required Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def build_instance_image(\n    test_spec: TestSpec,\n    client: docker.DockerClient,\n    logger: logging.Logger | None,\n    nocache: bool,\n):\n    \"\"\"\n    Builds the instance image for the given test spec if it does not already exist.\n\n    Args:\n        test_spec (TestSpec): Test spec to build the instance image for\n        client (docker.DockerClient): Docker client to use for building the image\n        logger (logging.Logger): Logger to use for logging the build process\n        nocache (bool): Whether to use the cache when building\n    \"\"\"\n    # Set up logging for the build process\n    build_dir = INSTANCE_IMAGE_BUILD_DIR / test_spec.instance_image_key.replace(\n        \":\", \"__\"\n    )\n    new_logger = False\n    if logger is None:\n        new_logger = True\n        logger = setup_logger(test_spec.instance_id, build_dir / \"prepare_image.log\")\n\n    # Get the image names and dockerfile for the instance image\n    image_name = test_spec.instance_image_key\n    env_image_name = test_spec.env_image_key\n    dockerfile = test_spec.instance_dockerfile\n\n    # Check that the env. image the instance image is based on exists\n    try:\n        env_image = client.images.get(env_image_name)\n    except docker.errors.ImageNotFound as e:\n        raise BuildImageError(\n            test_spec.instance_id,\n            f\"Environment image {env_image_name} not found for {test_spec.instance_id}\",\n            logger,\n        ) from e\n    logger.info(\n        f\"Environment image {env_image_name} found for {test_spec.instance_id}\\n\"\n        f\"Building instance image {image_name} for {test_spec.instance_id}\"\n    )\n\n    # Check if the instance image already exists\n    image_exists = False\n    try:\n        client.images.get(image_name)\n        image_exists = True\n    except docker.errors.ImageNotFound:\n        pass\n\n    # Build the instance image\n    if not image_exists:\n        build_image(\n            image_name=image_name,\n            setup_scripts={\n                \"setup_repo.sh\": test_spec.install_repo_script,\n            },\n            dockerfile=dockerfile,\n            platform=test_spec.platform,\n            client=client,\n            build_dir=build_dir,\n            nocache=nocache,\n        )\n    else:\n        logger.info(f\"Image {image_name} already exists, skipping build.\")\n\n    if new_logger:\n        close_logger(logger)\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_build.build_container","title":"build_container","text":"<pre><code>build_container(test_spec: TestSpec, client: DockerClient, run_id: str, logger: Logger, nocache: bool, force_rebuild: bool = False)\n</code></pre> <p>Builds the instance image for the given test spec and creates a container from the image.</p> <p>Parameters:</p> Name Type Description Default <code>test_spec</code> <code>TestSpec</code> <p>Test spec to build the instance image and container for</p> required <code>client</code> <code>DockerClient</code> <p>Docker client for building image + creating the container</p> required <code>run_id</code> <code>str</code> <p>Run ID identifying process, used for the container name</p> required <code>logger</code> <code>Logger</code> <p>Logger to use for logging the build process</p> required <code>nocache</code> <code>bool</code> <p>Whether to use the cache when building</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild the image even if it already exists</p> <code>False</code> Source code in <code>swebench/harness/docker_build.py</code> <pre><code>def build_container(\n    test_spec: TestSpec,\n    client: docker.DockerClient,\n    run_id: str,\n    logger: logging.Logger,\n    nocache: bool,\n    force_rebuild: bool = False,\n):\n    \"\"\"\n    Builds the instance image for the given test spec and creates a container from the image.\n\n    Args:\n        test_spec (TestSpec): Test spec to build the instance image and container for\n        client (docker.DockerClient): Docker client for building image + creating the container\n        run_id (str): Run ID identifying process, used for the container name\n        logger (logging.Logger): Logger to use for logging the build process\n        nocache (bool): Whether to use the cache when building\n        force_rebuild (bool): Whether to force rebuild the image even if it already exists\n    \"\"\"\n    # Build corresponding instance image\n    if force_rebuild:\n        remove_image(client, test_spec.instance_image_key, \"quiet\")\n    if not test_spec.is_remote_image:\n        build_instance_image(test_spec, client, logger, nocache)\n    else:\n        try:\n            client.images.get(test_spec.instance_image_key)\n        except docker.errors.ImageNotFound:\n            try:\n                client.images.pull(test_spec.instance_image_key)\n            except docker.errors.NotFound as e:\n                raise BuildImageError(test_spec.instance_id, str(e), logger) from e\n            except Exception as e:\n                raise Exception(\n                    f\"Error occurred while pulling image {test_spec.base_image_key}: {str(e)}\"\n                )\n\n    container = None\n    try:\n        # Create the container\n        logger.info(f\"Creating container for {test_spec.instance_id}...\")\n\n        # Define arguments for running the container\n        run_args = test_spec.docker_specs.get(\"run_args\", {})\n        cap_add = run_args.get(\"cap_add\", [])\n\n        container = client.containers.create(\n            image=test_spec.instance_image_key,\n            name=test_spec.get_instance_container_name(run_id),\n            user=DOCKER_USER,\n            detach=True,\n            command=\"tail -f /dev/null\",\n            platform=test_spec.platform,\n            cap_add=cap_add,\n        )\n        logger.info(f\"Container for {test_spec.instance_id} created: {container.id}\")\n        return container\n    except Exception as e:\n        # If an error occurs, clean up the container and raise an exception\n        logger.error(f\"Error creating container for {test_spec.instance_id}: {e}\")\n        logger.info(traceback.format_exc())\n        cleanup_container(client, container, logger)\n        raise BuildImageError(test_spec.instance_id, str(e), logger) from e\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils","title":"docker_utils","text":""},{"location":"api/harness/#swebench.harness.docker_utils.HEREDOC_DELIMITER","title":"HEREDOC_DELIMITER  <code>module-attribute</code>","text":"<pre><code>HEREDOC_DELIMITER = 'EOF_1399519320'\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.copy_to_container","title":"copy_to_container","text":"<pre><code>copy_to_container(container: Container, src: Path, dst: Path)\n</code></pre> <p>Copy a file from local to a docker container</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>Container</code> <p>Docker container to copy to</p> required <code>src</code> <code>Path</code> <p>Source file path</p> required <code>dst</code> <code>Path</code> <p>Destination file path in the container</p> required Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def copy_to_container(container: Container, src: Path, dst: Path):\n    \"\"\"\n    Copy a file from local to a docker container\n\n    Args:\n        container (Container): Docker container to copy to\n        src (Path): Source file path\n        dst (Path): Destination file path in the container\n    \"\"\"\n    # Check if destination path is valid\n    if os.path.dirname(dst) == \"\":\n        raise ValueError(\n            f\"Destination path parent directory cannot be empty!, dst: {dst}\"\n        )\n\n    # temporary tar file\n    tar_path = src.with_suffix(\".tar\")\n    with tarfile.open(tar_path, \"w\") as tar:\n        tar.add(\n            src, arcname=dst.name\n        )  # use destination name, so after `put_archive`, name is correct\n\n    # get bytes for put_archive cmd\n    with open(tar_path, \"rb\") as tar_file:\n        data = tar_file.read()\n\n    # Make directory if necessary\n    container.exec_run(f\"mkdir -p {dst.parent}\")\n\n    # Send tar file to container and extract\n    container.put_archive(os.path.dirname(dst), data)\n\n    # clean up in locally and in container\n    tar_path.unlink()\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.write_to_container","title":"write_to_container","text":"<pre><code>write_to_container(container: Container, data: str, dst: Path)\n</code></pre> <p>Write a string to a file in a docker container</p> Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def write_to_container(container: Container, data: str, dst: Path):\n    \"\"\"\n    Write a string to a file in a docker container\n    \"\"\"\n    # echo with heredoc to file\n    command = f\"cat &lt;&lt;'{HEREDOC_DELIMITER}' &gt; {dst}\\n{data}\\n{HEREDOC_DELIMITER}\"\n    container.exec_run(command)\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.remove_image","title":"remove_image","text":"<pre><code>remove_image(client, image_id, logger=None)\n</code></pre> <p>Remove a Docker image by ID.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client.</p> required <code>image_id</code> <code>str</code> <p>Image ID.</p> required <code>rm_image</code> <code>bool</code> <p>Whether to remove the image.</p> required <code>logger</code> <code>Logger</code> <p>Logger to use for output. If None, print to stdout.</p> <code>None</code> Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def remove_image(client, image_id, logger=None):\n    \"\"\"\n    Remove a Docker image by ID.\n\n    Args:\n        client (docker.DockerClient): Docker client.\n        image_id (str): Image ID.\n        rm_image (bool): Whether to remove the image.\n        logger (logging.Logger): Logger to use for output. If None, print to stdout.\n    \"\"\"\n    if not logger:\n        # if logger is None, print to stdout\n        log_info = print\n        log_error = print\n        raise_error = True\n    elif logger == \"quiet\":\n        # if logger is \"quiet\", don't print anything\n        log_info = lambda x: None\n        log_error = lambda x: None\n        raise_error = True\n    else:\n        # if logger is a logger object, use it\n        log_error = logger.info\n        log_info = logger.info\n        raise_error = False\n    try:\n        log_info(f\"Attempting to remove image {image_id}...\")\n        client.images.remove(image_id, force=True)\n        log_info(f\"Image {image_id} removed.\")\n    except docker.errors.ImageNotFound:\n        log_info(f\"Image {image_id} not found, removing has no effect.\")\n    except Exception as e:\n        if raise_error:\n            raise e\n        log_error(f\"Failed to remove image {image_id}: {e}\\n{traceback.format_exc()}\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.cleanup_container","title":"cleanup_container","text":"<pre><code>cleanup_container(client, container, logger)\n</code></pre> <p>Stop and remove a Docker container. Performs this forcefully if the container cannot be stopped with the python API.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client.</p> required <code>container</code> <code>Container</code> <p>Container to remove.</p> required <code>logger</code> <code>Logger</code> <p>Logger to use for output. If None, print to stdout</p> required Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def cleanup_container(client, container, logger):\n    \"\"\"\n    Stop and remove a Docker container.\n    Performs this forcefully if the container cannot be stopped with the python API.\n\n    Args:\n        client (docker.DockerClient): Docker client.\n        container (docker.models.containers.Container): Container to remove.\n        logger (logging.Logger): Logger to use for output. If None, print to stdout\n    \"\"\"\n    if not container:\n        return\n\n    container_id = container.id\n\n    if not logger:\n        # if logger is None, print to stdout\n        log_error = print\n        log_info = print\n        raise_error = True\n    elif logger == \"quiet\":\n        # if logger is \"quiet\", don't print anything\n        log_info = lambda x: None\n        log_error = lambda x: None\n        raise_error = True\n    else:\n        # if logger is a logger object, use it\n        log_error = logger.info\n        log_info = logger.info\n        raise_error = False\n\n    # Attempt to stop the container\n    try:\n        if container:\n            log_info(f\"Attempting to stop container {container.name}...\")\n            container.stop(timeout=15)\n    except Exception as e:\n        log_error(\n            f\"Failed to stop container {container.name}: {e}. Trying to forcefully kill...\"\n        )\n        try:\n            # Get the PID of the container\n            container_info = client.api.inspect_container(container_id)\n            pid = container_info[\"State\"].get(\"Pid\", 0)\n\n            # If container PID found, forcefully kill the container\n            if pid &gt; 0:\n                log_info(\n                    f\"Forcefully killing container {container.name} with PID {pid}...\"\n                )\n                os.kill(pid, signal.SIGKILL)\n            else:\n                log_error(f\"PID for container {container.name}: {pid} - not killing.\")\n        except Exception as e2:\n            if raise_error:\n                raise e2\n            log_error(\n                f\"Failed to forcefully kill container {container.name}: {e2}\\n\"\n                f\"{traceback.format_exc()}\"\n            )\n\n    # Attempt to remove the container\n    try:\n        log_info(f\"Attempting to remove container {container.name}...\")\n        container.remove(force=True)\n        log_info(f\"Container {container.name} removed.\")\n    except Exception as e:\n        if raise_error:\n            raise e\n        log_error(\n            f\"Failed to remove container {container.name}: {e}\\n\"\n            f\"{traceback.format_exc()}\"\n        )\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.exec_run_with_timeout","title":"exec_run_with_timeout","text":"<pre><code>exec_run_with_timeout(container, cmd, timeout: int | None = 60)\n</code></pre> <p>Run a command in a container with a timeout.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>Container</code> <p>Container to run the command in.</p> required <code>cmd</code> <code>str</code> <p>Command to run.</p> required <code>timeout</code> <code>int</code> <p>Timeout in seconds.</p> <code>60</code> Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def exec_run_with_timeout(container, cmd, timeout: int | None = 60):\n    \"\"\"\n    Run a command in a container with a timeout.\n\n    Args:\n        container (docker.Container): Container to run the command in.\n        cmd (str): Command to run.\n        timeout (int): Timeout in seconds.\n    \"\"\"\n    # Local variables to store the result of executing the command\n    exec_result = b\"\"\n    exec_id = None\n    exception = None\n    timed_out = False\n\n    # Wrapper function to run the command\n    def run_command():\n        nonlocal exec_result, exec_id, exception\n        try:\n            exec_id = container.client.api.exec_create(container.id, cmd)[\"Id\"]\n            exec_stream = container.client.api.exec_start(exec_id, stream=True)\n            for chunk in exec_stream:\n                exec_result += chunk\n        except Exception as e:\n            exception = e\n\n    # Start the command in a separate thread\n    thread = threading.Thread(target=run_command)\n    start_time = time.time()\n    thread.start()\n    thread.join(timeout)\n\n    if exception:\n        raise exception\n\n    # If the thread is still alive, the command timed out\n    if thread.is_alive():\n        if exec_id is not None:\n            exec_pid = container.client.api.exec_inspect(exec_id)[\"Pid\"]\n            container.exec_run(f\"kill -TERM {exec_pid}\", detach=True)\n        timed_out = True\n    end_time = time.time()\n    return exec_result.decode(), timed_out, end_time - start_time\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.find_dependent_images","title":"find_dependent_images","text":"<pre><code>find_dependent_images(client: DockerClient, image_name: str)\n</code></pre> <p>Find all images that are built upon <code>image_name</code> image</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client.</p> required <code>image_name</code> <code>str</code> <p>Name of the base image.</p> required Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def find_dependent_images(client: docker.DockerClient, image_name: str):\n    \"\"\"\n    Find all images that are built upon `image_name` image\n\n    Args:\n        client (docker.DockerClient): Docker client.\n        image_name (str): Name of the base image.\n    \"\"\"\n    dependent_images = []\n\n    # Get all local images\n    all_images = client.images.list()\n\n    # Get the ID of the base image\n    try:\n        base_image = client.images.get(image_name)\n        base_image_id = base_image.id\n    except docker.errors.ImageNotFound:\n        print(f\"Base image {image_name} not found.\")\n        return []\n\n    for image in all_images:\n        # Skip the base image itself\n        if image.id == base_image_id:\n            continue\n\n        # Check if the base image is in this image's history\n        history = image.history()\n        for layer in history:\n            if layer[\"Id\"] == base_image_id:\n                # If found, add this image to the dependent images list\n                tags = image.tags\n                dependent_images.append(tags[0] if tags else image.id)\n                break\n\n    return dependent_images\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.list_images","title":"list_images","text":"<pre><code>list_images(client: DockerClient)\n</code></pre> <p>List all images from the Docker client.</p> Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def list_images(client: docker.DockerClient):\n    \"\"\"\n    List all images from the Docker client.\n    \"\"\"\n    # don't use this in multi-threaded context\n    return {tag for i in client.images.list(all=True) for tag in i.tags}\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.clean_images","title":"clean_images","text":"<pre><code>clean_images(client: DockerClient, prior_images: set, cache_level: str, clean: bool)\n</code></pre> <p>Clean Docker images based on cache level and clean flag.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DockerClient</code> <p>Docker client.</p> required <code>prior_images</code> <code>set</code> <p>Set of images that existed before the current run.</p> required <code>cache</code> <code>str</code> <p>Cache level to use.</p> required <code>clean</code> <code>bool</code> <p>Whether to clean; remove images that are higher in the cache hierarchy than the current cache level. E.g. if cache_level is set to env, remove all previously built instances images. if clean is false, previously built instances images will not be removed, but instance images built in the current run will be removed.</p> required Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def clean_images(\n    client: docker.DockerClient, prior_images: set, cache_level: str, clean: bool\n):\n    \"\"\"\n    Clean Docker images based on cache level and clean flag.\n\n    Args:\n        client (docker.DockerClient): Docker client.\n        prior_images (set): Set of images that existed before the current run.\n        cache (str): Cache level to use.\n        clean (bool): Whether to clean; remove images that are higher in the cache hierarchy than the current\n            cache level. E.g. if cache_level is set to env, remove all previously built instances images. if\n            clean is false, previously built instances images will not be removed, but instance images built\n            in the current run will be removed.\n    \"\"\"\n    images = list_images(client)\n    removed = 0\n    print(\"Cleaning cached images...\")\n    for image_name in images:\n        if should_remove(image_name, cache_level, clean, prior_images):\n            try:\n                remove_image(client, image_name, \"quiet\")\n                removed += 1\n            except Exception as e:\n                print(f\"Error removing image {image_name}: {e}\")\n                continue\n    print(f\"Removed {removed} images.\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.docker_utils.should_remove","title":"should_remove","text":"<pre><code>should_remove(image_name: str, cache_level: str, clean: bool, prior_images: set)\n</code></pre> <p>Determine if an image should be removed based on cache level and clean flag.</p> Source code in <code>swebench/harness/docker_utils.py</code> <pre><code>def should_remove(image_name: str, cache_level: str, clean: bool, prior_images: set):\n    \"\"\"\n    Determine if an image should be removed based on cache level and clean flag.\n    \"\"\"\n    existed_before = image_name in prior_images\n    if \"/\" in image_name:\n        image_name = image_name.split(\"/\", 1)[-1]\n    if image_name.startswith(\"sweb.base\"):\n        if cache_level in {\"none\"} and (clean or not existed_before):\n            return True\n    elif image_name.startswith(\"sweb.env\"):\n        if cache_level in {\"none\", \"base\"} and (clean or not existed_before):\n            return True\n    elif image_name.startswith(\"sweb.eval\"):\n        if cache_level in {\"none\", \"base\", \"env\"} and (clean or not existed_before):\n            return True\n    return False\n</code></pre>"},{"location":"api/harness/#swebench.harness.dockerfiles","title":"dockerfiles","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['get_dockerfile_base', 'get_dockerfile_env', 'get_dockerfile_instance']\n</code></pre>"},{"location":"api/harness/#swebench.harness.dockerfiles.get_dockerfile_base","title":"get_dockerfile_base","text":"<pre><code>get_dockerfile_base(platform, arch, language, **kwargs)\n</code></pre> Source code in <code>swebench/harness/dockerfiles/__init__.py</code> <pre><code>def get_dockerfile_base(platform, arch, language, **kwargs):\n    if arch == \"arm64\":\n        conda_arch = \"aarch64\"\n    else:\n        conda_arch = arch\n\n    # Special handling for some js repos that require a different base image.\n    # If other languages also start using variants, this logic should be moved\n    # to a helper function\n    if \"_variant\" in kwargs and kwargs[\"_variant\"] == \"js_2\":\n        del kwargs[\"_variant\"]\n        return _DOCKERFILE_BASE_JS_2.format(platform=platform, **kwargs)\n\n    return _DOCKERFILE_BASE[language].format(\n        platform=platform, conda_arch=conda_arch, **kwargs\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.dockerfiles.get_dockerfile_env","title":"get_dockerfile_env","text":"<pre><code>get_dockerfile_env(platform, arch, language, base_image_key, **kwargs)\n</code></pre> Source code in <code>swebench/harness/dockerfiles/__init__.py</code> <pre><code>def get_dockerfile_env(platform, arch, language, base_image_key, **kwargs):\n    # Some languages do not have an environment Dockerfile. In those cases, the\n    # base Dockerfile is used as the environment Dockerfile.\n    dockerfile = _DOCKERFILE_ENV.get(language, _DOCKERFILE_BASE[language])\n\n    if \"_variant\" in kwargs and kwargs[\"_variant\"] == \"js_2\":\n        del kwargs[\"_variant\"]\n        return _DOCKERFILE_BASE_JS_2.format(platform=platform, **kwargs)\n\n    return dockerfile.format(\n        platform=platform, arch=arch, base_image_key=base_image_key, **kwargs\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.dockerfiles.get_dockerfile_instance","title":"get_dockerfile_instance","text":"<pre><code>get_dockerfile_instance(platform, language, env_image_name)\n</code></pre> Source code in <code>swebench/harness/dockerfiles/__init__.py</code> <pre><code>def get_dockerfile_instance(platform, language, env_image_name):\n    return _DOCKERFILE_INSTANCE[language].format(\n        platform=platform, env_image_name=env_image_name\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.dockerfiles.c","title":"c","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.go","title":"go","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.java","title":"java","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.javascript","title":"javascript","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.php","title":"php","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.python","title":"python","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.ruby","title":"ruby","text":""},{"location":"api/harness/#swebench.harness.dockerfiles.rust","title":"rust","text":""},{"location":"api/harness/#swebench.harness.grading","title":"grading","text":""},{"location":"api/harness/#swebench.harness.grading.test_passed","title":"test_passed","text":"<pre><code>test_passed(case: str, sm: dict[str, str]) -&gt; bool\n</code></pre> Source code in <code>swebench/harness/grading.py</code> <pre><code>def test_passed(case: str, sm: dict[str, str]) -&gt; bool:\n    return case in sm and sm[case] in [TestStatus.PASSED.value, TestStatus.XFAIL.value]\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.test_failed","title":"test_failed","text":"<pre><code>test_failed(case: str, sm: dict[str, str]) -&gt; bool\n</code></pre> Source code in <code>swebench/harness/grading.py</code> <pre><code>def test_failed(case: str, sm: dict[str, str]) -&gt; bool:\n    return case not in sm or sm[case] in [TestStatus.FAILED.value, TestStatus.ERROR.value]\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.get_logs_eval","title":"get_logs_eval","text":"<pre><code>get_logs_eval(test_spec: TestSpec, log_fp: str) -&gt; tuple[dict[str, str], bool]\n</code></pre> <p>Retrieve evaluation results for a task instance from its corresponding log file</p> <p>Parameters:</p> Name Type Description Default <code>log_fp</code> <code>str</code> <p>path to log file</p> required <p>Returns:     bool: whether the patch applied successfully     dict: status map</p> <p>TODO(john-b-yang): Check this is working properly...</p> Source code in <code>swebench/harness/grading.py</code> <pre><code>def get_logs_eval(test_spec: TestSpec, log_fp: str) -&gt; tuple[dict[str, str], bool]:\n    \"\"\"\n    Retrieve evaluation results for a task instance from its corresponding log file\n\n    Args:\n        log_fp (str): path to log file\n    Returns:\n        bool: whether the patch applied successfully\n        dict: status map\n\n    TODO(john-b-yang): Check this is working properly...\n    \"\"\"\n    repo = test_spec.repo\n    version = test_spec.version\n    log_parser = MAP_REPO_TO_PARSER[repo]\n    test_cmd = MAP_REPO_VERSION_TO_SPECS[repo][version][\"test_cmd\"]\n    if isinstance(test_cmd, list):\n        test_cmd = test_cmd[-1]\n\n    with open(log_fp) as f:\n        content = f.read()\n        # TODO fix constant here\n        bad_codes = list(\n            filter(\n                lambda x: x in content,\n                [\n                    APPLY_PATCH_FAIL,\n                    RESET_FAILED,\n                    TESTS_ERROR,\n                    TESTS_TIMEOUT,\n                ],\n            )\n        )\n        if bad_codes:\n            return {}, False\n        elif not (START_TEST_OUTPUT in content and END_TEST_OUTPUT in content):\n            # Test patch did not apply (should not happen at all)\n            return {}, False\n\n        # Get status map of evaluation results\n        content = content.split(START_TEST_OUTPUT)[1].split(END_TEST_OUTPUT)[0]\n        return log_parser(content, test_spec), True\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.get_eval_tests_report","title":"get_eval_tests_report","text":"<pre><code>get_eval_tests_report(eval_status_map: dict[str, str], gold_results: dict[str, str], calculate_to_fail: bool = False, eval_type: EvalType = PASS_AND_FAIL) -&gt; dict[str, dict[str, list[str]]]\n</code></pre> <p>Create a report based on failure/pass change from gold results to eval results.</p> <p>Parameters:</p> Name Type Description Default <code>eval_sm</code> <code>dict</code> <p>evaluation status map</p> required <code>gold_results</code> <code>dict</code> <p>gold results</p> required <code>calculate_to_fail</code> <code>bool</code> <p>whether to calculate metrics for \"x to fail\" tests</p> <code>False</code> <p>Returns:     report (dict): report of metrics</p> <p>Metric Definitions (Gold Result Pair + Eval Result): - Fail-Pass (F2P) + P: Success (Resolution) - Pass-Pass (P2P) + P: Success (Maintenance) - Fail-Pass (F2P) + F: Failure - Pass-Pass (P2P) + F: Failure</p> <p>Miscellaneous Definitions - Fail-Fail (F2F) + F: Failure Maintenance - Pass-Fail (P2F) + F: Not considered - Fail-Fail (F2F) + P: Success (Extra Credit) - Pass-Fail (P2F) + P: Not considered</p> Source code in <code>swebench/harness/grading.py</code> <pre><code>def get_eval_tests_report(\n    eval_status_map: dict[str, str],\n    gold_results: dict[str, str],\n    calculate_to_fail: bool = False,\n    eval_type: EvalType = EvalType.PASS_AND_FAIL,\n) -&gt; dict[str, dict[str, list[str]]]:\n    \"\"\"\n    Create a report based on failure/pass change from gold results to eval results.\n\n    Args:\n        eval_sm (dict): evaluation status map\n        gold_results (dict): gold results\n        calculate_to_fail (bool): whether to calculate metrics for \"x to fail\" tests\n    Returns:\n        report (dict): report of metrics\n\n    Metric Definitions (Gold Result Pair + Eval Result):\n    - Fail-Pass (F2P) + P: Success (Resolution)\n    - Pass-Pass (P2P) + P: Success (Maintenance)\n    - Fail-Pass (F2P) + F: Failure\n    - Pass-Pass (P2P) + F: Failure\n\n    Miscellaneous Definitions\n    - Fail-Fail (F2F) + F: Failure Maintenance\n    - Pass-Fail (P2F) + F: Not considered\n    - Fail-Fail (F2F) + P: Success (Extra Credit)\n    - Pass-Fail (P2F) + P: Not considered\n    \"\"\"\n\n    def check_pass_and_fail(test_case, eval_status_map, success, failed):\n        if test_passed(test_case, eval_status_map):\n            # Assume silent success for now (test case not in eval_sm)\n            success.append(test_case)\n        elif test_failed(test_case, eval_status_map):\n            failed.append(test_case)\n\n    def check_fail_only(test_case, eval_status_map, success, failed):\n        if (\n            test_case in eval_status_map\n            and eval_status_map[test_case] == TestStatus.FAILED.value\n        ):\n            failed.append(test_case)\n        else:\n            success.append(test_case)\n\n    check_test_case = (\n        check_pass_and_fail if eval_type == EvalType.PASS_AND_FAIL else check_fail_only\n    )\n\n    # Calculate resolution metrics\n    f2p_success = []\n    f2p_failure = []\n    for test_case in gold_results[FAIL_TO_PASS]:\n        check_test_case(test_case, eval_status_map, f2p_success, f2p_failure)\n\n    # Calculate maintenance metrics\n    p2p_success = []\n    p2p_failure = []\n    for test_case in gold_results[PASS_TO_PASS]:\n        check_test_case(test_case, eval_status_map, p2p_success, p2p_failure)\n\n    results = {\n        FAIL_TO_PASS: {\n            \"success\": f2p_success,\n            \"failure\": f2p_failure,\n        },\n        PASS_TO_PASS: {\n            \"success\": p2p_success,\n            \"failure\": p2p_failure,\n        },\n    }\n\n    f2f_success = []\n    f2f_failure = []\n    p2f_success = []\n    p2f_failure = []\n    if calculate_to_fail:\n        # Calculate \"extra credit\" metrics\n        for test_case in gold_results[FAIL_TO_FAIL]:\n            check_test_case(test_case, eval_status_map, f2f_success, f2f_failure)\n\n        # Calculate not considered metrics\n        for test_case in gold_results[PASS_TO_FAIL]:\n            check_test_case(test_case, eval_status_map, p2f_success, p2f_failure)\n\n    results.update(\n        {\n            FAIL_TO_FAIL: {\n                \"success\": f2f_success,\n                \"failure\": f2f_failure,\n            },\n            PASS_TO_FAIL: {\n                \"success\": p2f_success,\n                \"failure\": p2f_failure,\n            },\n        }\n    )\n    return results\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.compute_fail_to_pass","title":"compute_fail_to_pass","text":"<pre><code>compute_fail_to_pass(report: dict[str, dict[str, Any]]) -&gt; float\n</code></pre> <p>Compute fail-to-pass metric. Accepts single report as argument.</p> Source code in <code>swebench/harness/grading.py</code> <pre><code>def compute_fail_to_pass(report: dict[str, dict[str, Any]]) -&gt; float:\n    \"\"\"\n    Compute fail-to-pass metric. Accepts single report as argument.\n    \"\"\"\n    total = len(report[FAIL_TO_PASS][\"success\"]) + len(report[FAIL_TO_PASS][\"failure\"])\n    if total == 0:\n        return 1\n    return len(report[FAIL_TO_PASS][\"success\"]) / total\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.compute_pass_to_pass","title":"compute_pass_to_pass","text":"<pre><code>compute_pass_to_pass(report: dict[str, dict[str, Any]]) -&gt; float\n</code></pre> <p>Compute pass-to-pass metric. Accepts single report as argument.</p> Source code in <code>swebench/harness/grading.py</code> <pre><code>def compute_pass_to_pass(report: dict[str, dict[str, Any]]) -&gt; float:\n    \"\"\"\n    Compute pass-to-pass metric. Accepts single report as argument.\n    \"\"\"\n    total = len(report[PASS_TO_PASS][\"success\"]) + len(report[PASS_TO_PASS][\"failure\"])\n    if total == 0:\n        # TODO: Don't factor in p2p metrics\n        return 1\n    return len(report[PASS_TO_PASS][\"success\"]) / total\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.get_resolution_status","title":"get_resolution_status","text":"<pre><code>get_resolution_status(report: dict[str, dict[str, Any]]) -&gt; str\n</code></pre> <p>Determine resolved status of an evaluation instance</p> Criteria <ul> <li>If fail-to-pass (Resolution) = 1 and pass-to-pass (Maintenance) = 1 -&gt; FULL</li> <li>If (fail-to-pass (Resolution) &lt; 1 and &gt; 0) and pass-to-pass (Maintenance) = 1 -&gt; PARTIAL</li> <li>Otherwise -&gt; NO</li> </ul> Source code in <code>swebench/harness/grading.py</code> <pre><code>def get_resolution_status(report: dict[str, dict[str, Any]]) -&gt; str:\n    \"\"\"\n    Determine resolved status of an evaluation instance\n\n    Criteria:\n        - If fail-to-pass (Resolution) = 1 and pass-to-pass (Maintenance) = 1 -&gt; FULL\n        - If (fail-to-pass (Resolution) &lt; 1 and &gt; 0) and pass-to-pass (Maintenance) = 1 -&gt; PARTIAL\n        - Otherwise -&gt; NO\n    \"\"\"\n    f2p = compute_fail_to_pass(report)\n    p2p = compute_pass_to_pass(report)\n\n    if f2p == 1 and p2p == 1:\n        return ResolvedStatus.FULL.value\n    elif f2p &lt; 1 and f2p &gt; 0 and p2p == 1:\n        return ResolvedStatus.PARTIAL.value\n    else:\n        return ResolvedStatus.NO.value\n</code></pre>"},{"location":"api/harness/#swebench.harness.grading.get_eval_report","title":"get_eval_report","text":"<pre><code>get_eval_report(test_spec: TestSpec, prediction: dict[str, str], test_log_path: str, include_tests_status: bool) -&gt; dict[str, Any]\n</code></pre> <p>Generate a report of model evaluation results from a prediction, task instance, and evaluation log.</p> <p>Parameters:</p> Name Type Description Default <code>test_spec</code> <code>dict</code> <p>test spec containing keys \"instance_id\", \"FAIL_TO_PASS\", and \"PASS_TO_PASS\"</p> required <code>prediction</code> <code>dict</code> <p>prediction containing keys \"instance_id\", \"model_name_or_path\", and \"model_patch\"</p> required <code>log_path</code> <code>str</code> <p>path to evaluation log</p> required <code>include_tests_status</code> <code>bool</code> <p>whether to include the status of each test in the returned report</p> required <p>Returns:     report (dict): report of metrics</p> Source code in <code>swebench/harness/grading.py</code> <pre><code>def get_eval_report(\n    test_spec: TestSpec,\n    prediction: dict[str, str],\n    test_log_path: str,\n    include_tests_status: bool,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Generate a report of model evaluation results from a prediction, task instance,\n    and evaluation log.\n\n    Args:\n        test_spec (dict): test spec containing keys \"instance_id\", \"FAIL_TO_PASS\", and \"PASS_TO_PASS\"\n        prediction (dict): prediction containing keys \"instance_id\", \"model_name_or_path\", and \"model_patch\"\n        log_path (str): path to evaluation log\n        include_tests_status (bool): whether to include the status of each test in the returned report\n    Returns:\n        report (dict): report of metrics\n    \"\"\"\n    report_map = {}\n\n    instance_id = prediction[KEY_INSTANCE_ID]\n    report_map[instance_id] = {\n        \"patch_is_None\": False,\n        \"patch_exists\": False,\n        \"patch_successfully_applied\": False,\n        \"resolved\": False,\n    }\n\n    # Check if the model patch exists\n    if prediction[KEY_PREDICTION] is None:\n        report_map[instance_id][\"patch_is_None\"] = True\n        return report_map\n    report_map[instance_id][\"patch_exists\"] = True\n\n    # Get evaluation logs\n    eval_status_map, found = get_logs_eval(test_spec, test_log_path)\n\n    if not found:\n        return report_map\n    report_map[instance_id][\"patch_successfully_applied\"] = True\n\n    eval_ref = {\n        KEY_INSTANCE_ID: test_spec.instance_id,\n        FAIL_TO_PASS: test_spec.FAIL_TO_PASS,\n        PASS_TO_PASS: test_spec.PASS_TO_PASS,\n    }\n\n    eval_type = EvalType.FAIL_ONLY if test_spec.repo in FAIL_ONLY_REPOS \\\n        else EvalType.PASS_AND_FAIL\n\n    report = get_eval_tests_report(\n        eval_status_map, eval_ref, eval_type=eval_type\n    )\n    if get_resolution_status(report) == ResolvedStatus.FULL.value:\n        report_map[instance_id][\"resolved\"] = True\n\n    if include_tests_status:\n        report_map[instance_id][\"tests_status\"] = report  # type: ignore\n\n    return report_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers","title":"log_parsers","text":""},{"location":"api/harness/#swebench.harness.log_parsers.MAP_REPO_TO_PARSER","title":"MAP_REPO_TO_PARSER  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER = {None: MAP_REPO_TO_PARSER_C, None: MAP_REPO_TO_PARSER_GO, None: MAP_REPO_TO_PARSER_JAVA, None: MAP_REPO_TO_PARSER_JS, None: MAP_REPO_TO_PARSER_PHP, None: MAP_REPO_TO_PARSER_PY, None: MAP_REPO_TO_PARSER_RUST, None: MAP_REPO_TO_PARSER_RUBY}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['MAP_REPO_TO_PARSER']\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.c","title":"c","text":""},{"location":"api/harness/#swebench.harness.log_parsers.c.MAP_REPO_TO_PARSER_C","title":"MAP_REPO_TO_PARSER_C  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_C = {'redis/redis': parse_log_redis, 'jqlang/jq': parse_log_jq, 'nlohmann/json': parse_log_doctest, 'micropython/micropython': parse_log_micropython_test, 'valkey-io/valkey': parse_log_redis, 'fmtlib/fmt': parse_log_googletest}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.c.parse_log_redis","title":"parse_log_redis","text":"<pre><code>parse_log_redis(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/c.py</code> <pre><code>def parse_log_redis(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^\\[(ok|err|skip|ignore)\\]:\\s(.+?)(?:\\s\\((\\d+\\s*m?s)\\))?$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name, _duration = match.groups()\n            if status == \"ok\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"err\":\n                # Strip out file path information from failed test names\n                test_name = re.sub(r\"\\s+in\\s+\\S+$\", \"\", test_name)\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif status == \"skip\" or status == \"ignore\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.c.parse_log_jq","title":"parse_log_jq","text":"<pre><code>parse_log_jq(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/c.py</code> <pre><code>def parse_log_jq(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^\\s*(PASS|FAIL):\\s(.+)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name = match.groups()\n            if status == \"PASS\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"FAIL\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.c.parse_log_doctest","title":"parse_log_doctest","text":"<pre><code>parse_log_doctest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Assumes test binary runs with -s -r=xml.</p> Source code in <code>swebench/harness/log_parsers/c.py</code> <pre><code>def parse_log_doctest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Assumes test binary runs with -s -r=xml.\n    \"\"\"\n    test_status_map = {}\n\n    # Extract XML content\n    start_tag = \"&lt;doctest\"\n    end_tag = \"&lt;/doctest&gt;\"\n    start_index = log.find(start_tag)\n    end_index = (\n        log.find(end_tag, start_index) + len(end_tag) if start_index != -1 else -1\n    )\n\n    if start_index != -1 and end_index != -1:\n        xml_string = log[start_index:end_index]\n        root = ET.fromstring(xml_string)\n\n        for testcase in root.findall(\".//TestCase\"):\n            testcase_name = testcase.get(\"name\")\n            for subcase in testcase.findall(\".//SubCase\"):\n                subcase_name = subcase.get(\"name\")\n                name = f\"{testcase_name} &gt; {subcase_name}\"\n\n                expressions = subcase.findall(\".//Expression\")\n                subcase_passed = all(\n                    expr.get(\"success\") == \"true\" for expr in expressions\n                )\n\n                if subcase_passed:\n                    test_status_map[name] = TestStatus.PASSED.value\n                else:\n                    test_status_map[name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.c.parse_log_micropython_test","title":"parse_log_micropython_test","text":"<pre><code>parse_log_micropython_test(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> Source code in <code>swebench/harness/log_parsers/c.py</code> <pre><code>def parse_log_micropython_test(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    test_status_map = {}\n\n    pattern = r\"^(pass|FAIL|skip)\\s+(.+)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name = match.groups()\n            if status == \"pass\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"FAIL\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif status == \"skip\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.c.parse_log_googletest","title":"parse_log_googletest","text":"<pre><code>parse_log_googletest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> Source code in <code>swebench/harness/log_parsers/c.py</code> <pre><code>def parse_log_googletest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    test_status_map = {}\n\n    pattern = r\"^.*\\[\\s*(OK|FAILED)\\s*\\]\\s(.*)\\s\\(.*\\)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name = match.groups()\n            if status == \"OK\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"FAILED\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.go","title":"go","text":""},{"location":"api/harness/#swebench.harness.log_parsers.go.MAP_REPO_TO_PARSER_GO","title":"MAP_REPO_TO_PARSER_GO  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_GO = {'caddyserver/caddy': parse_log_gotest, 'hashicorp/terraform': parse_log_gotest, 'prometheus/prometheus': parse_log_gotest, 'gohugoio/hugo': parse_log_gotest, 'gin-gonic/gin': parse_log_gotest}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.go.parse_log_gotest","title":"parse_log_gotest","text":"<pre><code>parse_log_gotest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with 'go test'</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <code>test_spec</code> <code>TestSpec</code> <p>test spec (unused)</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/go.py</code> <pre><code>def parse_log_gotest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with 'go test'\n\n    Args:\n        log (str): log content\n        test_spec (TestSpec): test spec (unused)\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    # Pattern to match test result lines\n    pattern = r\"^--- (PASS|FAIL|SKIP): (.+) \\((.+)\\)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name, _duration = match.groups()\n            if status == \"PASS\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"FAIL\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif status == \"SKIP\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.java","title":"java","text":""},{"location":"api/harness/#swebench.harness.log_parsers.java.MAP_REPO_TO_PARSER_JAVA","title":"MAP_REPO_TO_PARSER_JAVA  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_JAVA = {'google/gson': parse_log_maven, 'apache/druid': parse_log_maven, 'javaparser/javaparser': parse_log_maven, 'projectlombok/lombok': parse_log_ant, 'apache/lucene': parse_log_gradle_custom, 'reactivex/rxjava': parse_log_gradle_custom}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.java.parse_log_maven","title":"parse_log_maven","text":"<pre><code>parse_log_maven(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with 'mvn test'. Annoyingly maven will not print the tests that have succeeded. For this log parser to work, each test must be run individually, and then we look for BUILD (SUCCESS|FAILURE) in the logs.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/java.py</code> <pre><code>def parse_log_maven(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with 'mvn test'.\n    Annoyingly maven will not print the tests that have succeeded. For this log\n    parser to work, each test must be run individually, and then we look for\n    BUILD (SUCCESS|FAILURE) in the logs.\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    current_test_name = \"---NO TEST NAME FOUND YET---\"\n\n    # Get the test name from the command used to execute the test.\n    # Assumes we run evaluation with set -x\n    test_name_pattern = r\"^.*-Dtest=(\\S+).*$\"\n    result_pattern = r\"^.*BUILD (SUCCESS|FAILURE)$\"\n\n    for line in log.split(\"\\n\"):\n        test_name_match = re.match(test_name_pattern, line.strip())\n        if test_name_match:\n            current_test_name = test_name_match.groups()[0]\n\n        result_match = re.match(result_pattern, line.strip())\n        if result_match:\n            status = result_match.groups()[0]\n            if status == \"SUCCESS\":\n                test_status_map[current_test_name] = TestStatus.PASSED.value\n            elif status == \"FAILURE\":\n                test_status_map[current_test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.java.parse_log_ant","title":"parse_log_ant","text":"<pre><code>parse_log_ant(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> Source code in <code>swebench/harness/log_parsers/java.py</code> <pre><code>def parse_log_ant(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    test_status_map = {}\n\n    pattern = r\"^\\s*\\[junit\\]\\s+\\[(PASS|FAIL|ERR)\\]\\s+(.*)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name = match.groups()\n            if status == \"PASS\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status in [\"FAIL\", \"ERR\"]:\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.java.parse_log_gradle_custom","title":"parse_log_gradle_custom","text":"<pre><code>parse_log_gradle_custom(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with 'gradle test'. Assumes that the pre-install script to update the gradle config has run.</p> Source code in <code>swebench/harness/log_parsers/java.py</code> <pre><code>def parse_log_gradle_custom(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with 'gradle test'. Assumes that the\n    pre-install script to update the gradle config has run.\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^([^&gt;].+)\\s+(PASSED|FAILED)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            test_name, status = match.groups()\n            if status == \"PASSED\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"FAILED\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript","title":"javascript","text":""},{"location":"api/harness/#swebench.harness.log_parsers.javascript.MAP_REPO_TO_PARSER_JS","title":"MAP_REPO_TO_PARSER_JS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_JS = {'Automattic/wp-calypso': parse_log_calypso, 'chartjs/Chart.js': parse_log_chart_js, 'markedjs/marked': parse_log_marked, 'processing/p5.js': parse_log_p5js, 'diegomura/react-pdf': parse_log_react_pdf, 'babel/babel': parse_log_jest, 'vuejs/core': parse_log_vitest, 'facebook/docusaurus': parse_log_jest, 'immutable-js/immutable-js': parse_log_immutable_js, 'mrdoob/three.js': parse_log_tap, 'preactjs/preact': parse_log_karma, 'axios/axios': parse_log_tap}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_calypso","title":"parse_log_calypso","text":"<pre><code>parse_log_calypso(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated by Calypso test suite</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_calypso(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated by Calypso test suite\n    \"\"\"\n    test_status_map = {}\n    suite = []\n\n    get_test_name = lambda suite, match_pattern, line: \" - \".join(\n        [\" - \".join([x[0] for x in suite]), re.match(match_pattern, line).group(1)]\n    ).strip()\n\n    for log in log.split(\" ./node_modules/.bin/jest \")[1:]:\n        for line in log.split(\"\\n\"):\n            if any([line.startswith(x) for x in [\"Test Suites\", \"  \u25cf \"]]):\n                break\n            elif line.strip().startswith(\"\u2713\"):\n                # Test passed\n                match_pattern = (\n                    r\"^\\s+\u2713\\s(.*)\\(\\d+ms\\)$\"\n                    if re.search(r\"\\(\\d+ms\\)\", line) is not None\n                    else r\"^\\s+\u2713\\s(.*)\"\n                )\n                test_status_map[get_test_name(suite, match_pattern, line)] = (\n                    TestStatus.PASSED.value\n                )\n            elif line.strip().startswith(\"\u2715\"):\n                # Test failed\n                match_pattern = (\n                    r\"^\\s+\u2715\\s(.*)\\(\\d+ms\\)$\"\n                    if re.search(r\"\\(\\d+ms\\)\", line) is not None\n                    else r\"^\\s+\u2715\\s(.*)\"\n                )\n                test_status_map[get_test_name(suite, match_pattern, line)] = (\n                    TestStatus.FAILED.value\n                )\n            elif len(line) - len(line.lstrip()) &gt; 0:\n                # Adjust suite name\n                indent = len(line) - len(line.lstrip())\n                if len(suite) == 0:\n                    # If suite is empty, initialize it\n                    suite = [(line.strip(), indent)]\n                else:\n                    while len(suite) &gt; 0 and suite[-1][-1] &gt;= indent:\n                        # Pop until the last element with indent less than current indent\n                        suite.pop()\n                    suite.append([line.strip(), indent])\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_chart_js","title":"parse_log_chart_js","text":"<pre><code>parse_log_chart_js(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated by ChartJS test suite</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_chart_js(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated by ChartJS test suite\n    \"\"\"\n    log = ansi_escape(log)\n    test_status_map = {}\n    failure_case_patterns = [\n        # use [^\\S\\r\\n] to avoid overlapping Chrome groups on separate lines\n        (r\"Chrome\\s[\\d\\.]+[^\\S\\r\\n]\\(.+?\\)[^\\S\\r\\n](.*)FAILED$\", re.MULTILINE),\n    ]\n    for failure_case_pattern, flags in failure_case_patterns:\n        failures = re.findall(failure_case_pattern, log, flags)\n        if len(failures) == 0:\n            continue\n        for failure in failures:\n            test_status_map[failure] = TestStatus.FAILED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_marked","title":"parse_log_marked","text":"<pre><code>parse_log_marked(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated by Marked test suite</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_marked(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated by Marked test suite\n    \"\"\"\n    test_status_map = {}\n    for line in log.split(\"\\n\"):\n        if re.search(r\"^\\d+\\)\\s(.*)\", line):\n            test = re.search(r\"^\\d+\\)\\s(.*)\", line).group(1)\n            test_status_map[test.strip()] = TestStatus.FAILED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_p5js","title":"parse_log_p5js","text":"<pre><code>parse_log_p5js(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_p5js(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    def remove_json_blocks(log_content):\n        filtered_lines = []\n        in_json_block = False\n        in_json_list_block = False\n        for line in log_content.split(\"\\n\"):\n            stripped_line = line.rstrip()  # Remove trailing whitespace\n            if stripped_line.endswith(\"{\"):\n                in_json_block = True\n                continue\n            if stripped_line.endswith(\"[\"):\n                in_json_list_block = True\n                continue\n            if stripped_line == \"}\" and in_json_block:\n                in_json_block = False\n                continue\n            if stripped_line == \"]\" and in_json_list_block:\n                in_json_list_block = False\n                continue\n            if in_json_block or in_json_list_block:\n                continue\n            if stripped_line.startswith(\"{\") and stripped_line.endswith(\"}\"):\n                continue\n            if stripped_line.startswith(\"[\") and stripped_line.endswith(\"]\"):\n                continue\n            filtered_lines.append(line)\n        return \"\\n\".join(filtered_lines)\n\n    def remove_xml_blocks(log_content):\n        xml_pat = re.compile(r\"&lt;(\\w+)&gt;[\\s\\S]*?&lt;\\/\\1&gt;\", re.MULTILINE)\n        match = xml_pat.search(log_content)\n        while match:\n            # count the number of opening tags in the match\n            opening_tags = match.group().count(rf\"&lt;{match.group(1)}&gt;\") - 1\n            opening_tags = max(opening_tags, 0)\n            start = match.start()\n            end = match.end()\n            log_content = (\n                log_content[:start]\n                + f\"&lt;{match.group(1)}&gt;\" * opening_tags\n                + log_content[end:]\n            )\n            match = xml_pat.search(log_content)\n        return log_content\n\n    def is_valid_fail(match):\n        last_line_indent = 0\n        for line in match.group(2).split(\"\\n\"):\n            line_indent = len(line) - len(line.lstrip())\n            if line_indent &lt;= last_line_indent:\n                return False\n            last_line_indent = line_indent\n        return True\n\n    log = ansi_escape(log)\n    log = remove_json_blocks(log)\n    log = remove_xml_blocks(log)\n    test_results = {}\n\n    # Parse failing tests\n    fail_pattern = re.compile(r\"^\\s*(\\d+)\\)(.{0,1000}?):\", re.MULTILINE | re.DOTALL)\n    for match in fail_pattern.finditer(log):\n        if is_valid_fail(match):\n            test_names = list(map(str.strip, match.group(2).split(\"\\n\")))\n            full_name = \":\".join(test_names)\n            test_results[full_name] = TestStatus.FAILED.value\n\n    return test_results\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_react_pdf","title":"parse_log_react_pdf","text":"<pre><code>parse_log_react_pdf(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated by Carbon test suite</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_react_pdf(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated by Carbon test suite\n    \"\"\"\n    test_status_map = {}\n    for line in log.split(\"\\n\"):\n        for pattern in [\n            (r\"^PASS\\s(.*)\\s\\([\\d\\.]+ms\\)\", TestStatus.PASSED.value),\n            (r\"^PASS\\s(.*)\\s\\([\\d\\.]+\\ss\\)\", TestStatus.PASSED.value),\n            (r\"^PASS\\s(.*)\\s\\([\\d\\.]+s\\)\", TestStatus.PASSED.value),\n            (r\"^PASS\\s(.*)\", TestStatus.PASSED.value),\n            (r\"^FAIL\\s(.*)\\s\\([\\d\\.]+ms\\)\", TestStatus.FAILED.value),\n            (r\"^FAIL\\s(.*)\\s\\([\\d\\.]+\\ss\\)\", TestStatus.FAILED.value),\n            (r\"^FAIL\\s(.*)\\s\\([\\d\\.]+s\\)\", TestStatus.FAILED.value),\n            (r\"^FAIL\\s(.*)\", TestStatus.FAILED.value),\n        ]:\n            if re.search(pattern[0], line):\n                test_name = re.match(pattern[0], line).group(1)\n                test_status_map[test_name] = pattern[1]\n                break\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_jest","title":"parse_log_jest","text":"<pre><code>parse_log_jest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with Jest. Assumes --verbose flag.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_jest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Jest. Assumes --verbose flag.\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^\\s*(\u2713|\u2715|\u25cb)\\s(.+?)(?:\\s\\((\\d+\\s*m?s)\\))?$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status_symbol, test_name, _duration = match.groups()\n            if status_symbol == \"\u2713\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status_symbol == \"\u2715\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif status_symbol == \"\u25cb\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_jest_json","title":"parse_log_jest_json","text":"<pre><code>parse_log_jest_json(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with Jest. Assumes the --json flag has been piped into JEST_JSON_JQ_TRANSFORM. Unlike --verbose, tests with the same name in different describe blocks print with different names.</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_jest_json(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Jest. Assumes the --json flag has been\n    piped into JEST_JSON_JQ_TRANSFORM. Unlike --verbose, tests with the same name\n    in different describe blocks print with different names.\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^\\[(PASSED|FAILED)\\]\\s(.+)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, test_name = match.groups()\n            if status == \"PASSED\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"FAILED\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_vitest","title":"parse_log_vitest","text":"<pre><code>parse_log_vitest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with vitest. Assumes --reporter=verbose flag.</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_vitest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with vitest. Assumes --reporter=verbose flag.\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^\\s*(\u2713|\u00d7|\u2193)\\s(.+?)(?:\\s(\\d+\\s*m?s?|\\[skipped\\]))?$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status_symbol, test_name, _duration_or_skipped = match.groups()\n            if status_symbol == \"\u2713\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status_symbol == \"\u00d7\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif status_symbol == \"\u2193\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_karma","title":"parse_log_karma","text":"<pre><code>parse_log_karma(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with Karma. Handles duplicate test names in different describe blocks. Logic is brittle.</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_karma(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Karma. Handles duplicate test names in\n    different describe blocks. Logic is brittle.\n    \"\"\"\n    test_status_map = {}\n    current_indent = -1\n    current_suite = []\n    started = False\n\n    pattern = r\"^(\\s*)?([\u2714\u2716])?\\s(.*)$\"\n\n    for line in log.split(\"\\n\"):\n        if line.startswith(\"SUMMARY:\"):\n            # Individual test logs end here\n            return test_status_map\n\n        if \"Starting browser\" in line:\n            started = True\n            continue\n\n        if not started:\n            continue\n\n        match = re.match(pattern, line)\n        if match:\n            indent, status, name = match.groups()\n\n            if indent and not status:\n                new_indent = len(indent)\n                if new_indent &gt; current_indent:\n                    current_indent = new_indent\n                    current_suite.append(name)\n                elif new_indent &lt; current_indent:\n                    current_indent = new_indent\n                    current_suite.pop()\n                    continue\n\n            if status in (\"\u2714\", \"\u2716\"):\n                full_test_name = \" &gt; \".join(current_suite + [name])\n                test_status_map[full_test_name] = (\n                    TestStatus.PASSED.value\n                    if status == \"\u2714\"\n                    else TestStatus.FAILED.value\n                )\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_tap","title":"parse_log_tap","text":"<pre><code>parse_log_tap(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with TAP</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_tap(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with TAP\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    # Pattern to match TAP result lines\n    pattern = r\"^(ok|not ok) (\\d+) (.+)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            status, _test_number, test_name = match.groups()\n            if status == \"ok\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif status == \"not ok\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.javascript.parse_log_immutable_js","title":"parse_log_immutable_js","text":"<pre><code>parse_log_immutable_js(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Different immutable.js instances use different test runners and log formats. This function selects the appropriate log parser based on the instance id.</p> Source code in <code>swebench/harness/log_parsers/javascript.py</code> <pre><code>def parse_log_immutable_js(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Different immutable.js instances use different test runners and log formats.\n    This function selects the appropriate log parser based on the instance id.\n    \"\"\"\n    pr_number = test_spec.instance_id.split(\"-\")[-1]\n\n    if pr_number in [\"2006\"]:\n        return parse_log_jest(log, test_spec)\n    elif pr_number in [\"2005\"]:\n        return parse_log_jest_json(log, test_spec)\n    else:\n        raise ValueError(f\"Unknown instance id: {test_spec.instance_id}\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.php","title":"php","text":""},{"location":"api/harness/#swebench.harness.log_parsers.php.MAP_REPO_TO_PARSER_PHP","title":"MAP_REPO_TO_PARSER_PHP  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_PHP = {'phpoffice/phpspreadsheet': parse_log_phpunit, 'laravel/framework': parse_log_phpunit, 'php-cs-fixer/php-cs-fixer': parse_log_phpunit, 'briannesbitt/carbon': parse_log_phpunit}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.php.parse_log_phpunit","title":"parse_log_phpunit","text":"<pre><code>parse_log_phpunit(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for phpunit logs with the --testdox option. Args:     log (str): log content     test_spec (TestSpec): test spec (unused) Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/php.py</code> <pre><code>def parse_log_phpunit(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for phpunit logs with the --testdox option.\n    Args:\n        log (str): log content\n        test_spec (TestSpec): test spec (unused)\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    suite = None\n\n    suite_pattern = r\"^(\\w.+) \\(.+\\)$\"\n    test_pattern = r\"^\\s*([\u2714\u2718\u21a9])\\s*(.*)$\"\n\n    for line in log.split(\"\\n\"):\n        suite_match = re.match(suite_pattern, line)\n        if suite_match:\n            suite = suite_match.groups()[0]\n            continue\n\n        test_match = re.match(test_pattern, line)\n        if test_match:\n            status, test_name = test_match.groups()\n            full_test_name = f\"{suite} &gt; {test_name}\"\n\n            if status == \"\u2714\":\n                test_status_map[full_test_name] = TestStatus.PASSED.value\n            elif status == \"\u2718\":\n                test_status_map[full_test_name] = TestStatus.FAILED.value\n            elif status == \"\u21a9\":\n                test_status_map[full_test_name] = TestStatus.SKIPPED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python","title":"python","text":""},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_astroid","title":"parse_log_astroid  <code>module-attribute</code>","text":"<pre><code>parse_log_astroid = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_flask","title":"parse_log_flask  <code>module-attribute</code>","text":"<pre><code>parse_log_flask = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_marshmallow","title":"parse_log_marshmallow  <code>module-attribute</code>","text":"<pre><code>parse_log_marshmallow = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pvlib","title":"parse_log_pvlib  <code>module-attribute</code>","text":"<pre><code>parse_log_pvlib = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pyvista","title":"parse_log_pyvista  <code>module-attribute</code>","text":"<pre><code>parse_log_pyvista = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_sqlfluff","title":"parse_log_sqlfluff  <code>module-attribute</code>","text":"<pre><code>parse_log_sqlfluff = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_xarray","title":"parse_log_xarray  <code>module-attribute</code>","text":"<pre><code>parse_log_xarray = parse_log_pytest\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pydicom","title":"parse_log_pydicom  <code>module-attribute</code>","text":"<pre><code>parse_log_pydicom = parse_log_pytest_options\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_requests","title":"parse_log_requests  <code>module-attribute</code>","text":"<pre><code>parse_log_requests = parse_log_pytest_options\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pylint","title":"parse_log_pylint  <code>module-attribute</code>","text":"<pre><code>parse_log_pylint = parse_log_pytest_options\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_astropy","title":"parse_log_astropy  <code>module-attribute</code>","text":"<pre><code>parse_log_astropy = parse_log_pytest_v2\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_scikit","title":"parse_log_scikit  <code>module-attribute</code>","text":"<pre><code>parse_log_scikit = parse_log_pytest_v2\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_sphinx","title":"parse_log_sphinx  <code>module-attribute</code>","text":"<pre><code>parse_log_sphinx = parse_log_pytest_v2\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.MAP_REPO_TO_PARSER_PY","title":"MAP_REPO_TO_PARSER_PY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_PY = {'astropy/astropy': parse_log_astropy, 'django/django': parse_log_django, 'marshmallow-code/marshmallow': parse_log_marshmallow, 'matplotlib/matplotlib': parse_log_matplotlib, 'mwaskom/seaborn': parse_log_seaborn, 'pallets/flask': parse_log_flask, 'psf/requests': parse_log_requests, 'pvlib/pvlib-python': parse_log_pvlib, 'pydata/xarray': parse_log_xarray, 'pydicom/pydicom': parse_log_pydicom, 'pylint-dev/astroid': parse_log_astroid, 'pylint-dev/pylint': parse_log_pylint, 'pytest-dev/pytest': parse_log_pytest, 'pyvista/pyvista': parse_log_pyvista, 'scikit-learn/scikit-learn': parse_log_scikit, 'sqlfluff/sqlfluff': parse_log_sqlfluff, 'sphinx-doc/sphinx': parse_log_sphinx, 'sympy/sympy': parse_log_sympy}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pytest","title":"parse_log_pytest","text":"<pre><code>parse_log_pytest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with PyTest framework</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_pytest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    for line in log.split(\"\\n\"):\n        if any([line.startswith(x.value) for x in TestStatus]):\n            # Additional parsing for FAILED status\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(\" - \", \" \")\n            test_case = line.split()\n            if len(test_case) &lt;= 1:\n                continue\n            test_status_map[test_case[1]] = test_case[0]\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pytest_options","title":"parse_log_pytest_options","text":"<pre><code>parse_log_pytest_options(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with PyTest framework with options</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_pytest_options(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework with options\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    option_pattern = re.compile(r\"(.*?)\\[(.*)\\]\")\n    test_status_map = {}\n    for line in log.split(\"\\n\"):\n        if any([line.startswith(x.value) for x in TestStatus]):\n            # Additional parsing for FAILED status\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(\" - \", \" \")\n            test_case = line.split()\n            if len(test_case) &lt;= 1:\n                continue\n            has_option = option_pattern.search(test_case[1])\n            if has_option:\n                main, option = has_option.groups()\n                if (\n                    option.startswith(\"/\")\n                    and not option.startswith(\"//\")\n                    and \"*\" not in option\n                ):\n                    option = \"/\" + option.split(\"/\")[-1]\n                test_name = f\"{main}[{option}]\"\n            else:\n                test_name = test_case[1]\n            test_status_map[test_name] = test_case[0]\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_django","title":"parse_log_django","text":"<pre><code>parse_log_django(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with Django tester framework</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_django(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Django tester framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    lines = log.split(\"\\n\")\n\n    prev_test = None\n    for line in lines:\n        line = line.strip()\n\n        # This isn't ideal but the test output spans multiple lines\n        if \"--version is equivalent to version\" in line:\n            test_status_map[\"--version is equivalent to version\"] = (\n                TestStatus.PASSED.value\n            )\n\n        # Log it in case of error\n        if \" ... \" in line:\n            prev_test = line.split(\" ... \")[0]\n\n        pass_suffixes = (\" ... ok\", \" ... OK\", \" ...  OK\")\n        for suffix in pass_suffixes:\n            if line.endswith(suffix):\n                # TODO: Temporary, exclusive fix for django__django-7188\n                # The proper fix should involve somehow getting the test results to\n                # print on a separate line, rather than the same line\n                if line.strip().startswith(\n                    \"Applying sites.0002_alter_domain_unique...test_no_migrations\"\n                ):\n                    line = line.split(\"...\", 1)[-1].strip()\n                test = line.rsplit(suffix, 1)[0]\n                test_status_map[test] = TestStatus.PASSED.value\n                break\n        if \" ... skipped\" in line:\n            test = line.split(\" ... skipped\")[0]\n            test_status_map[test] = TestStatus.SKIPPED.value\n        if line.endswith(\" ... FAIL\"):\n            test = line.split(\" ... FAIL\")[0]\n            test_status_map[test] = TestStatus.FAILED.value\n        if line.startswith(\"FAIL:\"):\n            test = line.split()[1].strip()\n            test_status_map[test] = TestStatus.FAILED.value\n        if line.endswith(\" ... ERROR\"):\n            test = line.split(\" ... ERROR\")[0]\n            test_status_map[test] = TestStatus.ERROR.value\n        if line.startswith(\"ERROR:\"):\n            test = line.split()[1].strip()\n            test_status_map[test] = TestStatus.ERROR.value\n\n        if line.lstrip().startswith(\"ok\") and prev_test is not None:\n            # It means the test passed, but there's some additional output (including new lines)\n            # between \"...\" and \"ok\" message\n            test = prev_test\n            test_status_map[test] = TestStatus.PASSED.value\n\n    # TODO: This is very brittle, we should do better\n    # There's a bug in the django logger, such that sometimes a test output near the end gets\n    # interrupted by a particular long multiline print statement.\n    # We have observed this in one of 3 forms:\n    # - \"{test_name} ... Testing against Django installed in {*} silenced.\\nok\"\n    # - \"{test_name} ... Internal Server Error: \\/(.*)\\/\\nok\"\n    # - \"{test_name} ... System check identified no issues (0 silenced).\\nok\"\n    patterns = [\n        r\"^(.*?)\\s\\.\\.\\.\\sTesting\\ against\\ Django\\ installed\\ in\\ ((?s:.*?))\\ silenced\\)\\.\\nok$\",\n        r\"^(.*?)\\s\\.\\.\\.\\sInternal\\ Server\\ Error:\\ \\/(.*)\\/\\nok$\",\n        r\"^(.*?)\\s\\.\\.\\.\\sSystem check identified no issues \\(0 silenced\\)\\nok$\",\n    ]\n    for pattern in patterns:\n        for match in re.finditer(pattern, log, re.MULTILINE):\n            test_name = match.group(1)\n            test_status_map[test_name] = TestStatus.PASSED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_pytest_v2","title":"parse_log_pytest_v2","text":"<pre><code>parse_log_pytest_v2(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with PyTest framework (Later Version)</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_pytest_v2(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework (Later Version)\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    escapes = \"\".join([chr(char) for char in range(1, 32)])\n    for line in log.split(\"\\n\"):\n        line = re.sub(r\"\\[(\\d+)m\", \"\", line)\n        translator = str.maketrans(\"\", \"\", escapes)\n        line = line.translate(translator)\n        if any([line.startswith(x.value) for x in TestStatus]):\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(\" - \", \" \")\n            test_case = line.split()\n            if len(test_case) &gt;= 2:\n                test_status_map[test_case[1]] = test_case[0]\n        # Support older pytest versions by checking if the line ends with the test status\n        elif any([line.endswith(x.value) for x in TestStatus]):\n            test_case = line.split()\n            if len(test_case) &gt;= 2:\n                test_status_map[test_case[0]] = test_case[1]\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_seaborn","title":"parse_log_seaborn","text":"<pre><code>parse_log_seaborn(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with seaborn testing framework</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_seaborn(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with seaborn testing framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    for line in log.split(\"\\n\"):\n        if line.startswith(TestStatus.FAILED.value):\n            test_case = line.split()[1]\n            test_status_map[test_case] = TestStatus.FAILED.value\n        elif f\" {TestStatus.PASSED.value} \" in line:\n            parts = line.split()\n            if parts[1] == TestStatus.PASSED.value:\n                test_case = parts[0]\n                test_status_map[test_case] = TestStatus.PASSED.value\n        elif line.startswith(TestStatus.PASSED.value):\n            parts = line.split()\n            test_case = parts[1]\n            test_status_map[test_case] = TestStatus.PASSED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_sympy","title":"parse_log_sympy","text":"<pre><code>parse_log_sympy(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with Sympy framework</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_sympy(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Sympy framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    pattern = r\"(_*) (.*)\\.py:(.*) (_*)\"\n    matches = re.findall(pattern, log)\n    for match in matches:\n        test_case = f\"{match[1]}.py:{match[2]}\"\n        test_status_map[test_case] = TestStatus.FAILED.value\n    for line in log.split(\"\\n\"):\n        line = line.strip()\n        if line.startswith(\"test_\"):\n            if line.endswith(\" E\"):\n                test = line.split()[0]\n                test_status_map[test] = TestStatus.ERROR.value\n            if line.endswith(\" F\"):\n                test = line.split()[0]\n                test_status_map[test] = TestStatus.FAILED.value\n            if line.endswith(\" ok\"):\n                test = line.split()[0]\n                test_status_map[test] = TestStatus.PASSED.value\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.python.parse_log_matplotlib","title":"parse_log_matplotlib","text":"<pre><code>parse_log_matplotlib(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parser for test logs generated with PyTest framework</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/python.py</code> <pre><code>def parse_log_matplotlib(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    for line in log.split(\"\\n\"):\n        line = line.replace(\"MouseButton.LEFT\", \"1\")\n        line = line.replace(\"MouseButton.RIGHT\", \"3\")\n        if any([line.startswith(x.value) for x in TestStatus]):\n            # Additional parsing for FAILED status\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(\" - \", \" \")\n            test_case = line.split()\n            if len(test_case) &lt;= 1:\n                continue\n            test_status_map[test_case[1]] = test_case[0]\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.ruby","title":"ruby","text":""},{"location":"api/harness/#swebench.harness.log_parsers.ruby.MAP_REPO_TO_PARSER_RUBY","title":"MAP_REPO_TO_PARSER_RUBY  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_RUBY = {'jekyll/jekyll': parse_log_jekyll, 'fluent/fluentd': parse_log_ruby_unit, 'fastlane/fastlane': parse_log_rspec_transformed_json, 'jordansissel/fpm': parse_log_rspec_transformed_json, 'faker-ruby/faker': parse_log_ruby_unit, 'rubocop/rubocop': parse_log_rspec_transformed_json}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.ruby.parse_log_minitest","title":"parse_log_minitest","text":"<pre><code>parse_log_minitest(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/ruby.py</code> <pre><code>def parse_log_minitest(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^(.+)\\. .*=.*(\\.|F|E).*$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            test_name, outcome = match.groups()\n            if outcome == \".\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif outcome in [\"F\", \"E\"]:\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.ruby.parse_log_cucumber","title":"parse_log_cucumber","text":"<pre><code>parse_log_cucumber(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Assumes --format progress is used.</p> Source code in <code>swebench/harness/log_parsers/ruby.py</code> <pre><code>def parse_log_cucumber(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Assumes --format progress is used.\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^(.*) \\.+(\\.|F)\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            test_name, outcome = match.groups()\n            if outcome == \".\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif outcome == \"F\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.ruby.parse_log_ruby_unit","title":"parse_log_ruby_unit","text":"<pre><code>parse_log_ruby_unit(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> Source code in <code>swebench/harness/log_parsers/ruby.py</code> <pre><code>def parse_log_ruby_unit(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    test_status_map = {}\n\n    pattern = r\"^\\s*(?:test: )?(.+):\\s+(\\.|E\\b|F\\b|O\\b)\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            test_name, outcome = match.groups()\n            if outcome == \".\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif outcome in [\"E\", \"F\"]:\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif outcome == \"O\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.ruby.parse_log_rspec_transformed_json","title":"parse_log_rspec_transformed_json","text":"<pre><code>parse_log_rspec_transformed_json(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> Source code in <code>swebench/harness/log_parsers/ruby.py</code> <pre><code>def parse_log_rspec_transformed_json(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    test_status_map = {}\n\n    pattern = r\"(.+) - (passed|failed)\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            test_name, outcome = match.groups()\n            if outcome == \"passed\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif outcome == \"failed\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n            elif outcome == \"pending\":\n                test_status_map[test_name] = TestStatus.SKIPPED.value\n            else:\n                raise ValueError(f\"Unknown outcome: {outcome}\")\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.ruby.parse_log_jekyll","title":"parse_log_jekyll","text":"<pre><code>parse_log_jekyll(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Different jekyll instances use different test runners and log formats. This function selects the appropriate log parser based on the instance id.</p> Source code in <code>swebench/harness/log_parsers/ruby.py</code> <pre><code>def parse_log_jekyll(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Different jekyll instances use different test runners and log formats.\n    This function selects the appropriate log parser based on the instance id.\n    \"\"\"\n    pr_number = test_spec.instance_id.split(\"-\")[1]\n\n    if pr_number in [\"9141\", \"8047\", \"8167\"]:\n        return parse_log_minitest(log, test_spec)\n    elif pr_number in [\"8761\", \"8771\"]:\n        return parse_log_cucumber(log, test_spec)\n    else:\n        raise ValueError(f\"Unknown instance id: {test_spec.instance_id}\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.rust","title":"rust","text":""},{"location":"api/harness/#swebench.harness.log_parsers.rust.MAP_REPO_TO_PARSER_RUST","title":"MAP_REPO_TO_PARSER_RUST  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_PARSER_RUST = {'burntsushi/ripgrep': parse_log_cargo, 'sharkdp/bat': parse_log_cargo, 'astral-sh/ruff': parse_log_cargo, 'tokio-rs/tokio': parse_log_cargo, 'uutils/coreutils': parse_log_cargo, 'nushell/nushell': parse_log_cargo, 'tokio-rs/axum': parse_log_cargo}\n</code></pre>"},{"location":"api/harness/#swebench.harness.log_parsers.rust.parse_log_cargo","title":"parse_log_cargo","text":"<pre><code>parse_log_cargo(log: str, test_spec: TestSpec) -&gt; dict[str, str]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>str</code> <p>log content</p> required <p>Returns:     dict: test case to test status mapping</p> Source code in <code>swebench/harness/log_parsers/rust.py</code> <pre><code>def parse_log_cargo(log: str, test_spec: TestSpec) -&gt; dict[str, str]:\n    \"\"\"\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n\n    pattern = r\"^test\\s+(\\S+)\\s+\\.\\.\\.\\s+(\\w+)$\"\n\n    for line in log.split(\"\\n\"):\n        match = re.match(pattern, line.strip())\n        if match:\n            test_name, outcome = match.groups()\n            if outcome == \"ok\":\n                test_status_map[test_name] = TestStatus.PASSED.value\n            elif outcome == \"FAILED\":\n                test_status_map[test_name] = TestStatus.FAILED.value\n\n    return test_status_map\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval","title":"modal_eval","text":""},{"location":"api/harness/#swebench.harness.modal_eval.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['run_instances_modal', 'validate_modal_credentials']\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_instances_modal","title":"run_instances_modal","text":"<pre><code>run_instances_modal(predictions: dict, instances: list, full_dataset: list, run_id: str, timeout: int)\n</code></pre> <p>Run all instances for the given predictions on Modal.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>Predictions dict generated by the model</p> required <code>instances</code> <code>list</code> <p>List of instances</p> required <code>run_id</code> <code>str</code> <p>Run ID</p> required <code>timeout</code> <code>int</code> <p>Timeout for running tests</p> required Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def run_instances_modal(\n    predictions: dict,\n    instances: list,\n    full_dataset: list,\n    run_id: str,\n    timeout: int,\n):\n    \"\"\"\n    Run all instances for the given predictions on Modal.\n\n    Args:\n        predictions (dict): Predictions dict generated by the model\n        instances (list): List of instances\n        run_id (str): Run ID\n        timeout (int): Timeout for running tests\n    \"\"\"\n    test_specs = list(map(make_test_spec, instances))\n\n    with modal.enable_output():\n        with app.run():\n            run_test_specs = []\n\n            # Check for instances that have already been run\n            for test_spec in test_specs:\n                log_dir = get_log_dir(\n                    predictions[test_spec.instance_id], run_id, test_spec.instance_id\n                )\n                if log_dir.exists():\n                    continue\n                run_test_specs.append(test_spec)\n\n            if run_test_specs:\n                # Run instances that haven't been run yet\n                results = run_instance_modal.starmap(\n                    [\n                        (\n                            test_spec,\n                            predictions[test_spec.instance_id],\n                            run_id,\n                            timeout,\n                        )\n                        for test_spec in run_test_specs\n                    ],\n                    return_exceptions=True,\n                )\n\n                for result in results:\n                    if not isinstance(result, TestOutput):\n                        print(f\"Result failed with error: {result}\")\n                        continue\n\n                    # Save logs locally\n                    log_dir = result.log_dir\n                    log_dir.mkdir(parents=True, exist_ok=True)\n                    with open(log_dir / \"run_instance.log\", \"w\") as f:\n                        f.write(result.run_instance_log)\n                    with open(log_dir / \"test_output.txt\", \"w\") as f:\n                        f.write(result.test_output)\n                    with open(log_dir / \"patch.diff\", \"w\") as f:\n                        f.write(result.patch_diff)\n                    with open(log_dir / \"report.json\", \"w\") as f:\n                        try:\n                            report_json = json.loads(result.report_json_str)\n                            json.dump(report_json, f, indent=4)\n                        except Exception:\n                            # This happens if the test fails with any exception\n                            print(f\"{result.instance_id}: no report.json\")\n\n            make_run_report(predictions, full_dataset, run_id)\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.validate_modal_credentials","title":"validate_modal_credentials","text":"<pre><code>validate_modal_credentials()\n</code></pre> <p>Validate that Modal credentials exist by checking for ~/.modal.toml file. Raises an exception if credentials are not configured.</p> Source code in <code>swebench/harness/modal_eval/utils.py</code> <pre><code>def validate_modal_credentials():\n    \"\"\"\n    Validate that Modal credentials exist by checking for ~/.modal.toml file.\n    Raises an exception if credentials are not configured.\n    \"\"\"\n    modal_config_path = Path.home() / \".modal.toml\"\n    if not modal_config_path.exists():\n        raise RuntimeError(\n            \"~/.modal.toml not found - it looks like you haven't configured credentials for Modal.\\n\"\n            \"Run 'modal token new' in your terminal to configure credentials.\"\n        )\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal","title":"run_evaluation_modal","text":""},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.SANDBOX_ENTRYPOINT","title":"SANDBOX_ENTRYPOINT  <code>module-attribute</code>","text":"<pre><code>SANDBOX_ENTRYPOINT = 'run_evaluation_modal_entrypoint'\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.LOCAL_SANDBOX_ENTRYPOINT_PATH","title":"LOCAL_SANDBOX_ENTRYPOINT_PATH  <code>module-attribute</code>","text":"<pre><code>LOCAL_SANDBOX_ENTRYPOINT_PATH = resolve()\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.REMOTE_SANDBOX_ENTRYPOINT_PATH","title":"REMOTE_SANDBOX_ENTRYPOINT_PATH  <code>module-attribute</code>","text":"<pre><code>REMOTE_SANDBOX_ENTRYPOINT_PATH = f'/root/{SANDBOX_ENTRYPOINT}.py'\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = App('swebench-evaluation')\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.swebench_image","title":"swebench_image  <code>module-attribute</code>","text":"<pre><code>swebench_image = pip_install('swebench', 'tenacity')\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.TestOutput","title":"TestOutput  <code>dataclass</code>","text":"<pre><code>TestOutput(instance_id: str, test_output: str, report_json_str: str, run_instance_log: str, patch_diff: str, log_dir: Path, errored: bool)\n</code></pre> <code></code> instance_id <code>instance-attribute</code> <pre><code>instance_id: str\n</code></pre> <code></code> test_output <code>instance-attribute</code> <pre><code>test_output: str\n</code></pre> <code></code> report_json_str <code>instance-attribute</code> <pre><code>report_json_str: str\n</code></pre> <code></code> run_instance_log <code>instance-attribute</code> <pre><code>run_instance_log: str\n</code></pre> <code></code> patch_diff <code>instance-attribute</code> <pre><code>patch_diff: str\n</code></pre> <code></code> log_dir <code>instance-attribute</code> <pre><code>log_dir: Path\n</code></pre> <code></code> errored <code>instance-attribute</code> <pre><code>errored: bool\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.ModalSandboxRuntime","title":"ModalSandboxRuntime","text":"<pre><code>ModalSandboxRuntime(test_spec: TestSpec, timeout: int | None = None, verbose: bool = True)\n</code></pre> <p>Runtime for running instances in a Modal Sandbox.</p> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def __init__(\n    self, test_spec: TestSpec, timeout: int | None = None, verbose: bool = True\n):\n    self.test_spec = test_spec\n    self.image = ModalSandboxRuntime.get_instance_image(test_spec)\n    self.sandbox = self._get_sandbox(timeout)\n    self.verbose = verbose\n    self._stream_tasks = []\n\n    # Hack for pylint\n    self.write_file(\"/sys/fs/cgroup/cpu/cpu.shares\", \"2048\")\n</code></pre> <code></code> test_spec <code>instance-attribute</code> <pre><code>test_spec = test_spec\n</code></pre> <code></code> image <code>instance-attribute</code> <pre><code>image = get_instance_image(test_spec)\n</code></pre> <code></code> sandbox <code>instance-attribute</code> <pre><code>sandbox = _get_sandbox(timeout)\n</code></pre> <code></code> verbose <code>instance-attribute</code> <pre><code>verbose = verbose\n</code></pre> <code></code> write_file <pre><code>write_file(file_path: str, content: str)\n</code></pre> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def write_file(self, file_path: str, content: str):\n    self.sandbox.open(file_path, \"w\").write(content)\n</code></pre> <code></code> exec <pre><code>exec(command: str) -&gt; tuple[str, int]\n</code></pre> <p>Execute a command in the sandbox.</p> <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>tuple[str, int]: Sandbox output and return code.</p> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def exec(self, command: str) -&gt; tuple[str, int]:\n    \"\"\"\n    Execute a command in the sandbox.\n\n    Returns:\n        tuple[str, int]: Sandbox output and return code.\n    \"\"\"\n    p = self.sandbox.exec(\"python\", \"-m\", SANDBOX_ENTRYPOINT, command)\n    stdout = []\n    stderr = []\n    try:\n        # We separate stdout/stderr because some tests rely on them being separate.\n        # We still read stdout/stderr simultaneously to continuously\n        # flush both streams and avoid blocking.\n        asyncio.run(self._read_output(p, stdout, stderr))\n    except Exception as e:\n        print(f\"Error during command execution: {e}\")\n    p.wait()\n    return \"\".join(stdout + stderr), p.returncode\n</code></pre> <code></code> __exit__ <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    if self._stream_tasks:\n        try:\n            # Forcefully kill remaining streams\n            for task in self._stream_tasks:\n                if not task.done():\n                    task.cancel()\n                    try:\n                        asyncio.wait_for(task, timeout=0.1)\n                    except asyncio.TimeoutError:\n                        pass\n                    except Exception:\n                        pass\n\n            self.sandbox.terminate()\n        except Exception:\n            pass\n        finally:\n            self._stream_tasks = []\n</code></pre> <code></code> get_instance_image <code>staticmethod</code> <pre><code>get_instance_image(test_spec: TestSpec) -&gt; Image\n</code></pre> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>    @staticmethod\n    def get_instance_image(test_spec: TestSpec) -&gt; modal.Image:\n        env_script = test_spec.setup_env_script\n        # add trusted host flag for Modal's PyPI mirror\n        env_script = env_script.replace(\n            \"conda activate testbed &amp;&amp; python -m pip install -r $HOME/requirements.txt\",\n            \"conda activate testbed &amp;&amp; python -m pip install --trusted-host pypi-mirror.modal.local -r $HOME/requirements.txt\",\n        )\n        repo_script = test_spec.install_repo_script\n\n        remote_env_script_path = \"/root/setup_env.sh\"\n        remote_repo_script_path = \"/root/setup_repo.sh\"\n\n        Path(remote_env_script_path).write_text(env_script)\n        Path(remote_repo_script_path).write_text(repo_script)\n\n        # Modal automatically caches images\n        # https://modal.com/docs/guide/custom-container#image-caching-and-rebuilds\n        return (\n            modal.Image.from_registry(\"ubuntu:22.04\", add_python=\"3.11\")\n            .run_commands(\"apt update\")\n            .env({\"DEBIAN_FRONTEND\": \"noninteractive\", \"TZ\": \"Etc/UTC\"})\n            .apt_install(\n                \"wget\",\n                \"git\",\n                \"build-essential\",\n                \"libffi-dev\",\n                \"libtiff-dev\",\n                \"jq\",\n                \"curl\",\n                \"locales\",\n                \"locales-all\",\n                \"tzdata\",\n            )\n            .run_commands(\n                \"wget 'https://repo.anaconda.com/miniconda/Miniconda3-py311_23.11.0-2-Linux-x86_64.sh' -O miniconda.sh\",\n                \"bash miniconda.sh -b -p /opt/miniconda3\",\n                \"echo 'export PATH=/opt/miniconda3/bin:$PATH' &gt;&gt; ~/.bashrc\",\n                \"/opt/miniconda3/bin/conda init --all\",\n                \"/opt/miniconda3/bin/conda config --append channels conda-forge\",\n                \"adduser --disabled-password --gecos 'dog' nonroot\",\n            )\n            .add_local_file(\nPath(remote_env_script_path), remote_env_script_path, copy=True\n            )\n            .add_local_file(\n                Path(remote_repo_script_path), remote_repo_script_path, copy=True\n            )\n            .run_commands(\n                f\"chmod +x {remote_env_script_path}\",\n                f\"/bin/bash -c 'source ~/.bashrc &amp;&amp; {remote_env_script_path}'\",\n                \"echo 'source /opt/miniconda3/etc/profile.d/conda.sh &amp;&amp; conda activate testbed' &gt;&gt; /root/.bashrc\",\n                f\"/bin/bash {remote_repo_script_path}\",\n            )\n            .workdir(\"/testbed/\")\n        )\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.get_log_dir","title":"get_log_dir","text":"<pre><code>get_log_dir(pred: dict, run_id: str, instance_id: str) -&gt; Path\n</code></pre> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def get_log_dir(pred: dict, run_id: str, instance_id: str) -&gt; Path:\n    model_name_or_path = cast(\n        str, pred.get(\"model_name_or_path\", \"None\").replace(\"/\", \"__\")\n    )\n    return RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.run_instance_modal","title":"run_instance_modal","text":"<pre><code>run_instance_modal(test_spec: TestSpec, pred: dict, run_id: str, timeout: int | None = None) -&gt; TestOutput\n</code></pre> <p>Run a single instance with the given prediction.</p> <p>Parameters:</p> Name Type Description Default <code>test_spec</code> <code>TestSpec</code> <p>TestSpec instance</p> required <code>pred</code> <code>dict</code> <p>Prediction w/ model_name_or_path, model_patch, instance_id</p> required <code>run_id</code> <code>str</code> <p>Run ID</p> required <code>timeout</code> <code>int</code> <p>Timeout for running tests</p> <code>None</code> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>@app.function(\n    image=swebench_image.add_local_file(\n        LOCAL_SANDBOX_ENTRYPOINT_PATH,\n        REMOTE_SANDBOX_ENTRYPOINT_PATH,\n    ),\n    timeout=120\n    * 60,  # Much larger than default timeout to account for image build time\n    include_source=True,\n)\ndef run_instance_modal(\n    test_spec: TestSpec,\n    pred: dict,\n    run_id: str,\n    timeout: int | None = None,\n) -&gt; TestOutput:\n    \"\"\"\n    Run a single instance with the given prediction.\n\n    Args:\n        test_spec (TestSpec): TestSpec instance\n        pred (dict): Prediction w/ model_name_or_path, model_patch, instance_id\n        run_id (str): Run ID\n        timeout (int): Timeout for running tests\n    \"\"\"\n    instance_id = test_spec.instance_id\n    log_dir = get_log_dir(pred, run_id, instance_id)\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    log_file = log_dir / \"run_instance.log\"\n\n    logger = setup_logger(instance_id, log_file, add_stdout=True)\n\n    try:\n        runner = ModalSandboxRuntime(test_spec, timeout)\n    except Exception as e:\n        print(f\"Error creating sandbox: {e}\")\n        raise EvaluationError(\n            instance_id,\n            f\"Error creating sandbox: {e}\",\n            logger,\n        ) from e\n\n    patch_diff = pred.get(\"model_patch\", \"\")\n\n    try:\n        patch_file = \"/tmp/patch.diff\"\n        runner.write_file(patch_file, patch_diff)\n\n        apply_patch_output, returncode = runner.exec(\n            \"cd /testbed &amp;&amp; git apply -v /tmp/patch.diff\",\n        )\n\n        if returncode != 0:\n            logger.info(\"Failed to apply patch to container, trying again...\")\n\n            apply_patch_output, returncode = runner.exec(\n                \"cd /testbed &amp;&amp; patch --batch --fuzz=5 -p1 -i /tmp/patch.diff\",\n            )\n\n            if returncode != 0:\n                logger.info(f\"{APPLY_PATCH_FAIL}:\\n{apply_patch_output}\")\n                raise EvaluationError(\n                    instance_id,\n                    f\"{APPLY_PATCH_FAIL}:\\n{apply_patch_output}\",\n                    logger,\n                )\n            else:\n                logger.info(f\"{APPLY_PATCH_PASS}:\\n{apply_patch_output}\")\n        else:\n            logger.info(f\"{APPLY_PATCH_PASS}:\\n{apply_patch_output}\")\n\n        # Get git diff before running eval script\n        git_diff_output_before, returncode = runner.exec(\n            \"cd /testbed &amp;&amp; git diff\",\n        )\n        logger.info(f\"Git diff before:\\n{git_diff_output_before}\")\n\n        eval_file = \"/root/eval.sh\"\n        eval_script = test_spec.eval_script\n        # django hack\n        eval_script = eval_script.replace(\"locale-gen\", \"locale-gen en_US.UTF-8\")\n        runner.write_file(eval_file, eval_script)\n\n        start_time = time.time()\n\n        run_command = \"cd /testbed\"\n        # pylint hack\n        if \"pylint\" in test_spec.instance_id:\n            run_command += \" &amp;&amp; PYTHONPATH=\"\n        # increase recursion limit for testing\n        run_command += \" &amp;&amp; python3 -c 'import sys; sys.setrecursionlimit(10000)'\"\n        # run eval script\n        run_command += \" &amp;&amp; /bin/bash /root/eval.sh\"\n        test_output, returncode = runner.exec(run_command)\n\n        total_runtime = time.time() - start_time\n\n        test_output_path = log_dir / \"test_output.txt\"\n        logger.info(f\"Test runtime: {total_runtime:_.2f} seconds\")\n        with open(test_output_path, \"w\") as f:\n            f.write(test_output)\n            logger.info(f\"Test output for {instance_id} written to {test_output_path}\")\n            print(f\"Test output for {instance_id} written to {test_output_path}\")\n\n        # Get git diff after running eval script\n        git_diff_output_after, returncode = runner.exec(\"cd /testbed &amp;&amp; git diff\")\n\n        # Check if git diff changed after running eval script\n        logger.info(f\"Git diff after:\\n{git_diff_output_after}\")\n        if git_diff_output_after != git_diff_output_before:\n            logger.info(\"Git diff changed after running eval script\")\n\n        # Get report from test output\n        logger.info(f\"Grading answer for {instance_id}...\")\n        report = get_eval_report(\n            test_spec=test_spec,\n            prediction=pred,\n            test_log_path=test_output_path,\n            include_tests_status=True,\n        )\n        logger.info(\n            f\"report: {report}\\n\"\n            f\"Result for {instance_id}: resolved: {report[instance_id]['resolved']}\"\n        )\n\n        return TestOutput(\n            instance_id=instance_id,\n            test_output=test_output,\n            report_json_str=json.dumps(report, indent=4),\n            run_instance_log=log_file.read_text(),\n            patch_diff=patch_diff,\n            log_dir=log_dir,\n            errored=False,\n        )\n    except modal.exception.SandboxTimeoutError as e:\n        raise EvaluationError(\n            instance_id,\n            f\"Test timed out after {timeout} seconds.\",\n            logger,\n        ) from e\n    except EvaluationError:\n        error_msg = traceback.format_exc()\n        logger.info(error_msg)\n        return TestOutput(\n            instance_id=instance_id,\n            test_output=\"\",\n            report_json_str=\"\",\n            run_instance_log=log_file.read_text(),\n            patch_diff=patch_diff,\n            log_dir=log_dir,\n            errored=True,\n        )\n    except Exception as e:\n        error_msg = (\n            f\"Error in evaluating model for {instance_id}: {e}\\n\"\n            f\"{traceback.format_exc()}\\n\"\n            f\"Check ({logger.log_file}) for more information.\"\n        )\n        logger.error(error_msg)\n        return TestOutput(\n            instance_id=instance_id,\n            test_output=\"\",\n            report_json_str=\"\",\n            run_instance_log=log_file.read_text(),\n            patch_diff=patch_diff,\n            log_dir=log_dir,\n            errored=True,\n        )\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal.run_instances_modal","title":"run_instances_modal","text":"<pre><code>run_instances_modal(predictions: dict, instances: list, full_dataset: list, run_id: str, timeout: int)\n</code></pre> <p>Run all instances for the given predictions on Modal.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>Predictions dict generated by the model</p> required <code>instances</code> <code>list</code> <p>List of instances</p> required <code>run_id</code> <code>str</code> <p>Run ID</p> required <code>timeout</code> <code>int</code> <p>Timeout for running tests</p> required Source code in <code>swebench/harness/modal_eval/run_evaluation_modal.py</code> <pre><code>def run_instances_modal(\n    predictions: dict,\n    instances: list,\n    full_dataset: list,\n    run_id: str,\n    timeout: int,\n):\n    \"\"\"\n    Run all instances for the given predictions on Modal.\n\n    Args:\n        predictions (dict): Predictions dict generated by the model\n        instances (list): List of instances\n        run_id (str): Run ID\n        timeout (int): Timeout for running tests\n    \"\"\"\n    test_specs = list(map(make_test_spec, instances))\n\n    with modal.enable_output():\n        with app.run():\n            run_test_specs = []\n\n            # Check for instances that have already been run\n            for test_spec in test_specs:\n                log_dir = get_log_dir(\n                    predictions[test_spec.instance_id], run_id, test_spec.instance_id\n                )\n                if log_dir.exists():\n                    continue\n                run_test_specs.append(test_spec)\n\n            if run_test_specs:\n                # Run instances that haven't been run yet\n                results = run_instance_modal.starmap(\n                    [\n                        (\n                            test_spec,\n                            predictions[test_spec.instance_id],\n                            run_id,\n                            timeout,\n                        )\n                        for test_spec in run_test_specs\n                    ],\n                    return_exceptions=True,\n                )\n\n                for result in results:\n                    if not isinstance(result, TestOutput):\n                        print(f\"Result failed with error: {result}\")\n                        continue\n\n                    # Save logs locally\n                    log_dir = result.log_dir\n                    log_dir.mkdir(parents=True, exist_ok=True)\n                    with open(log_dir / \"run_instance.log\", \"w\") as f:\n                        f.write(result.run_instance_log)\n                    with open(log_dir / \"test_output.txt\", \"w\") as f:\n                        f.write(result.test_output)\n                    with open(log_dir / \"patch.diff\", \"w\") as f:\n                        f.write(result.patch_diff)\n                    with open(log_dir / \"report.json\", \"w\") as f:\n                        try:\n                            report_json = json.loads(result.report_json_str)\n                            json.dump(report_json, f, indent=4)\n                        except Exception:\n                            # This happens if the test fails with any exception\n                            print(f\"{result.instance_id}: no report.json\")\n\n            make_run_report(predictions, full_dataset, run_id)\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal_entrypoint","title":"run_evaluation_modal_entrypoint","text":""},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal_entrypoint.STDIO_RATE_LIMIT_BYTES_PER_SEC","title":"STDIO_RATE_LIMIT_BYTES_PER_SEC  <code>module-attribute</code>","text":"<pre><code>STDIO_RATE_LIMIT_BYTES_PER_SEC = 64 * 1024 // 2\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal_entrypoint.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description='Execute a shell command and stream output')\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal_entrypoint.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal_entrypoint.exec","title":"exec  <code>async</code>","text":"<pre><code>exec(command: str) -&gt; int\n</code></pre> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal_entrypoint.py</code> <pre><code>async def exec(command: str) -&gt; int:\n    p = await asyncio.create_subprocess_shell(\n        command,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n        limit=1024 * 1024,\n    )\n\n    stdout_lines = []\n    stderr_lines = []\n\n    async def read_stream(stream, lines, fd):\n        tokens = STDIO_RATE_LIMIT_BYTES_PER_SEC\n        last_refill = asyncio.get_event_loop().time()\n\n        while True:\n            try:\n                line = await stream.readline()\n                if not line:\n                    break\n            except (asyncio.LimitOverrunError, ValueError):\n                # buffer exceeded asyncio stream limit\n                fallback_chunk_size = 8192\n                line = await stream.read(fallback_chunk_size)\n                if not line:\n                    break\n\n            remaining_data = line\n            buffer = bytearray()\n\n            while remaining_data:\n                current_time = asyncio.get_event_loop().time()\n                time_passed = current_time - last_refill\n\n                tokens = min(\n                    STDIO_RATE_LIMIT_BYTES_PER_SEC,\n                    tokens + (time_passed * STDIO_RATE_LIMIT_BYTES_PER_SEC),\n                )\n                last_refill = current_time\n\n                chunk_size = min(\n                    len(remaining_data), STDIO_RATE_LIMIT_BYTES_PER_SEC, int(tokens)\n                )\n\n                if chunk_size == 0:\n                    sleep_time = max(\n                        0.01,\n                        (0.01 * STDIO_RATE_LIMIT_BYTES_PER_SEC - tokens)\n                        / STDIO_RATE_LIMIT_BYTES_PER_SEC,\n                    )\n                    await asyncio.sleep(sleep_time)\n                    continue\n\n                buffer.extend(remaining_data[:chunk_size])\n\n                # Find last valid UTF-8 character boundary.\n                # This is to avoid partial characters being written to\n                # container stdout/stderr, which results in a very small\n                # chance of errors of the form: \"Error reading stream: 'utf-8' codec can't decode bytes in position ...\"\n                valid_bytes = len(\n                    buffer.decode(\"utf-8\", errors=\"ignore\").encode(\"utf-8\")\n                )\n\n                if valid_bytes &gt; 0:\n                    chunk = buffer[:valid_bytes]\n                    if fd == \"stdout\":\n                        sys.stdout.buffer.write(chunk)\n                        sys.stdout.buffer.flush()\n                    else:\n                        sys.stderr.buffer.write(chunk)\n                        sys.stderr.buffer.flush()\n\n                    buffer = buffer[valid_bytes:]\n                    tokens -= valid_bytes\n\n                remaining_data = remaining_data[chunk_size:]\n\n            if buffer:\n                if fd == \"stdout\":\n                    sys.stdout.buffer.write(buffer)\n                    sys.stdout.buffer.flush()\n                else:\n                    sys.stderr.buffer.write(buffer)\n                    sys.stderr.buffer.flush()\n\n            lines.append(line)\n\n    await asyncio.gather(\n        read_stream(p.stdout, stdout_lines, \"stdout\"),\n        read_stream(p.stderr, stderr_lines, \"stderr\"),\n    )\n\n    return await p.wait()\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.run_evaluation_modal_entrypoint.main","title":"main  <code>async</code>","text":"<pre><code>main(command: str)\n</code></pre> Source code in <code>swebench/harness/modal_eval/run_evaluation_modal_entrypoint.py</code> <pre><code>async def main(command: str):\n    returncode = await exec(command)\n    exit(returncode)\n</code></pre>"},{"location":"api/harness/#swebench.harness.modal_eval.utils","title":"utils","text":""},{"location":"api/harness/#swebench.harness.modal_eval.utils.validate_modal_credentials","title":"validate_modal_credentials","text":"<pre><code>validate_modal_credentials()\n</code></pre> <p>Validate that Modal credentials exist by checking for ~/.modal.toml file. Raises an exception if credentials are not configured.</p> Source code in <code>swebench/harness/modal_eval/utils.py</code> <pre><code>def validate_modal_credentials():\n    \"\"\"\n    Validate that Modal credentials exist by checking for ~/.modal.toml file.\n    Raises an exception if credentials are not configured.\n    \"\"\"\n    modal_config_path = Path.home() / \".modal.toml\"\n    if not modal_config_path.exists():\n        raise RuntimeError(\n            \"~/.modal.toml not found - it looks like you haven't configured credentials for Modal.\\n\"\n            \"Run 'modal token new' in your terminal to configure credentials.\"\n        )\n</code></pre>"},{"location":"api/harness/#swebench.harness.prepare_images","title":"prepare_images","text":""},{"location":"api/harness/#swebench.harness.prepare_images.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser()\n</code></pre>"},{"location":"api/harness/#swebench.harness.prepare_images.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/harness/#swebench.harness.prepare_images.filter_dataset_to_build","title":"filter_dataset_to_build","text":"<pre><code>filter_dataset_to_build(dataset: list, instance_ids: list | None, client: DockerClient, force_rebuild: bool, namespace: str = None, tag: str = None)\n</code></pre> <p>Filter the dataset to only include instances that need to be built.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>List of instances (usually all of SWE-bench dev/test split)</p> required <code>instance_ids</code> <code>list</code> <p>List of instance IDs to build.</p> required <code>client</code> <code>DockerClient</code> <p>Docker client.</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild all images.</p> required Source code in <code>swebench/harness/prepare_images.py</code> <pre><code>def filter_dataset_to_build(\n    dataset: list,\n    instance_ids: list | None,\n    client: docker.DockerClient,\n    force_rebuild: bool,\n    namespace: str = None,\n    tag: str = None,\n):\n    \"\"\"\n    Filter the dataset to only include instances that need to be built.\n\n    Args:\n        dataset (list): List of instances (usually all of SWE-bench dev/test split)\n        instance_ids (list): List of instance IDs to build.\n        client (docker.DockerClient): Docker client.\n        force_rebuild (bool): Whether to force rebuild all images.\n    \"\"\"\n    # Get existing images\n    existing_images = list_images(client)\n    data_to_build = []\n\n    if instance_ids is None:\n        instance_ids = [instance[KEY_INSTANCE_ID] for instance in dataset]\n\n    # Check if all instance IDs are in the dataset\n    not_in_dataset = set(instance_ids).difference(\n        set([instance[KEY_INSTANCE_ID] for instance in dataset])\n    )\n    if not_in_dataset:\n        raise ValueError(f\"Instance IDs not found in dataset: {not_in_dataset}\")\n\n    for instance in dataset:\n        if instance[KEY_INSTANCE_ID] not in instance_ids:\n            # Skip instances not in the list\n            continue\n\n        # Check if the instance needs to be built (based on force_rebuild flag and existing images)\n        spec = make_test_spec(instance, namespace=namespace, instance_image_tag=tag)\n        if force_rebuild:\n            data_to_build.append(instance)\n        elif spec.instance_image_key not in existing_images:\n            data_to_build.append(instance)\n\n    return data_to_build\n</code></pre>"},{"location":"api/harness/#swebench.harness.prepare_images.main","title":"main","text":"<pre><code>main(dataset_name, split, instance_ids, max_workers, force_rebuild, open_file_limit, namespace, tag)\n</code></pre> <p>Build Docker images for the specified instances.</p> <p>Parameters:</p> Name Type Description Default <code>instance_ids</code> <code>list</code> <p>List of instance IDs to build.</p> required <code>max_workers</code> <code>int</code> <p>Number of workers for parallel processing.</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild all images.</p> required <code>open_file_limit</code> <code>int</code> <p>Open file limit.</p> required Source code in <code>swebench/harness/prepare_images.py</code> <pre><code>def main(\n    dataset_name,\n    split,\n    instance_ids,\n    max_workers,\n    force_rebuild,\n    open_file_limit,\n    namespace,\n    tag,\n):\n    \"\"\"\n    Build Docker images for the specified instances.\n\n    Args:\n        instance_ids (list): List of instance IDs to build.\n        max_workers (int): Number of workers for parallel processing.\n        force_rebuild (bool): Whether to force rebuild all images.\n        open_file_limit (int): Open file limit.\n    \"\"\"\n    # Set open file limit\n    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n    client = docker.from_env()\n\n    # Filter out instances that were not specified\n    dataset = load_swebench_dataset(dataset_name, split)\n    dataset = filter_dataset_to_build(\n        dataset, instance_ids, client, force_rebuild, namespace, tag\n    )\n\n    if len(dataset) == 0:\n        print(\"All images exist. Nothing left to build.\")\n        return 0\n\n    # Build images for remaining instances\n    successful, failed = build_instance_images(\n        client=client,\n        dataset=dataset,\n        force_rebuild=force_rebuild,\n        max_workers=max_workers,\n        namespace=namespace,\n        tag=tag,\n    )\n    print(f\"Successfully built {len(successful)} images\")\n    print(f\"Failed to build {len(failed)} images\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.remove_containers","title":"remove_containers","text":""},{"location":"api/harness/#swebench.harness.remove_containers.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description=__doc__)\n</code></pre>"},{"location":"api/harness/#swebench.harness.remove_containers.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/harness/#swebench.harness.remove_containers.instance_ids","title":"instance_ids  <code>module-attribute</code>","text":"<pre><code>instance_ids = [(strip()) for i in (split(','))] if instance_ids else []\n</code></pre>"},{"location":"api/harness/#swebench.harness.remove_containers.main","title":"main","text":"<pre><code>main(instance_ids, predictions_path)\n</code></pre> Source code in <code>swebench/harness/remove_containers.py</code> <pre><code>def main(instance_ids, predictions_path):\n    all_ids = set()\n    if predictions_path:\n        with open(predictions_path, \"r\") as f:\n            predictions = json.loads(f.read())\n            for pred in predictions:\n                all_ids.add(pred[\"instance_id\"])\n\n    if instance_ids:\n        all_ids |= set(instance_ids)\n\n    if not all_ids:\n        print(\"No instance IDs provided, exiting.\")\n        return\n\n    for instance_id in all_ids:\n        try:\n            client = docker.from_env()\n            container = client.containers.get(f\"sweb.eval.{instance_id}\")\n            container.stop()\n            container.remove()\n            print(f\"Removed container {instance_id}\")\n        except docker.errors.NotFound:\n            print(f\"Container {instance_id} not found, skipping.\")\n        except Exception as e:\n            print(f\"Error removing container {instance_id}: {e}\")\n            continue\n</code></pre>"},{"location":"api/harness/#swebench.harness.reporting","title":"reporting","text":""},{"location":"api/harness/#swebench.harness.reporting.make_run_report","title":"make_run_report","text":"<pre><code>make_run_report(predictions: dict, full_dataset: list, run_id: str, client: Optional[DockerClient] = None) -&gt; Path\n</code></pre> <p>Make a final evaluation and run report of the instances that have been run. Also reports on images and containers that may still running if client is provided.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>Predictions dict generated by the model</p> required <code>full_dataset</code> <code>list</code> <p>List of all instances</p> required <code>run_id</code> <code>str</code> <p>Run ID</p> required <code>client</code> <code>DockerClient</code> <p>Docker client (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to report file</p> Source code in <code>swebench/harness/reporting.py</code> <pre><code>def make_run_report(\n    predictions: dict,\n    full_dataset: list,\n    run_id: str,\n    client: Optional[docker.DockerClient] = None,\n) -&gt; Path:\n    \"\"\"\n    Make a final evaluation and run report of the instances that have been run.\n    Also reports on images and containers that may still running if client is provided.\n\n    Args:\n        predictions (dict): Predictions dict generated by the model\n        full_dataset (list): List of all instances\n        run_id (str): Run ID\n        client (docker.DockerClient): Docker client (optional)\n\n    Returns:\n        Path to report file\n    \"\"\"\n    # instantiate sets to store IDs of different outcomes\n    completed_ids = set()\n    resolved_ids = set()\n    error_ids = set()\n    unstopped_containers = set()\n    unremoved_images = set()\n    unresolved_ids = set()\n    incomplete_ids = set()\n    # get instances with empty patches\n    empty_patch_ids = set()\n\n    # iterate through dataset and check if the instance has been run\n    for instance in full_dataset:\n        instance_id = instance[KEY_INSTANCE_ID]\n        if instance_id not in predictions:\n            # skip instances without predictions\n            incomplete_ids.add(instance_id)\n            continue\n        prediction = predictions[instance_id]\n        if prediction.get(KEY_PREDICTION, None) in [\"\", None]:\n            empty_patch_ids.add(instance_id)\n            continue\n        report_file = (\n            RUN_EVALUATION_LOG_DIR\n            / run_id\n            / prediction[KEY_MODEL].replace(\"/\", \"__\")\n            / prediction[KEY_INSTANCE_ID]\n            / LOG_REPORT\n        )\n        if report_file.exists():\n            # If report file exists, then the instance has been run\n            completed_ids.add(instance_id)\n            report = json.loads(report_file.read_text())\n            if report[instance_id][\"resolved\"]:\n                # Record if the instance was resolved\n                resolved_ids.add(instance_id)\n            else:\n                unresolved_ids.add(instance_id)\n        else:\n            # Otherwise, the instance was not run successfully\n            error_ids.add(instance_id)\n\n    if client:\n        # get remaining images and containers\n        images = list_images(client)\n        test_specs = list(map(make_test_spec, full_dataset))\n        for spec in test_specs:\n            image_name = spec.instance_image_key\n            if image_name in images:\n                unremoved_images.add(image_name)\n        containers = client.containers.list(all=True)\n        for container in containers:\n            if run_id in container.name:\n                unstopped_containers.add(container.name)\n\n    # print final report\n    dataset_ids = {i[KEY_INSTANCE_ID] for i in full_dataset}\n    print(f\"Total instances: {len(full_dataset)}\")\n    print(f\"Instances submitted: {len(set(predictions.keys()) &amp; dataset_ids)}\")\n    print(f\"Instances completed: {len(completed_ids)}\")\n    print(f\"Instances incomplete: {len(incomplete_ids)}\")\n    print(f\"Instances resolved: {len(resolved_ids)}\")\n    print(f\"Instances unresolved: {len(unresolved_ids)}\")\n    print(f\"Instances with empty patches: {len(empty_patch_ids)}\")\n    print(f\"Instances with errors: {len(error_ids)}\")\n    if client:\n        print(f\"Unstopped containers: {len(unstopped_containers)}\")\n        print(f\"Unremoved images: {len(unremoved_images)}\")\n\n    # write report to file\n    report = {\n        \"total_instances\": len(full_dataset),\n        \"submitted_instances\": len(predictions),\n        \"completed_instances\": len(completed_ids),\n        \"resolved_instances\": len(resolved_ids),\n        \"unresolved_instances\": len(unresolved_ids),\n        \"empty_patch_instances\": len(empty_patch_ids),\n        \"error_instances\": len(error_ids),\n        \"completed_ids\": list(sorted(completed_ids)),\n        \"incomplete_ids\": list(sorted(incomplete_ids)),\n        \"empty_patch_ids\": list(sorted(empty_patch_ids)),\n        \"submitted_ids\": list(sorted(predictions.keys())),\n        \"resolved_ids\": list(sorted(resolved_ids)),\n        \"unresolved_ids\": list(sorted(unresolved_ids)),\n        \"error_ids\": list(sorted(error_ids)),\n        \"schema_version\": 2,\n    }\n    if not client:\n        report.update(\n            {\n                \"unstopped_instances\": len(unstopped_containers),\n                \"unstopped_containers\": list(sorted(unstopped_containers)),\n                \"unremoved_images\": list(sorted(unremoved_images)),\n            }\n        )\n    report_file = Path(\n        list(predictions.values())[0][KEY_MODEL].replace(\"/\", \"__\")\n        + f\".{run_id}\"\n        + \".json\"\n    )\n    with open(report_file, \"w\") as f:\n        print(json.dumps(report, indent=4), file=f)\n    print(f\"Report written to {report_file}\")\n    return report_file\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation","title":"run_evaluation","text":""},{"location":"api/harness/#swebench.harness.run_evaluation.GIT_APPLY_CMDS","title":"GIT_APPLY_CMDS  <code>module-attribute</code>","text":"<pre><code>GIT_APPLY_CMDS = ['git apply --verbose', 'git apply --verbose --reject', 'patch --batch --fuzz=5 -p1 -i']\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description='Run evaluation harness for the given dataset and predictions.', formatter_class=ArgumentDefaultsHelpFormatter)\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation.run_instance","title":"run_instance","text":"<pre><code>run_instance(test_spec: TestSpec, pred: dict, rm_image: bool, force_rebuild: bool, client: DockerClient, run_id: str, timeout: int | None = None, rewrite_reports: bool = False)\n</code></pre> <p>Run a single instance with the given prediction.</p> <p>Parameters:</p> Name Type Description Default <code>test_spec</code> <code>TestSpec</code> <p>TestSpec instance</p> required <code>pred</code> <code>dict</code> <p>Prediction w/ model_name_or_path, model_patch, instance_id</p> required <code>rm_image</code> <code>bool</code> <p>Whether to remove the image after running</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuild the image</p> required <code>client</code> <code>DockerClient</code> <p>Docker client</p> required <code>run_id</code> <code>str</code> <p>Run ID</p> required <code>timeout</code> <code>int</code> <p>Timeout for running tests</p> <code>None</code> <code>rewrite_reports</code> <code>bool</code> <p>True if eval run is just to reformat existing report</p> <code>False</code> Source code in <code>swebench/harness/run_evaluation.py</code> <pre><code>def run_instance(\n    test_spec: TestSpec,\n    pred: dict,\n    rm_image: bool,\n    force_rebuild: bool,\n    client: docker.DockerClient,\n    run_id: str,\n    timeout: int | None = None,\n    rewrite_reports: bool = False,\n):\n    \"\"\"\n    Run a single instance with the given prediction.\n\n    Args:\n        test_spec (TestSpec): TestSpec instance\n        pred (dict): Prediction w/ model_name_or_path, model_patch, instance_id\n        rm_image (bool): Whether to remove the image after running\n        force_rebuild (bool): Whether to force rebuild the image\n        client (docker.DockerClient): Docker client\n        run_id (str): Run ID\n        timeout (int): Timeout for running tests\n        rewrite_reports (bool): True if eval run is just to reformat existing report\n    \"\"\"\n    # Set up logging directory\n    instance_id = test_spec.instance_id\n    model_name_or_path = pred.get(KEY_MODEL, \"None\").replace(\"/\", \"__\")\n    log_dir = RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id\n\n    # Set up report file\n    report_path = log_dir / LOG_REPORT\n    if rewrite_reports:\n        test_output_path = log_dir / LOG_TEST_OUTPUT\n        if not test_output_path.exists():\n            raise ValueError(f\"Test output file {test_output_path} does not exist\")\n        report = get_eval_report(\n            test_spec=test_spec,\n            prediction=pred,\n            test_log_path=test_output_path,\n            include_tests_status=True,\n        )\n        # Write report to report.json\n        with open(report_path, \"w\") as f:\n            f.write(json.dumps(report, indent=4))\n        return instance_id, report\n    if report_path.exists():\n        return instance_id, json.loads(report_path.read_text())\n\n    if not test_spec.is_remote_image:\n        # Link the image build dir in the log dir\n        build_dir = INSTANCE_IMAGE_BUILD_DIR / test_spec.instance_image_key.replace(\n            \":\", \"__\"\n        )\n        image_build_link = log_dir / \"image_build_dir\"\n        if not image_build_link.exists():\n            try:\n                # link the image build dir in the log dir\n                image_build_link.symlink_to(\n                    build_dir.absolute(), target_is_directory=True\n                )\n            except:\n                # some error, idk why\n                pass\n\n    # Set up logger\n    log_dir.mkdir(parents=True, exist_ok=True)\n    log_file = log_dir / LOG_INSTANCE\n    logger = setup_logger(instance_id, log_file)\n\n    # Run the instance\n    container = None\n    try:\n        # Build + start instance container (instance image should already be built)\n        container = build_container(\n            test_spec, client, run_id, logger, rm_image, force_rebuild\n        )\n        container.start()\n        logger.info(f\"Container for {instance_id} started: {container.id}\")\n\n        # Copy model prediction as patch file to container\n        patch_file = Path(log_dir / \"patch.diff\")\n        patch_file.write_text(pred[KEY_PREDICTION] or \"\")\n        logger.info(\n            f\"Intermediate patch for {instance_id} written to {patch_file}, now applying to container...\"\n        )\n        copy_to_container(container, patch_file, PurePosixPath(DOCKER_PATCH))\n\n        # Attempt to apply patch to container (TODO: FIX THIS)\n        applied_patch = False\n        for git_apply_cmd in GIT_APPLY_CMDS:\n            val = container.exec_run(\n                f\"{git_apply_cmd} {DOCKER_PATCH}\",\n                workdir=DOCKER_WORKDIR,\n                user=DOCKER_USER,\n            )\n            if val.exit_code == 0:\n                logger.info(f\"{APPLY_PATCH_PASS}:\\n{val.output.decode(UTF8)}\")\n                applied_patch = True\n                break\n            else:\n                logger.info(f\"Failed to apply patch to container: {git_apply_cmd}\")\n        if not applied_patch:\n            logger.info(f\"{APPLY_PATCH_FAIL}:\\n{val.output.decode(UTF8)}\")\n            raise EvaluationError(\n                instance_id,\n                f\"{APPLY_PATCH_FAIL}:\\n{val.output.decode(UTF8)}\",\n                logger,\n            )\n\n        # Get git diff before running eval script\n        git_diff_output_before = (\n            container.exec_run(\n                \"git -c core.fileMode=false diff\", workdir=DOCKER_WORKDIR\n            )\n            .output.decode(UTF8)\n            .strip()\n        )\n        logger.info(f\"Git diff before:\\n{git_diff_output_before}\")\n\n        eval_file = Path(log_dir / \"eval.sh\")\n        eval_file.write_text(test_spec.eval_script)\n        logger.info(\n            f\"Eval script for {instance_id} written to {eval_file}; copying to container...\"\n        )\n        copy_to_container(container, eval_file, PurePosixPath(\"/eval.sh\"))\n\n        # Run eval script, write output to logs\n        test_output, timed_out, total_runtime = exec_run_with_timeout(\n            container, \"/bin/bash /eval.sh\", timeout\n        )\n        test_output_path = log_dir / LOG_TEST_OUTPUT\n        logger.info(f\"Test runtime: {total_runtime:_.2f} seconds\")\n        with open(test_output_path, \"w\") as f:\n            f.write(test_output)\n            logger.info(f\"Test output for {instance_id} written to {test_output_path}\")\n            if timed_out:\n                f.write(f\"\\n\\nTimeout error: {timeout} seconds exceeded.\")\n                raise EvaluationError(\n                    instance_id,\n                    f\"Test timed out after {timeout} seconds.\",\n                    logger,\n                )\n\n        # Get git diff after running eval script (ignore permission changes)\n        git_diff_output_after = (\n            container.exec_run(\n                \"git -c core.fileMode=false diff\", workdir=DOCKER_WORKDIR\n            )\n            .output.decode(UTF8)\n            .strip()\n        )\n\n        # Check if git diff changed after running eval script\n        logger.info(f\"Git diff after:\\n{git_diff_output_after}\")\n        if git_diff_output_after != git_diff_output_before:\n            logger.info(\"Git diff changed after running eval script\")\n\n        # Get report from test output\n        logger.info(f\"Grading answer for {instance_id}...\")\n        report = get_eval_report(\n            test_spec=test_spec,\n            prediction=pred,\n            test_log_path=test_output_path,\n            include_tests_status=True,\n        )\n        logger.info(\n            f\"report: {report}\\n\"\n            f\"Result for {instance_id}: resolved: {report[instance_id]['resolved']}\"\n        )\n\n        # Write report to report.json\n        with open(report_path, \"w\") as f:\n            f.write(json.dumps(report, indent=4))\n        return instance_id, report\n    except EvaluationError as e:\n        error_msg = traceback.format_exc()\n        logger.info(error_msg)\n        print(e)\n    except BuildImageError as e:\n        error_msg = traceback.format_exc()\n        logger.info(error_msg)\n        print(e)\n    except Exception as e:\n        error_msg = (\n            f\"Error in evaluating model for {instance_id}: {e}\\n\"\n            f\"{traceback.format_exc()}\\n\"\n            f\"Check ({logger.log_file}) for more information.\"\n        )\n        logger.error(error_msg)\n    finally:\n        # Remove instance container + image, close logger\n        cleanup_container(client, container, logger)\n        if rm_image:\n            remove_image(client, test_spec.instance_image_key, logger)\n        close_logger(logger)\n    return\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation.run_instances","title":"run_instances","text":"<pre><code>run_instances(predictions: dict, instances: list, cache_level: str, clean: bool, force_rebuild: bool, max_workers: int, run_id: str, timeout: int, namespace: str | None = 'swebench', instance_image_tag: str = 'latest', rewrite_reports: bool = False)\n</code></pre> <p>Run all instances for the given predictions in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>Predictions dict generated by the model</p> required <code>instances</code> <code>list</code> <p>List of instances</p> required <code>cache_level</code> <code>str</code> <p>Cache level</p> required <code>clean</code> <code>bool</code> <p>Clean images above cache level</p> required <code>force_rebuild</code> <code>bool</code> <p>Force rebuild images</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of workers</p> required <code>run_id</code> <code>str</code> <p>Run ID</p> required <code>timeout</code> <code>int</code> <p>Timeout for running tests</p> required Source code in <code>swebench/harness/run_evaluation.py</code> <pre><code>def run_instances(\n    predictions: dict,\n    instances: list,\n    cache_level: str,\n    clean: bool,\n    force_rebuild: bool,\n    max_workers: int,\n    run_id: str,\n    timeout: int,\n    namespace: str | None = \"swebench\",\n    instance_image_tag: str = \"latest\",\n    rewrite_reports: bool = False,\n):\n    \"\"\"\n    Run all instances for the given predictions in parallel.\n\n    Args:\n        predictions (dict): Predictions dict generated by the model\n        instances (list): List of instances\n        cache_level (str): Cache level\n        clean (bool): Clean images above cache level\n        force_rebuild (bool): Force rebuild images\n        max_workers (int): Maximum number of workers\n        run_id (str): Run ID\n        timeout (int): Timeout for running tests\n    \"\"\"\n    client = docker.from_env()\n    test_specs = list(\n        map(\n            lambda instance: make_test_spec(\n                instance, namespace=namespace, instance_image_tag=instance_image_tag\n            ),\n            instances,\n        )\n    )\n\n    # print number of existing instance images\n    instance_image_ids = {x.instance_image_key for x in test_specs}\n    existing_images = {\n        tag\n        for i in client.images.list(all=True)\n        for tag in i.tags\n        if tag in instance_image_ids\n    }\n    if not force_rebuild and len(existing_images):\n        print(\n            f\"Found {len(existing_images)} existing instance images. Will reuse them.\"\n        )\n\n    # run instances in parallel\n    payloads = []\n    for test_spec in test_specs:\n        payloads.append(\n            (\n                test_spec,\n                predictions[test_spec.instance_id],\n                should_remove(\n                    test_spec.instance_image_key,\n                    cache_level,\n                    clean,\n                    existing_images,\n                ),\n                force_rebuild,\n                client,\n                run_id,\n                timeout,\n                rewrite_reports,\n            )\n        )\n\n    # run instances in parallel\n    print(f\"Running {len(instances)} instances...\")\n    run_threadpool(run_instance, payloads, max_workers)\n    print(\"All instances run.\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation.get_dataset_from_preds","title":"get_dataset_from_preds","text":"<pre><code>get_dataset_from_preds(dataset_name: str, split: str, instance_ids: list, predictions: dict, run_id: str, rewrite_reports: bool, exclude_completed: bool = True)\n</code></pre> <p>Return only instances that have predictions and are in the dataset. If instance_ids is provided, only return instances with those IDs. If exclude_completed is True, only return instances that have not been run yet.</p> Source code in <code>swebench/harness/run_evaluation.py</code> <pre><code>def get_dataset_from_preds(\n    dataset_name: str,\n    split: str,\n    instance_ids: list,\n    predictions: dict,\n    run_id: str,\n    rewrite_reports: bool,\n    exclude_completed: bool = True,\n):\n    \"\"\"\n    Return only instances that have predictions and are in the dataset.\n    If instance_ids is provided, only return instances with those IDs.\n    If exclude_completed is True, only return instances that have not been run yet.\n    \"\"\"\n    # load dataset\n    dataset = load_swebench_dataset(dataset_name, split)\n    dataset_ids = {i[KEY_INSTANCE_ID] for i in dataset}\n\n    if instance_ids:\n        # check that all instance IDs have predictions\n        missing_preds = set(instance_ids) - set(predictions.keys())\n        if missing_preds:\n            print(\n                f\"Warning: Missing predictions for {len(missing_preds)} instance IDs.\"\n            )\n\n    # check that all prediction IDs are in the dataset\n    prediction_ids = set(predictions.keys())\n    if prediction_ids - dataset_ids:\n        raise ValueError(\n            (\n                \"Some prediction IDs not found in dataset!\"\n                f\"\\nMissing IDs:\\n{' '.join(prediction_ids - dataset_ids)}\"\n            )\n        )\n    if instance_ids:\n        dataset = [i for i in dataset if i[KEY_INSTANCE_ID] in instance_ids]\n\n    if rewrite_reports:\n        # we only return instances that have existing test outputs\n        test_output_ids = set()\n        for instance in dataset:\n            if instance[KEY_INSTANCE_ID] not in predictions:\n                continue\n            prediction = predictions[instance[KEY_INSTANCE_ID]]\n            test_output_file = (\n                RUN_EVALUATION_LOG_DIR\n                / run_id\n                / prediction[\"model_name_or_path\"].replace(\"/\", \"__\")\n                / prediction[KEY_INSTANCE_ID]\n                / \"test_output.txt\"\n            )\n            if test_output_file.exists():\n                test_output_ids.add(instance[KEY_INSTANCE_ID])\n        dataset = [\n            i\n            for i in dataset\n            if i[KEY_INSTANCE_ID] in prediction_ids\n            and i[KEY_INSTANCE_ID] in test_output_ids\n        ]\n        return dataset\n\n    # check which instance IDs have already been run\n    completed_ids = set()\n    for instance in dataset:\n        if instance[KEY_INSTANCE_ID] not in prediction_ids:\n            # skip instances without predictions\n            continue\n        prediction = predictions[instance[KEY_INSTANCE_ID]]\n        report_file = (\n            RUN_EVALUATION_LOG_DIR\n            / run_id\n            / prediction[KEY_MODEL].replace(\"/\", \"__\")\n            / prediction[KEY_INSTANCE_ID]\n            / LOG_REPORT\n        )\n        if report_file.exists():\n            completed_ids.add(instance[KEY_INSTANCE_ID])\n\n    if completed_ids and exclude_completed:\n        # filter dataset to only instances that have not been run\n        print(f\"{len(completed_ids)} instances already run, skipping...\")\n        dataset = [i for i in dataset if i[KEY_INSTANCE_ID] not in completed_ids]\n\n    empty_patch_ids = {\n        k\n        for k, v in predictions.items()\n        if v[KEY_PREDICTION] == \"\" or v[KEY_PREDICTION] is None\n    }\n\n    # filter dataset to only instances with predictions\n    dataset = [\n        i\n        for i in dataset\n        if i[KEY_INSTANCE_ID] in prediction_ids\n        and i[KEY_INSTANCE_ID] not in empty_patch_ids\n    ]\n    return dataset\n</code></pre>"},{"location":"api/harness/#swebench.harness.run_evaluation.main","title":"main","text":"<pre><code>main(dataset_name: str, split: str, instance_ids: list, predictions_path: str, max_workers: int, force_rebuild: bool, cache_level: str, clean: bool, open_file_limit: int, run_id: str, timeout: int, namespace: str | None, rewrite_reports: bool, modal: bool, instance_image_tag: str = 'latest', report_dir: str = '.')\n</code></pre> <p>Run evaluation harness for the given dataset and predictions.</p> Source code in <code>swebench/harness/run_evaluation.py</code> <pre><code>def main(\n    dataset_name: str,\n    split: str,\n    instance_ids: list,\n    predictions_path: str,\n    max_workers: int,\n    force_rebuild: bool,\n    cache_level: str,\n    clean: bool,\n    open_file_limit: int,\n    run_id: str,\n    timeout: int,\n    namespace: str | None,\n    rewrite_reports: bool,\n    modal: bool,\n    instance_image_tag: str = \"latest\",\n    report_dir: str = \".\",\n):\n    \"\"\"\n    Run evaluation harness for the given dataset and predictions.\n    \"\"\"\n    if dataset_name == \"SWE-bench/SWE-bench_Multimodal\" and split == \"test\":\n        print(\n            \"\u26a0\ufe0f Local evaluation for the test split of SWE-bench Multimodal is not supported. \"\n            \"Please check out sb-cli (https://github.com/swe-bench/sb-cli/) for instructions on how to submit predictions.\"\n        )\n        return\n\n    # set open file limit\n    assert len(run_id) &gt; 0, \"Run ID must be provided\"\n    if report_dir is not None:\n        report_dir = Path(report_dir)\n        if not report_dir.exists():\n            report_dir.mkdir(parents=True)\n\n    if force_rebuild and namespace is not None:\n        raise ValueError(\"Cannot force rebuild and use a namespace at the same time.\")\n\n    # load predictions as map of instance_id to prediction\n    predictions = get_predictions_from_file(predictions_path, dataset_name, split)\n    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}\n\n    # get dataset from predictions\n    dataset = get_dataset_from_preds(\n        dataset_name, split, instance_ids, predictions, run_id, rewrite_reports\n    )\n    full_dataset = load_swebench_dataset(dataset_name, split, instance_ids)\n\n    if modal:\n        # run instances on Modal\n        if not dataset:\n            print(\"No instances to run.\")\n        else:\n            validate_modal_credentials()\n            run_instances_modal(predictions, dataset, full_dataset, run_id, timeout)\n        return\n\n    # run instances locally\n    if platform.system() == \"Linux\":\n        resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n    client = docker.from_env()\n\n    existing_images = list_images(client)\n    if not dataset:\n        print(\"No instances to run.\")\n    else:\n        # build environment images + run instances\n        if namespace is None and not rewrite_reports:\n            build_env_images(client, dataset, force_rebuild, max_workers)\n        run_instances(\n            predictions,\n            dataset,\n            cache_level,\n            clean,\n            force_rebuild,\n            max_workers,\n            run_id,\n            timeout,\n            namespace=namespace,\n            instance_image_tag=instance_image_tag,\n            rewrite_reports=rewrite_reports,\n        )\n\n    # clean images + make final report\n    clean_images(client, existing_images, cache_level, clean)\n    return make_run_report(predictions, full_dataset, run_id, client)\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec","title":"test_spec","text":""},{"location":"api/harness/#swebench.harness.test_spec.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['test_spec', 'create_scripts', 'javascript', 'python']\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.create_scripts","title":"create_scripts","text":""},{"location":"api/harness/#swebench.harness.test_spec.create_scripts.make_repo_script_list","title":"make_repo_script_list","text":"<pre><code>make_repo_script_list(specs, repo, repo_directory, base_commit, env_name) -&gt; list\n</code></pre> <p>Create a list of bash commands to set up the repository for testing. This is the setup script for the instance image.</p> Source code in <code>swebench/harness/test_spec/create_scripts.py</code> <pre><code>def make_repo_script_list(specs, repo, repo_directory, base_commit, env_name) -&gt; list:\n    \"\"\"\n    Create a list of bash commands to set up the repository for testing.\n    This is the setup script for the instance image.\n    \"\"\"\n    ext = MAP_REPO_TO_EXT[repo]\n    func = {\n        \"py\": make_repo_script_list_py,\n    }.get(ext, make_repo_script_list_common)\n    return func(specs, repo, repo_directory, base_commit, env_name)\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.create_scripts.make_env_script_list","title":"make_env_script_list","text":"<pre><code>make_env_script_list(instance, specs, env_name) -&gt; list\n</code></pre> <p>Creates the list of commands to set up the environment for testing. This is the setup script for the environment image.</p> Source code in <code>swebench/harness/test_spec/create_scripts.py</code> <pre><code>def make_env_script_list(instance, specs, env_name) -&gt; list:\n    \"\"\"\n    Creates the list of commands to set up the environment for testing.\n    This is the setup script for the environment image.\n    \"\"\"\n    ext = MAP_REPO_TO_EXT[instance[\"repo\"]]\n    func = {\n        \"py\": make_env_script_list_py,\n    }.get(ext, make_env_script_list_common)\n    return func(instance, specs, env_name)\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.create_scripts.make_eval_script_list","title":"make_eval_script_list","text":"<pre><code>make_eval_script_list(instance, specs, env_name, repo_directory, base_commit, test_patch) -&gt; list\n</code></pre> <p>Applies the test patch and runs the tests.</p> Source code in <code>swebench/harness/test_spec/create_scripts.py</code> <pre><code>def make_eval_script_list(\n    instance, specs, env_name, repo_directory, base_commit, test_patch\n) -&gt; list:\n    \"\"\"\n    Applies the test patch and runs the tests.\n    \"\"\"\n    ext = MAP_REPO_TO_EXT[instance[\"repo\"]]\n    common_func = make_eval_script_list_common\n    func = {\n        \"js\": make_eval_script_list_js,\n        \"py\": make_eval_script_list_py,\n    }.get(ext, common_func)\n    return func(instance, specs, env_name, repo_directory, base_commit, test_patch)\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.javascript","title":"javascript","text":""},{"location":"api/harness/#swebench.harness.test_spec.javascript.MAP_REPO_TO_TEST_CMDS","title":"MAP_REPO_TO_TEST_CMDS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_TEST_CMDS = {'Automattic/wp-calypso': get_test_cmds_calypso}\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.javascript.get_test_cmds_calypso","title":"get_test_cmds_calypso","text":"<pre><code>get_test_cmds_calypso(instance) -&gt; list\n</code></pre> Source code in <code>swebench/harness/test_spec/javascript.py</code> <pre><code>def get_test_cmds_calypso(instance) -&gt; list:\n    test_paths = [x.path for x in PatchSet(instance[\"test_patch\"])]\n    test_cmds = []\n    for test_path in test_paths:\n        if re.search(r\"__snapshots__/(.*).js.snap$\", test_path):\n            # Jest snapshots are not run directly\n            test_path = \"/\".join(test_path.split(\"/\")[:-2])\n\n        # Determine which testing script to use\n        if any([test_path.startswith(x) for x in [\"client\", \"packages\"]]):\n            pkg = test_path.split(\"/\")[0]\n            if instance[\"version\"] in [\n                \"10.10.0\",\n                \"10.12.0\",\n                \"10.13.0\",\n                \"10.14.0\",\n                \"10.15.2\",\n                \"10.16.3\",\n            ]:\n                test_cmds.append(\n                    f\"./node_modules/.bin/jest --verbose -c=test/{pkg}/jest.config.js '{test_path}'\"\n                )\n            elif instance[\"version\"] in [\n                \"6.11.5\",\n                \"8.9.1\",\n                \"8.9.3\",\n                \"8.9.4\",\n                \"8.11.0\",\n                \"8.11.2\",\n                \"10.4.1\",\n                \"10.5.0\",\n                \"10.6.0\",\n                \"10.9.0\",\n            ]:\n                test_cmds.append(\n                    f\"./node_modules/.bin/jest --verbose -c=test/{pkg}/jest.config.json '{test_path}'\"\n                )\n            else:\n                test_cmds.append(f\"npm run test-{pkg} --verbose '{test_path}'\")\n        elif any([test_path.startswith(x) for x in [\"test/e2e\"]]):\n            test_cmds.extend(\n                [\n                    \"cd test/e2e\",\n                    f\"NODE_CONFIG_ENV=test npm run test {test_path}\",\n                    \"cd ../..\",\n                ]\n            )\n\n    return test_cmds\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.javascript.get_download_img_commands","title":"get_download_img_commands","text":"<pre><code>get_download_img_commands(instance) -&gt; list\n</code></pre> Source code in <code>swebench/harness/test_spec/javascript.py</code> <pre><code>def get_download_img_commands(instance) -&gt; list:\n    cmds = []\n    image_assets = {}\n    if \"image_assets\" in instance:\n        if isinstance(instance[\"image_assets\"], str):\n            image_assets = json.loads(instance[\"image_assets\"])\n        else:\n            image_assets = instance[\"image_assets\"]\n    for i in image_assets.get(\"test_patch\", []):\n        folder = Path(i[\"path\"]).parent\n        cmds.append(f\"mkdir -p {folder}\")\n        cmds.append(f\"curl -o {i['path']} {i['url']}\")\n        cmds.append(f\"chmod 777 {i['path']}\")\n    return cmds\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.javascript.make_eval_script_list_js","title":"make_eval_script_list_js","text":"<pre><code>make_eval_script_list_js(instance, specs, env_name, repo_directory, base_commit, test_patch) -&gt; list\n</code></pre> <p>Applies the test patch and runs the tests.</p> Source code in <code>swebench/harness/test_spec/javascript.py</code> <pre><code>def make_eval_script_list_js(\n    instance, specs, env_name, repo_directory, base_commit, test_patch\n) -&gt; list:\n    \"\"\"\n    Applies the test patch and runs the tests.\n    \"\"\"\n    eval_commands = make_eval_script_list_common(\n        instance, specs, env_name, repo_directory, base_commit, test_patch\n    )\n    # Insert downloading right after reset command\n    eval_commands[4:4] = get_download_img_commands(instance)\n    if instance[\"repo\"] in MAP_REPO_TO_TEST_CMDS:\n        # Update test commands if they are custom commands\n        test_commands = MAP_REPO_TO_TEST_CMDS[instance[\"repo\"]](instance)\n        idx_start_test_out = eval_commands.index(f\": '{START_TEST_OUTPUT}'\")\n        idx_end_test_out = eval_commands.index(f\": '{END_TEST_OUTPUT}'\")\n        eval_commands[idx_start_test_out + 1 : idx_end_test_out] = test_commands\n    return eval_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python","title":"python","text":""},{"location":"api/harness/#swebench.harness.test_spec.python.HEADERS","title":"HEADERS  <code>module-attribute</code>","text":"<pre><code>HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.REPLACE_REQ_PACKAGES","title":"REPLACE_REQ_PACKAGES  <code>module-attribute</code>","text":"<pre><code>REPLACE_REQ_PACKAGES = [('types-pkg_resources', 'types-setuptools')]\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.get_environment_yml_by_commit","title":"get_environment_yml_by_commit  <code>cached</code>","text":"<pre><code>get_environment_yml_by_commit(repo: str, commit: str, env_name: str) -&gt; str\n</code></pre> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>@cache\ndef get_environment_yml_by_commit(repo: str, commit: str, env_name: str) -&gt; str:\n    for req_path in MAP_REPO_TO_ENV_YML_PATHS[repo]:\n        reqs_url = posixpath.join(SWE_BENCH_URL_RAW, repo, commit, req_path)\n        reqs = requests.get(reqs_url, headers=HEADERS)\n        if reqs.status_code == 200:\n            break\n    else:\n        raise ValueError(\n            f\"Could not find environment.yml at paths {MAP_REPO_TO_ENV_YML_PATHS[repo]} for repo {repo} at commit {commit}\"\n        )\n\n    lines = reqs.text.split(\"\\n\")\n    cleaned = []\n    for line in lines:\n        # Rename environment to given name\n        if line.startswith(\"name:\"):\n            cleaned.append(f\"name: {env_name}\")\n            continue\n        cleaned.append(line)\n\n    return \"\\n\".join(cleaned)\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.clean_environment_yml","title":"clean_environment_yml","text":"<pre><code>clean_environment_yml(yml_text: str) -&gt; str\n</code></pre> <p>Clean environment.yml by removing packages that have been yanked from PyPI</p> <p>conda style yamls take the form: ... - channels:     ... - dependencies:     ... - pip:     - pkg_to_replace     - pkg_to_replace - ... (more dependencies)</p> <p>We want to replace packages in the pip section only.</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def clean_environment_yml(yml_text: str) -&gt; str:\n    \"\"\"\n    Clean environment.yml by removing packages that have been yanked from PyPI\n\n    conda style yamls take the form:\n    ...\n    - channels:\n        ...\n    - dependencies:\n        ...\n    - pip:\n        - pkg_to_replace\n        - pkg_to_replace\n    - ... (more dependencies)\n\n    We want to replace packages in the pip section only.\n    \"\"\"\n    pip_match = re.search(r\"^(\\s*-\\s*pip\\s*:\\s*\\n)\", yml_text, flags=re.MULTILINE)\n    if not pip_match:\n        return yml_text\n    pip_line_start = pip_match.start()\n    # get indentation level of pip line\n    pip_indent = len(pip_match.group(1)) - len(pip_match.group(1).lstrip())\n    pip_content_start = pip_match.end()\n    # find where pip section ends by looking for a line that's at same or less indentation\n    # or a line that starts a new top-level dependency (not pip)\n    lines_after_pip = yml_text[pip_content_start:].split(\"\\n\")\n    pip_section_end = pip_content_start\n    for ix, line in enumerate(lines_after_pip):\n        if line.strip() == \"\":\n            continue\n        line_indent = len(line) - len(line.lstrip())\n        if line_indent &lt;= pip_indent:\n            # +1 to account for the newline\n            pip_section_end = pip_content_start + sum(\n                len(l) + 1 for l in lines_after_pip[:ix]\n            )\n            break\n    else:\n        pip_section_end = len(yml_text)\n    prefix = yml_text[:pip_content_start]\n    pip_portion = yml_text[pip_content_start:pip_section_end]\n    suffix = yml_text[pip_section_end:]\n    for pkg_to_replace, replacement in REPLACE_REQ_PACKAGES:\n        if replacement == None:\n            pip_portion = re.sub(\n                rf\"^(\\s*-\\s*){re.escape(pkg_to_replace)}([&lt;&gt;~]=?.*|$)\\n?\",\n                \"\",\n                pip_portion,\n                flags=re.MULTILINE,\n            )\n        else:\n            pip_portion = re.sub(\n                rf\"^(\\s*-\\s*){re.escape(pkg_to_replace)}([&lt;&gt;=!~]=?.*|$)\",\n                rf\"\\1{replacement}\",\n                pip_portion,\n                flags=re.MULTILINE,\n            )\n    return prefix + pip_portion + suffix\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.get_environment_yml","title":"get_environment_yml","text":"<pre><code>get_environment_yml(instance: SWEbenchInstance, env_name: str) -&gt; str\n</code></pre> <p>Get environment.yml for given task instance</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>dict</code> <p>SWE Bench Task instance</p> required <code>env_name</code> <code>str</code> <p>Rename retrieved environment.yml to this name</p> required <p>Returns:     environment.yml (str): Returns environment.yml as string</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def get_environment_yml(instance: SWEbenchInstance, env_name: str) -&gt; str:\n    \"\"\"\n    Get environment.yml for given task instance\n\n    Args:\n        instance (dict): SWE Bench Task instance\n        env_name (str): Rename retrieved environment.yml to this name\n    Returns:\n        environment.yml (str): Returns environment.yml as string\n    \"\"\"\n    # Attempt to find environment.yml at each path based on task instance's repo\n    commit = (\n        instance[\"environment_setup_commit\"]\n        if \"environment_setup_commit\" in instance\n        else instance[\"base_commit\"]\n    )\n    yml_text = get_environment_yml_by_commit(instance[\"repo\"], commit, env_name)\n    yml_text = clean_environment_yml(yml_text)\n    return yml_text\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.get_requirements_by_commit","title":"get_requirements_by_commit  <code>cached</code>","text":"<pre><code>get_requirements_by_commit(repo: str, commit: str) -&gt; str\n</code></pre> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>@cache\ndef get_requirements_by_commit(repo: str, commit: str) -&gt; str:\n    for req_path in MAP_REPO_TO_REQS_PATHS[repo]:\n        reqs_url = posixpath.join(SWE_BENCH_URL_RAW, repo, commit, req_path)\n        reqs = requests.get(reqs_url, headers=HEADERS)\n        if reqs.status_code == 200:\n            break\n    else:\n        raise ValueError(\n            f\"Could not find requirements.txt at paths {MAP_REPO_TO_REQS_PATHS[repo]} for repo {repo} at commit {commit}\"\n        )\n\n    lines = reqs.text\n    original_req = []\n    additional_reqs = []\n    req_dir = \"/\".join(req_path.split(\"/\")[:-1])\n    exclude_line = lambda line: any(\n        [line.strip().startswith(x) for x in [\"-e .\", \"#\", \".[test\"]]\n    )\n\n    for line in lines.split(\"\\n\"):\n        if line.strip().startswith(\"-r\"):\n            # Handle recursive requirements\n            file_name = line[len(\"-r\") :].strip()\n            reqs_url = os.path.join(\n                SWE_BENCH_URL_RAW,\n                repo,\n                commit,\n                req_dir,\n                file_name,\n            )\n            reqs = requests.get(reqs_url, headers=HEADERS)\n            if reqs.status_code == 200:\n                for line_extra in reqs.text.split(\"\\n\"):\n                    if not exclude_line(line_extra):\n                        additional_reqs.append(line_extra)\n        else:\n            if not exclude_line(line):\n                original_req.append(line)\n\n    # Combine all requirements into single text body\n    additional_reqs.append(\"\\n\".join(original_req))\n    all_reqs = \"\\n\".join(additional_reqs)\n\n    return all_reqs\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.clean_requirements","title":"clean_requirements","text":"<pre><code>clean_requirements(requirements_text: str) -&gt; str\n</code></pre> <p>Clean requirements.txt by replacing / removing packages</p> <p>E.g. types-pkg_resources has been yanked from PyPI, so we replace it with types-setuptools</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def clean_requirements(requirements_text: str) -&gt; str:\n    \"\"\"\n    Clean requirements.txt by replacing / removing packages\n\n    E.g. types-pkg_resources has been yanked from PyPI, so we replace it with types-setuptools\n    \"\"\"\n    for pkg_to_replace, replacement in REPLACE_REQ_PACKAGES:\n        if replacement == None:\n            requirements_text = re.sub(\n                rf\"^{re.escape(pkg_to_replace)}([&lt;&gt;=!~]=?.*|$)\\n?\",\n                \"\",\n                requirements_text,\n                flags=re.MULTILINE,\n            )\n        else:\n            # this replacement removes version specifier of the original package\n            requirements_text = re.sub(\n                rf\"^{re.escape(pkg_to_replace)}([&lt;&gt;=!~]=?.*|$)\",\n                replacement,\n                requirements_text,\n                flags=re.MULTILINE,\n            )\n    return requirements_text\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.get_requirements","title":"get_requirements","text":"<pre><code>get_requirements(instance: SWEbenchInstance) -&gt; str\n</code></pre> <p>Get requirements.txt for given task instance</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>dict</code> <p>task instance</p> required <p>Returns:     requirements.txt (str): Returns requirements.txt as string</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def get_requirements(instance: SWEbenchInstance) -&gt; str:\n    \"\"\"\n    Get requirements.txt for given task instance\n\n    Args:\n        instance (dict): task instance\n    Returns:\n        requirements.txt (str): Returns requirements.txt as string\n    \"\"\"\n    # Attempt to find requirements.txt at each path based on task instance's repo\n    commit = (\n        instance[\"environment_setup_commit\"]\n        if \"environment_setup_commit\" in instance\n        else instance[\"base_commit\"]\n    )\n\n    requirements_text = get_requirements_by_commit(instance[\"repo\"], commit)\n    requirements_text = clean_requirements(requirements_text)\n    return requirements_text\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.get_test_directives","title":"get_test_directives","text":"<pre><code>get_test_directives(instance: SWEbenchInstance) -&gt; list\n</code></pre> <p>Get test directives from the test_patch of a task instance</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>dict</code> <p>task instance</p> required <p>Returns:     directives (list): List of test directives</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def get_test_directives(instance: SWEbenchInstance) -&gt; list:\n    \"\"\"\n    Get test directives from the test_patch of a task instance\n\n    Args:\n        instance (dict): task instance\n    Returns:\n        directives (list): List of test directives\n    \"\"\"\n    # For seq2seq code repos, testing command is fixed\n    if instance[\"repo\"] == \"swe-bench/humaneval\":\n        return [\"test.py\"]\n\n    # Get test directives from test patch and remove non-test files\n    diff_pat = r\"diff --git a/.* b/(.*)\"\n    test_patch = instance[\"test_patch\"]\n    directives = re.findall(diff_pat, test_patch)\n    directives = [\n        d for d in directives if not any(d.endswith(ext) for ext in NON_TEST_EXTS)\n    ]\n\n    # For Django tests, remove extension + \"tests/\" prefix and convert slashes to dots (module referencing)\n    if instance[\"repo\"] == \"django/django\":\n        directives_transformed = []\n        for d in directives:\n            d = d[: -len(\".py\")] if d.endswith(\".py\") else d\n            d = d[len(\"tests/\") :] if d.startswith(\"tests/\") else d\n            d = d.replace(\"/\", \".\")\n            directives_transformed.append(d)\n        directives = directives_transformed\n\n    return directives\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.make_repo_script_list_py","title":"make_repo_script_list_py","text":"<pre><code>make_repo_script_list_py(specs, repo, repo_directory, base_commit, env_name) -&gt; list\n</code></pre> <p>Create a list of bash commands to set up the repository for testing. This is the setup script for the instance image.</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def make_repo_script_list_py(\n    specs, repo, repo_directory, base_commit, env_name\n) -&gt; list:\n    \"\"\"\n    Create a list of bash commands to set up the repository for testing.\n    This is the setup script for the instance image.\n    \"\"\"\n    setup_commands = [\n        f\"git clone -o origin https://github.com/{repo} {repo_directory}\",\n        f\"chmod -R 777 {repo_directory}\",  # So nonroot user can run tests\n        f\"cd {repo_directory}\",\n        f\"git reset --hard {base_commit}\",\n        # Remove the remote so the agent won't see newer commits.\n        \"git remote remove origin\",\n        # Make sure conda is available for later use\n        \"source /opt/miniconda3/bin/activate\",\n        f\"conda activate {env_name}\",\n        'echo \"Current environment: $CONDA_DEFAULT_ENV\"',\n    ]\n    if repo in MAP_REPO_TO_INSTALL:\n        setup_commands.append(MAP_REPO_TO_INSTALL[repo])\n\n    # Run pre-install set up if provided\n    if \"pre_install\" in specs:\n        for pre_install in specs[\"pre_install\"]:\n            setup_commands.append(pre_install)\n\n    if \"install\" in specs:\n        setup_commands.append(specs[\"install\"])\n\n    # If the setup modifies the repository in any way, it can be\n    # difficult to get a clean diff.  This ensures that `git diff`\n    # will only reflect the changes from the user while retaining the\n    # original state of the repository plus setup commands.\n    clean_diff_commands = [\n        \"git config --global user.email setup@swebench.config\",\n        \"git config --global user.name SWE-bench\",\n        \"git commit --allow-empty -am SWE-bench\",\n    ]\n\n    setup_commands += clean_diff_commands\n\n    return setup_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.make_env_script_list_py","title":"make_env_script_list_py","text":"<pre><code>make_env_script_list_py(instance, specs, env_name) -&gt; list\n</code></pre> <p>Creates the list of commands to set up the conda environment for testing. This is the setup script for the environment image.</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def make_env_script_list_py(instance, specs, env_name) -&gt; list:\n    \"\"\"\n    Creates the list of commands to set up the conda environment for testing.\n    This is the setup script for the environment image.\n    \"\"\"\n    HEREDOC_DELIMITER = \"EOF_59812759871\"\n    reqs_commands = [\n        \"source /opt/miniconda3/bin/activate\",\n    ]\n    # Create conda environment according to install instructinos\n    pkgs = specs.get(\"packages\", \"\")\n    if pkgs == \"requirements.txt\":\n        # Create environment\n        cmd = f\"conda create -n {env_name} python={specs['python']} -y\"\n        reqs_commands.append(cmd)\n\n        # Install dependencies\n        reqs = get_requirements(instance)\n        path_to_reqs = \"$HOME/requirements.txt\"\n        reqs_commands.append(\n            f\"cat &lt;&lt;'{HEREDOC_DELIMITER}' &gt; {path_to_reqs}\\n{reqs}\\n{HEREDOC_DELIMITER}\"\n        )\n        cmd = f\"conda activate {env_name} &amp;&amp; python -m pip install -r {path_to_reqs}\"\n        reqs_commands.append(cmd)\n        reqs_commands.append(f\"rm {path_to_reqs}\")\n    elif pkgs == \"environment.yml\":\n        # Create environment from yml\n        reqs = get_environment_yml(instance, env_name)\n        path_to_reqs = \"environment.yml\"\n        reqs_commands.append(\n            f\"cat &lt;&lt;'{HEREDOC_DELIMITER}' &gt; {path_to_reqs}\\n{reqs}\\n{HEREDOC_DELIMITER}\"\n        )\n        if \"no_use_env\" in specs and specs[\"no_use_env\"]:\n            # `conda create` based installation\n            cmd = (\n                f\"conda create -c conda-forge -n {env_name} python={specs['python']} -y\"\n            )\n            reqs_commands.append(cmd)\n\n            # Install dependencies\n            cmd = f\"conda env update -f {path_to_reqs}\"\n            reqs_commands.append(cmd)\n        else:\n            # `conda env create` based installation\n            cmd = f\"conda env create --file {path_to_reqs}\"\n            reqs_commands.append(cmd)\n\n            cmd = f\"conda activate {env_name} &amp;&amp; conda install python={specs['python']} -y\"\n            reqs_commands.append(cmd)\n\n        # Remove environment.yml\n        reqs_commands.append(f\"rm {path_to_reqs}\")\n    else:\n        # Create environment + install dependencies\n        cmd = f\"conda create -n {env_name} python={specs['python']} {pkgs} -y\"\n        reqs_commands.append(cmd)\n\n    reqs_commands.append(f\"conda activate {env_name}\")\n\n    # Install additional packages if specified\n    if \"pip_packages\" in specs:\n        pip_packages = \" \".join(specs[\"pip_packages\"])\n        cmd = f\"python -m pip install {pip_packages}\"\n        reqs_commands.append(cmd)\n    return reqs_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.python.make_eval_script_list_py","title":"make_eval_script_list_py","text":"<pre><code>make_eval_script_list_py(instance, specs, env_name, repo_directory, base_commit, test_patch) -&gt; list\n</code></pre> <p>Applies the test patch and runs the tests.</p> Source code in <code>swebench/harness/test_spec/python.py</code> <pre><code>def make_eval_script_list_py(\n    instance, specs, env_name, repo_directory, base_commit, test_patch\n) -&gt; list:\n    \"\"\"\n    Applies the test patch and runs the tests.\n    \"\"\"\n    HEREDOC_DELIMITER = \"EOF_114329324912\"\n    test_files = get_modified_files(test_patch)\n    # Reset test files to the state they should be in before the patch.\n    reset_tests_command = f\"git checkout {base_commit} {' '.join(test_files)}\"\n    apply_test_patch_command = (\n        f\"git apply -v - &lt;&lt;'{HEREDOC_DELIMITER}'\\n{test_patch}\\n{HEREDOC_DELIMITER}\"\n    )\n    test_command = \" \".join(\n        [\n            MAP_REPO_VERSION_TO_SPECS[instance[\"repo\"]][instance[\"version\"]][\n                \"test_cmd\"\n            ],\n            *get_test_directives(instance),\n        ]\n    )\n    eval_commands = [\n        \"source /opt/miniconda3/bin/activate\",\n        f\"conda activate {env_name}\",\n        f\"cd {repo_directory}\",\n    ]\n    if \"eval_commands\" in specs:\n        eval_commands += specs[\"eval_commands\"]\n    eval_commands += [\n        f\"git config --global --add safe.directory {repo_directory}\",  # for nonroot user\n        f\"cd {repo_directory}\",\n        # This is just informational, so we have a record\n        \"git status\",\n        \"git show\",\n        f\"git -c core.fileMode=false diff {base_commit}\",\n        \"source /opt/miniconda3/bin/activate\",\n        f\"conda activate {env_name}\",\n    ]\n    if \"install\" in specs:\n        eval_commands.append(specs[\"install\"])\n    eval_commands += [\n        reset_tests_command,\n        apply_test_patch_command,\n        f\": '{START_TEST_OUTPUT}'\",\n        test_command,\n        f\": '{END_TEST_OUTPUT}'\",\n        reset_tests_command,  # Revert tests after done, leave the repo in the same state as before\n    ]\n    return eval_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.test_spec","title":"test_spec","text":""},{"location":"api/harness/#swebench.harness.test_spec.test_spec.TestSpec","title":"TestSpec  <code>dataclass</code>","text":"<pre><code>TestSpec(instance_id: str, repo: str, version: str, repo_script_list: list[str], eval_script_list: list[str], env_script_list: list[str], arch: str, FAIL_TO_PASS: list[str], PASS_TO_PASS: list[str], language: str, docker_specs: dict, namespace: Optional[str], base_image_tag: str = LATEST, env_image_tag: str = LATEST, instance_image_tag: str = LATEST)\n</code></pre> <p>A dataclass that represents a test specification for a single instance of SWE-bench.</p> <code></code> instance_id <code>instance-attribute</code> <pre><code>instance_id: str\n</code></pre> <code></code> repo <code>instance-attribute</code> <pre><code>repo: str\n</code></pre> <code></code> version <code>instance-attribute</code> <pre><code>version: str\n</code></pre> <code></code> repo_script_list <code>instance-attribute</code> <pre><code>repo_script_list: list[str]\n</code></pre> <code></code> eval_script_list <code>instance-attribute</code> <pre><code>eval_script_list: list[str]\n</code></pre> <code></code> env_script_list <code>instance-attribute</code> <pre><code>env_script_list: list[str]\n</code></pre> <code></code> arch <code>instance-attribute</code> <pre><code>arch: str\n</code></pre> <code></code> FAIL_TO_PASS <code>instance-attribute</code> <pre><code>FAIL_TO_PASS: list[str]\n</code></pre> <code></code> PASS_TO_PASS <code>instance-attribute</code> <pre><code>PASS_TO_PASS: list[str]\n</code></pre> <code></code> language <code>instance-attribute</code> <pre><code>language: str\n</code></pre> <code></code> docker_specs <code>instance-attribute</code> <pre><code>docker_specs: dict\n</code></pre> <code></code> namespace <code>instance-attribute</code> <pre><code>namespace: Optional[str]\n</code></pre> <code></code> base_image_tag <code>class-attribute</code> <code>instance-attribute</code> <pre><code>base_image_tag: str = LATEST\n</code></pre> <code></code> env_image_tag <code>class-attribute</code> <code>instance-attribute</code> <pre><code>env_image_tag: str = LATEST\n</code></pre> <code></code> instance_image_tag <code>class-attribute</code> <code>instance-attribute</code> <pre><code>instance_image_tag: str = LATEST\n</code></pre> <code></code> setup_env_script <code>property</code> <pre><code>setup_env_script\n</code></pre> <code></code> eval_script <code>property</code> <pre><code>eval_script\n</code></pre> <code></code> install_repo_script <code>property</code> <pre><code>install_repo_script\n</code></pre> <code></code> base_image_key <code>property</code> <pre><code>base_image_key\n</code></pre> <p>If docker_specs are present, the base image key includes a hash of the specs.</p> <code></code> env_image_key <code>property</code> <pre><code>env_image_key\n</code></pre> <p>The key for the environment image is based on the hash of the environment script list. If the environment script list changes, the image will be rebuilt automatically.</p> <p>Note that old images are not automatically deleted, so consider cleaning up old images periodically.</p> <code></code> instance_image_key <code>property</code> <pre><code>instance_image_key\n</code></pre> <code></code> is_remote_image <code>property</code> <pre><code>is_remote_image\n</code></pre> <code></code> base_dockerfile <code>property</code> <pre><code>base_dockerfile\n</code></pre> <code></code> env_dockerfile <code>property</code> <pre><code>env_dockerfile\n</code></pre> <code></code> instance_dockerfile <code>property</code> <pre><code>instance_dockerfile\n</code></pre> <code></code> platform <code>property</code> <pre><code>platform\n</code></pre> <code></code> get_instance_container_name <pre><code>get_instance_container_name(run_id=None)\n</code></pre> Source code in <code>swebench/harness/test_spec/test_spec.py</code> <pre><code>def get_instance_container_name(self, run_id=None):\n    if not run_id:\n        return f\"sweb.eval.{self.instance_id}\"\n    return f\"sweb.eval.{self.instance_id.lower()}.{run_id}\"\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.test_spec.get_test_specs_from_dataset","title":"get_test_specs_from_dataset","text":"<pre><code>get_test_specs_from_dataset(dataset: Union[list[SWEbenchInstance], list[TestSpec]], namespace: Optional[str] = None, instance_image_tag: str = LATEST) -&gt; list[TestSpec]\n</code></pre> <p>Idempotent function that converts a list of SWEbenchInstance objects to a list of TestSpec objects.</p> Source code in <code>swebench/harness/test_spec/test_spec.py</code> <pre><code>def get_test_specs_from_dataset(\n    dataset: Union[list[SWEbenchInstance], list[TestSpec]],\n    namespace: Optional[str] = None,\n    instance_image_tag: str = LATEST,\n) -&gt; list[TestSpec]:\n    \"\"\"\n    Idempotent function that converts a list of SWEbenchInstance objects to a list of TestSpec objects.\n    \"\"\"\n    if isinstance(dataset[0], TestSpec):\n        return cast(list[TestSpec], dataset)\n    return list(\n        map(\n            lambda x: make_test_spec(x, namespace, instance_image_tag),\n            cast(list[SWEbenchInstance], dataset),\n        )\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.test_spec.make_test_spec","title":"make_test_spec","text":"<pre><code>make_test_spec(instance: SWEbenchInstance, namespace: Optional[str] = None, base_image_tag: str = LATEST, env_image_tag: str = LATEST, instance_image_tag: str = LATEST) -&gt; TestSpec\n</code></pre> Source code in <code>swebench/harness/test_spec/test_spec.py</code> <pre><code>def make_test_spec(\n    instance: SWEbenchInstance,\n    namespace: Optional[str] = None,\n    base_image_tag: str = LATEST,\n    env_image_tag: str = LATEST,\n    instance_image_tag: str = LATEST,\n) -&gt; TestSpec:\n    if isinstance(instance, TestSpec):\n        return instance\n    assert base_image_tag is not None, \"base_image_tag cannot be None\"\n    assert env_image_tag is not None, \"env_image_tag cannot be None\"\n    assert instance_image_tag is not None, \"instance_image_tag cannot be None\"\n    instance_id = instance[KEY_INSTANCE_ID]\n    repo = instance[\"repo\"]\n    version = instance.get(\"version\")\n    base_commit = instance[\"base_commit\"]\n    problem_statement = instance.get(\"problem_statement\")\n    hints_text = instance.get(\"hints_text\")  # Unused\n    test_patch = instance[\"test_patch\"]\n\n    def _from_json_or_obj(key: str) -&gt; Any:\n        \"\"\"If key points to string, load with json\"\"\"\n        if key not in instance:\n            # If P2P, F2P keys not found, it's a validation instance\n            return []\n        if isinstance(instance[key], str):\n            return json.loads(instance[key])\n        return instance[key]\n\n    pass_to_pass = _from_json_or_obj(\"PASS_TO_PASS\")\n    fail_to_pass = _from_json_or_obj(\"FAIL_TO_PASS\")\n\n    env_name = \"testbed\"\n    repo_directory = f\"/{env_name}\"\n    specs = MAP_REPO_VERSION_TO_SPECS[repo][version]\n    docker_specs = specs.get(\"docker_specs\", {})\n\n    repo_script_list = make_repo_script_list(\n        specs, repo, repo_directory, base_commit, env_name\n    )\n    env_script_list = make_env_script_list(instance, specs, env_name)\n    eval_script_list = make_eval_script_list(\n        instance, specs, env_name, repo_directory, base_commit, test_patch\n    )\n    if platform.machine() in {\"aarch64\", \"arm64\"}:\n        # use arm64 unless explicitly specified\n        arch = \"arm64\" if instance_id not in USE_X86 else \"x86_64\"\n    else:\n        arch = \"x86_64\"\n\n    return TestSpec(\n        instance_id=instance_id,\n        repo=repo,\n        env_script_list=env_script_list,\n        repo_script_list=repo_script_list,\n        eval_script_list=eval_script_list,\n        version=version,\n        arch=arch,\n        FAIL_TO_PASS=fail_to_pass,\n        PASS_TO_PASS=pass_to_pass,\n        language=MAP_REPO_TO_EXT[repo],\n        docker_specs=docker_specs,\n        namespace=namespace,\n        base_image_tag=base_image_tag,\n        env_image_tag=env_image_tag,\n        instance_image_tag=instance_image_tag,\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.utils","title":"utils","text":""},{"location":"api/harness/#swebench.harness.test_spec.utils.get_test_cmds","title":"get_test_cmds","text":"<pre><code>get_test_cmds(instance) -&gt; list\n</code></pre> Source code in <code>swebench/harness/test_spec/utils.py</code> <pre><code>def get_test_cmds(instance) -&gt; list:\n    test_cmd = MAP_REPO_VERSION_TO_SPECS[instance[\"repo\"]][instance[\"version\"]][\n        \"test_cmd\"\n    ]\n    return [test_cmd] if isinstance(test_cmd, str) else test_cmd\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.utils.make_repo_script_list_common","title":"make_repo_script_list_common","text":"<pre><code>make_repo_script_list_common(specs, repo, repo_directory, base_commit, env_name) -&gt; list\n</code></pre> <p>Create a list of bash commands to set up the repository for testing. This is the setup script for the instance image.</p> Source code in <code>swebench/harness/test_spec/utils.py</code> <pre><code>def make_repo_script_list_common(\n    specs, repo, repo_directory, base_commit, env_name\n) -&gt; list:\n    \"\"\"\n    Create a list of bash commands to set up the repository for testing.\n    This is the setup script for the instance image.\n    \"\"\"\n    setup_commands = [\n        f\"git clone -o origin https://github.com/{repo} {repo_directory}\",\n        f\"chmod -R 777 {repo_directory}\",  # So nonroot user can run tests\n        f\"cd {repo_directory}\",\n        f\"git reset --hard {base_commit}\",\n        \"git remote remove origin\",  # Remove the remote so the agent won't see newer commits\n    ]\n    if \"pre_install\" in specs:\n        setup_commands.extend(specs[\"pre_install\"])\n    if \"install\" in specs:\n        setup_commands.extend(specs[\"install\"])\n    if \"build\" in specs:\n        setup_commands.extend(specs[\"build\"])\n    return setup_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.utils.make_env_script_list_common","title":"make_env_script_list_common","text":"<pre><code>make_env_script_list_common(instance, specs, env_name) -&gt; list\n</code></pre> <p>Creates the list of commands to set up the environment for testing. This is the setup script for the environment image.</p> Source code in <code>swebench/harness/test_spec/utils.py</code> <pre><code>def make_env_script_list_common(instance, specs, env_name) -&gt; list:\n    \"\"\"\n    Creates the list of commands to set up the environment for testing.\n    This is the setup script for the environment image.\n    \"\"\"\n    reqs_commands = []\n    if \"apt-pkgs\" in specs:\n        reqs_commands += [\n            \"apt-get update\",\n            f\"apt-get install -y {' '.join(specs['apt-pkgs'])}\",\n        ]\n    return reqs_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.test_spec.utils.make_eval_script_list_common","title":"make_eval_script_list_common","text":"<pre><code>make_eval_script_list_common(instance, specs, env_name, repo_directory, base_commit, test_patch) -&gt; list\n</code></pre> <p>Applies the test patch and runs the tests.</p> Source code in <code>swebench/harness/test_spec/utils.py</code> <pre><code>def make_eval_script_list_common(\n    instance, specs, env_name, repo_directory, base_commit, test_patch\n) -&gt; list:\n    \"\"\"\n    Applies the test patch and runs the tests.\n    \"\"\"\n    HEREDOC_DELIMITER = \"EOF_114329324912\"\n    test_files = get_modified_files(test_patch)\n    # Reset test files to the state they should be in before the patch.\n    if test_files:\n        reset_tests_command = f\"git checkout {base_commit} {' '.join(test_files)}\"\n    else:\n        reset_tests_command = 'echo \"No test files to reset\"'\n\n    build_commands = []\n    if \"build\" in specs:\n        build_commands.extend(specs[\"build\"])\n\n    apply_test_patch_command = f\"git apply --verbose --reject - &lt;&lt;'{HEREDOC_DELIMITER}'\\n{test_patch}\\n{HEREDOC_DELIMITER}\"\n    test_commands = get_test_cmds(instance)\n    eval_commands = [\n        f\"cd {repo_directory}\",\n        f\"git config --global --add safe.directory {repo_directory}\",  # for nonroot user\n        f\"cd {repo_directory}\",\n        # This is just informational, so we have a record\n        # f\"git status\",\n        # f\"git show\",\n        # f\"git -c core.fileMode=false diff {base_commit}\",\n        reset_tests_command,\n        apply_test_patch_command,\n        *build_commands,\n        f\": '{START_TEST_OUTPUT}'\",\n        *test_commands,\n        f\": '{END_TEST_OUTPUT}'\",\n        reset_tests_command,\n    ]\n    return eval_commands\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils","title":"utils","text":""},{"location":"api/harness/#swebench.harness.utils.PATCH_PATTERN","title":"PATCH_PATTERN  <code>module-attribute</code>","text":"<pre><code>PATCH_PATTERN = compile('(?:diff[\\\\w\\\\_\\\\.\\\\ \\\\/\\\\-]+\\\\n)?\\\\-\\\\-\\\\-\\\\s+a\\\\/(?:.*?)\\\\n\\\\+\\\\+\\\\+\\\\s+b\\\\/(?:.*?)(?=diff\\\\ |\\\\-\\\\-\\\\-\\\\ a\\\\/|\\\\Z)', DOTALL)\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.PATCH_FILE_PATTERN","title":"PATCH_FILE_PATTERN  <code>module-attribute</code>","text":"<pre><code>PATCH_FILE_PATTERN = compile('\\\\-\\\\-\\\\-\\\\s+a\\\\/(?:.+)\\\\n\\\\+\\\\+\\\\+\\\\s+b\\\\/(?:.+)')\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.PATCH_HUNK_PATTERN","title":"PATCH_HUNK_PATTERN  <code>module-attribute</code>","text":"<pre><code>PATCH_HUNK_PATTERN = compile('\\\\@\\\\@\\\\s+\\\\-(\\\\d+),(\\\\d+)\\\\s+\\\\+(\\\\d+),(\\\\d+)\\\\s+\\\\@\\\\@(.+?)(?=diff\\\\ |\\\\-\\\\-\\\\-\\\\ a\\\\/|\\\\@\\\\@\\\\ \\\\-|\\\\Z)', DOTALL)\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.EvaluationError","title":"EvaluationError","text":"<pre><code>EvaluationError(instance_id, message, logger)\n</code></pre> <p>               Bases: <code>Exception</code></p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def __init__(self, instance_id, message, logger):\n    super().__init__(message)\n    self.instance_id = instance_id\n    self.log_file = logger.log_file\n    self.logger = logger\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.EvaluationError.instance_id","title":"instance_id  <code>instance-attribute</code>","text":"<pre><code>instance_id = instance_id\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.EvaluationError.log_file","title":"log_file  <code>instance-attribute</code>","text":"<pre><code>log_file = log_file\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.EvaluationError.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.EvaluationError.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>swebench/harness/utils.py</code> <pre><code>def __str__(self):\n    log_msg = traceback.format_exc()\n    self.logger.info(log_msg)\n    return (\n        f\"{self.instance_id}: {super().__str__()}\\n\"\n        f\"Check ({self.log_file}) for more information.\"\n    )\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.get_predictions_from_file","title":"get_predictions_from_file","text":"<pre><code>get_predictions_from_file(predictions_path: str, dataset_name: str, split: str)\n</code></pre> Source code in <code>swebench/harness/utils.py</code> <pre><code>def get_predictions_from_file(predictions_path: str, dataset_name: str, split: str):\n    if predictions_path == \"gold\":\n        print(\"Using gold predictions - ignoring predictions_path\")\n        dataset = load_swebench_dataset(dataset_name, split)\n        return [\n            {\n                KEY_INSTANCE_ID: datum[KEY_INSTANCE_ID],\n                KEY_PREDICTION: datum[\"patch\"],\n                KEY_MODEL: \"gold\",\n            }\n            for datum in dataset\n        ]\n    if predictions_path.endswith(\".json\"):\n        with open(predictions_path, \"r\") as f:\n            predictions = json.load(f)\n            if isinstance(predictions, dict):\n                predictions = list(\n                    predictions.values()\n                )  # compatible with SWE-agent predictions\n            if not isinstance(predictions, list):\n                raise ValueError(\n                    \"Predictions must be a list[prediction] or a dictionary[instance_id: prediction]\"\n                )\n    elif predictions_path.endswith(\".jsonl\"):\n        with open(predictions_path, \"r\") as f:\n            predictions = [json.loads(line) for line in f]\n    else:\n        raise ValueError(\"Predictions path must be .json or .jsonl\")\n\n    # Validate that each prediction has an instance_id\n    for pred in predictions:\n        if not isinstance(pred, dict):\n            raise ValueError(f\"Each prediction must be a dictionary, got {type(pred)}\")\n        if KEY_INSTANCE_ID not in pred:\n            raise ValueError(f\"Each prediction must contain '{KEY_INSTANCE_ID}'\")\n\n    return predictions\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.run_threadpool","title":"run_threadpool","text":"<pre><code>run_threadpool(func, payloads, max_workers)\n</code></pre> Source code in <code>swebench/harness/utils.py</code> <pre><code>def run_threadpool(func, payloads, max_workers):\n    if max_workers &lt;= 0:\n        return run_sequential(func, payloads)\n    succeeded, failed = [], []\n    with tqdm(total=len(payloads), smoothing=0) as pbar:\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Create a future for running each instance\n            futures = {executor.submit(func, *payload): payload for payload in payloads}\n            # Wait for each future to complete\n            for future in as_completed(futures):\n                try:\n                    # Check if instance ran successfully\n                    future.result()\n                    succeeded.append(futures[future])\n                except Exception as e:\n                    print(f\"{type(e)}: {e}\")\n                    traceback.print_exc()\n                    failed.append(futures[future])\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"{len(succeeded)} ran successfully, {len(failed)} failed\"\n                )\n    return succeeded, failed\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.run_sequential","title":"run_sequential","text":"<pre><code>run_sequential(func, args_list)\n</code></pre> <p>Run a function with a list of arguments sequentially</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def run_sequential(func, args_list):\n    \"\"\"\n    Run a function with a list of arguments sequentially\n    \"\"\"\n    succeeded, failed = [], []\n    pbar = tqdm(total=len(args_list), smoothing=0)\n    for args in args_list:\n        try:\n            func(*args)\n            succeeded.append(args)\n        except Exception:\n            traceback.print_exc()\n            failed.append(args)\n        pbar.update(1)\n        pbar.set_description(f\"{len(succeeded)} ran successfully, {len(failed)} failed\")\n    pbar.close()\n    return succeeded, failed\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.load_swebench_dataset","title":"load_swebench_dataset","text":"<pre><code>load_swebench_dataset(name='SWE-bench/SWE-bench', split='test', instance_ids=None) -&gt; list[SWEbenchInstance]\n</code></pre> <p>Load SWE-bench dataset from Hugging Face Datasets or local .json/.jsonl file</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def load_swebench_dataset(\n    name=\"SWE-bench/SWE-bench\", split=\"test\", instance_ids=None\n) -&gt; list[SWEbenchInstance]:\n    \"\"\"\n    Load SWE-bench dataset from Hugging Face Datasets or local .json/.jsonl file\n    \"\"\"\n    # check that all instance IDs are in the dataset\n    if instance_ids:\n        instance_ids = set(instance_ids)\n    # Load from local .json/.jsonl file\n    if name.endswith(\".json\"):\n        dataset = json.loads(Path(name).read_text())\n    elif name.endswith(\".jsonl\"):\n        dataset = [json.loads(line) for line in Path(name).read_text().splitlines()]\n    else:\n        # Load from Hugging Face Datasets\n        if name.lower() in {\"swe-bench\", \"swebench\", \"swe_bench\"}:\n            name = \"SWE-bench/SWE-bench\"\n        elif name.lower() in {\n            \"swe-bench-lite\",\n            \"swebench-lite\",\n            \"swe_bench_lite\",\n            \"swe-bench_lite\",\n            \"lite\",\n        }:\n            name = \"SWE-bench/SWE-bench_Lite\"\n        if (Path(name) / split / \"dataset_info.json\").exists():\n            dataset = cast(Dataset, load_from_disk(Path(name) / split))\n        else:\n            dataset = cast(Dataset, load_dataset(name, split=split))\n    dataset_ids = {instance[KEY_INSTANCE_ID] for instance in dataset}\n    if instance_ids:\n        if instance_ids - dataset_ids:\n            raise ValueError(\n                (\n                    \"Some instance IDs not found in dataset!\"\n                    f\"\\nMissing IDs:\\n{' '.join(instance_ids - dataset_ids)}\"\n                )\n            )\n        dataset = [\n            instance\n            for instance in dataset\n            if instance[KEY_INSTANCE_ID] in instance_ids\n        ]\n    return [cast(SWEbenchInstance, instance) for instance in dataset]\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.get_first_idx","title":"get_first_idx","text":"<pre><code>get_first_idx(charlist)\n</code></pre> <p>Get index of first occurrence of \"-\" or \"+\" in charlist</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def get_first_idx(charlist):\n    \"\"\"Get index of first occurrence of \"-\" or \"+\" in charlist\"\"\"\n    first_min = charlist.index(\"-\") if \"-\" in charlist else len(charlist)\n    first_plus = charlist.index(\"+\") if \"+\" in charlist else len(charlist)\n    return min(first_min, first_plus)\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.get_last_idx","title":"get_last_idx","text":"<pre><code>get_last_idx(charlist)\n</code></pre> <p>Get index of last occurrence of \"-\" or \"+\" in charlist</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def get_last_idx(charlist):\n    \"\"\"Get index of last occurrence of \"-\" or \"+\" in charlist\"\"\"\n    char_idx = get_first_idx(charlist[::-1])\n    last_idx = len(charlist) - char_idx\n    return last_idx + 1\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.strip_content","title":"strip_content","text":"<pre><code>strip_content(hunk)\n</code></pre> <p>Remove trailing non +/- lines and trailing whitespace per line per hunk</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def strip_content(hunk):\n    \"\"\"Remove trailing non +/- lines and trailing whitespace per line per hunk\"\"\"\n    first_chars = list(map(lambda x: None if not len(x) else x[0], hunk.split(\"\\n\")))\n    first_idx = get_first_idx(first_chars)\n    last_idx = get_last_idx(first_chars)\n    new_lines = list(map(lambda x: x.rstrip(), hunk.split(\"\\n\")[first_idx:last_idx]))\n    # should leave one space for empty context lines\n    new_lines = [line if line.strip() else \" \" for line in new_lines]\n    new_hunk = \"\\n\" + \"\\n\".join(new_lines) + \"\\n\"\n    return new_hunk, first_idx - 1\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.get_hunk_stats","title":"get_hunk_stats","text":"<pre><code>get_hunk_stats(pre_start, pre_len, post_start, post_len, hunk, total_delta)\n</code></pre> <p>Recalculate hunk start/end position and diff delta</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def get_hunk_stats(pre_start, pre_len, post_start, post_len, hunk, total_delta):\n    \"\"\"Recalculate hunk start/end position and diff delta\"\"\"\n    stats = {\"context\": 0, \"added\": 0, \"subtracted\": 0}\n    hunk = hunk.split(\"\\n\", 1)[-1].strip(\"\\n\")\n    for line in hunk.split(\"\\n\"):\n        if line.startswith(\"-\"):\n            stats[\"subtracted\"] += 1\n        elif line.startswith(\"+\"):\n            stats[\"added\"] += 1\n        else:\n            stats[\"context\"] += 1\n    context = stats[\"context\"]\n    added = stats[\"added\"]\n    subtracted = stats[\"subtracted\"]\n    pre_len = context + subtracted\n    post_start = pre_start + total_delta\n    post_len = context + added\n    total_delta = total_delta + (post_len - pre_len)\n    return pre_start, pre_len, post_start, post_len, total_delta\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.extract_minimal_patch","title":"extract_minimal_patch","text":"<pre><code>extract_minimal_patch(model_patch)\n</code></pre> <p>Wrapper function that takes hunk and * Removes trailing non +/- lines and trailing whitespace per line per hunk * Recalculates hunk start/end position and diff delta * Returns new patch</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def extract_minimal_patch(model_patch):\n    \"\"\"\n    Wrapper function that takes hunk and\n    * Removes trailing non +/- lines and trailing whitespace per line per hunk\n    * Recalculates hunk start/end position and diff delta\n    * Returns new patch\n    \"\"\"\n    model_patch = model_patch.lstrip(\"\\n\")\n    new_patch = \"\"\n    for patch in PATCH_PATTERN.findall(model_patch):\n        total_delta = 0\n        patch_header = PATCH_FILE_PATTERN.findall(patch)[0]\n        if patch_header:\n            new_patch += patch_header + \"\\n\"\n        for hunk in PATCH_HUNK_PATTERN.findall(patch):\n            pre_start, pre_len, post_start, post_len, content = hunk\n            pre_start, pre_len, post_start, post_len, content = list(\n                map(lambda x: int(x) if x.isnumeric() else x, hunk)\n            )\n            content, adjust_pre_start = strip_content(content)\n            pre_start += adjust_pre_start\n            pre_start, pre_len, post_start, post_len, total_delta = get_hunk_stats(\n                pre_start, pre_len, post_start, post_len, content, total_delta\n            )\n            new_patch += (\n                f\"@@ -{pre_start},{pre_len} +{post_start},{post_len} @@{content}\"\n            )\n    return new_patch\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.has_attribute_or_import_error","title":"has_attribute_or_import_error","text":"<pre><code>has_attribute_or_import_error(log_before)\n</code></pre> <p>Check to see if Attribute/Import-prefix is in log text</p> <p>Parameters:</p> Name Type Description Default <code>log_before</code> <code>str</code> <p>Validation log text before patch application</p> required Source code in <code>swebench/harness/utils.py</code> <pre><code>def has_attribute_or_import_error(log_before):\n    \"\"\"\n    Check to see if Attribute/Import-prefix is in log text\n\n    Args:\n        log_before (str): Validation log text before patch application\n    \"\"\"\n    log_before = log_before.lower()\n\n    if any([x in log_before for x in [\"attribute\", \"import\"]]):\n\n        def get_lines_with_word(text, target_word):\n            # Function to extract line(s) that contains target_word\n            text, target_word = text.lower(), target_word.lower()\n            lines, hits = text.split(\"\\n\")[::-1], []\n            for line in lines:\n                if target_word in line:\n                    hits.append(line)\n            return hits\n\n        # Get line with Attribute/Import error\n        lines_1 = get_lines_with_word(log_before, \"attribute\")\n        lines_2 = get_lines_with_word(log_before, \"import\")\n        lines_1 = \" \".join(lines_1)\n        lines_2 = \" \".join(lines_2)\n\n        if any([(x in lines_1 or x in lines_2) for x in [\"error\", \"fail\"]]):\n            return True\n    return False\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.str2bool","title":"str2bool","text":"<pre><code>str2bool(v)\n</code></pre> <p>Minor helper function to convert string to boolean</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def str2bool(v):\n    \"\"\"\n    Minor helper function to convert string to boolean\n    \"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise ArgumentTypeError(\"Boolean value expected.\")\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.optional_str","title":"optional_str","text":"<pre><code>optional_str(value: str) -&gt; str | None\n</code></pre> <p>Convert special string values to None, otherwise return the string as-is.</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def optional_str(value: str) -&gt; str | None:\n    \"\"\"\n    Convert special string values to None, otherwise return the string as-is.\n    \"\"\"\n    if value.lower() in (\"none\", \"null\", \"\"):\n        return None\n    return value\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.get_repo_file","title":"get_repo_file","text":"<pre><code>get_repo_file(repo, commit, filepath)\n</code></pre> Source code in <code>swebench/harness/utils.py</code> <pre><code>def get_repo_file(repo, commit, filepath):\n    url = f\"https://raw.githubusercontent.com/{repo}/{commit}/{filepath}\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            return response.text\n        return None\n    except:\n        return None\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.get_modified_files","title":"get_modified_files","text":"<pre><code>get_modified_files(patch: str) -&gt; list[str]\n</code></pre> <p>Get the list of modified files in a patch</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def get_modified_files(patch: str) -&gt; list[str]:\n    \"\"\"\n    Get the list of modified files in a patch\n    \"\"\"\n    source_files = []\n    for file in PatchSet(patch):\n        if file.source_file != \"/dev/null\":\n            source_files.append(file.source_file)\n    source_files = [x[2:] for x in source_files if x.startswith(\"a/\")]\n    return source_files\n</code></pre>"},{"location":"api/harness/#swebench.harness.utils.ansi_escape","title":"ansi_escape","text":"<pre><code>ansi_escape(text: str) -&gt; str\n</code></pre> <p>Remove ANSI escape sequences from text</p> Source code in <code>swebench/harness/utils.py</code> <pre><code>def ansi_escape(text: str) -&gt; str:\n    \"\"\"\n    Remove ANSI escape sequences from text\n    \"\"\"\n    return re.compile(r\"\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])\").sub(\"\", text)\n</code></pre>"},{"location":"api/inference/","title":"Inference API","text":""},{"location":"api/inference/#swebench.inference","title":"swebench.inference","text":""},{"location":"api/inference/#swebench.inference.llamao","title":"llamao","text":""},{"location":"api/inference/#swebench.inference.llamao.distributed_attention","title":"distributed_attention","text":""},{"location":"api/inference/#swebench.inference.llamao.distributed_attention.SeqAllToAll","title":"SeqAllToAll","text":"<p>               Bases: <code>Function</code></p> <code></code> forward <code>staticmethod</code> <pre><code>forward(ctx: Any, input: Tensor, scatter_idx: int, gather_idx: int, group: Any) -&gt; Tensor\n</code></pre> Source code in <code>swebench/inference/llamao/distributed_attention.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx: Any, input: Tensor, scatter_idx: int, gather_idx: int, group: Any\n) -&gt; Tensor:\n    ctx.scatter_idx = scatter_idx\n    ctx.gather_idx = gather_idx\n    ctx.group = group\n\n    world_size = dist.get_world_size(group)\n\n    input_list = [\n        t.contiguous() for t in torch.tensor_split(input, world_size, scatter_idx)\n    ]\n    output_list = [torch.empty_like(input_list[0]) for _ in range(world_size)]\n\n    dist.all_to_all(output_list, input_list, group=group)\n    return torch.cat(output_list, dim=gather_idx).contiguous()\n</code></pre> <code></code> backward <code>staticmethod</code> <pre><code>backward(ctx: Any, *grad_output: Tensor) -&gt; tuple[Tensor, None, None, None]\n</code></pre> Source code in <code>swebench/inference/llamao/distributed_attention.py</code> <pre><code>@staticmethod\ndef backward(ctx: Any, *grad_output: Tensor) -&gt; tuple[Tensor, None, None, None]:\n    return (\n        SeqAllToAll.apply(*grad_output, ctx.gather_idx, ctx.scatter_idx, ctx.group),\n        None,\n        None,\n        None,\n    )\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.distributed_attention.DistributedAttention","title":"DistributedAttention","text":"<pre><code>DistributedAttention(local_attention: Module, scatter_idx: int = -2, gather_idx: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initialization.</p> <p>Parameters:</p> Name Type Description Default <code>local_attention</code> <code>Module</code> <p>local attention with q,k,v</p> required <code>scatter_idx</code> <code>int</code> <p>scatter_idx for all2all comm</p> <code>-2</code> <code>gather_idx</code> <code>int</code> <p>gather_idx for all2all comm</p> <code>1</code> Source code in <code>swebench/inference/llamao/distributed_attention.py</code> <pre><code>def __init__(\n    self,\n    local_attention: Module,\n    scatter_idx: int = -2,\n    gather_idx: int = 1,\n) -&gt; None:\n    super().__init__()\n    self.local_attn = local_attention\n    self.scatter_idx = scatter_idx  # head axis\n    self.gather_idx = gather_idx  # seq axis\n</code></pre> <code></code> local_attn <code>instance-attribute</code> <pre><code>local_attn = local_attention\n</code></pre> <code></code> scatter_idx <code>instance-attribute</code> <pre><code>scatter_idx = scatter_idx\n</code></pre> <code></code> gather_idx <code>instance-attribute</code> <pre><code>gather_idx = gather_idx\n</code></pre> <code></code> forward <pre><code>forward(query: Tensor, key_values: Tensor, group: Any = None, **kwargs) -&gt; Tensor\n</code></pre> <p>forward</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>query input to the layer</p> required <code>key</code> <code>Tensor</code> <p>key input to the layer</p> required <code>value</code> <code>Tensor</code> <p>value input to the layer</p> required <code>args</code> <p>other args</p> required <p>Returns:</p> Type Description <code>Tensor</code> <ul> <li>output (Tensor): context output</li> </ul> Source code in <code>swebench/inference/llamao/distributed_attention.py</code> <pre><code>def forward(\n    self, query: Tensor, key_values: Tensor, group: Any = None, **kwargs\n) -&gt; Tensor:\n    \"\"\"forward\n\n    Arguments:\n        query (Tensor): query input to the layer\n        key (Tensor): key input to the layer\n        value (Tensor): value input to the layer\n        args: other args\n\n    Returns:\n        * output (Tensor): context output\n    \"\"\"\n    # in shape : e.g.,  [s/p:h:]\n    query_heads = SeqAllToAll.apply(query, self.scatter_idx, self.gather_idx, group)\n    key_values_heads = SeqAllToAll.apply(\n        key_values, self.scatter_idx, self.gather_idx, group\n    )\n\n    # out shape : e.g., [s:h/p:]\n    output_heads = self.local_attn(query_heads, key_values_heads, **kwargs)\n\n    # out e.g., [s/p::h]\n    return SeqAllToAll.apply(output_heads, self.gather_idx, self.scatter_idx, group)\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama","title":"modeling_flash_llama","text":"<p>PyTorch LLaMA model.</p>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = get_logger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaRMSNorm","title":"LlamaRMSNorm","text":"<pre><code>LlamaRMSNorm(hidden_size, eps=1e-06)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>LlamaRMSNorm is equivalent to T5LayerNorm</p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, hidden_size, eps=1e-6):\n    \"\"\"\n    LlamaRMSNorm is equivalent to T5LayerNorm\n    \"\"\"\n    super().__init__()\n    self.weight = nn.Parameter(torch.ones(hidden_size))\n    self.register_buffer(\n        \"variance_epsilon\",\n        torch.tensor(eps),\n        persistent=False,\n    )\n</code></pre> <code></code> weight <code>instance-attribute</code> <pre><code>weight = Parameter(ones(hidden_size))\n</code></pre> <code></code> forward <pre><code>forward(hidden_states)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(self, hidden_states):\n    return rmsnorm_func(hidden_states, self.weight, self.variance_epsilon)\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.FlashRotaryEmbedding","title":"FlashRotaryEmbedding","text":"<pre><code>FlashRotaryEmbedding(dim: int, base=10000.0, interleaved=False, scale_base=None, scaling_factor=1.0, pos_idx_in_fp32=True, device=None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>The rotary position embeddings from RoFormer_ (Su et. al). A crucial insight from the method is that the query and keys are transformed by rotation matrices which depend on the relative positions.</p> <p>Other implementations are available in the Rotary Transformer repo_ and in GPT-NeoX_, GPT-NeoX was an inspiration</p> <p>.. _RoFormer: https://arxiv.org/abs/2104.09864 .. _repo: https://github.com/ZhuiyiTechnology/roformer .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox</p> <p>If scale_base is not None, this implements XPos (Sun et al., https://arxiv.org/abs/2212.10554). A recommended value for scale_base is 512: https://github.com/HazyResearch/flash-attention/issues/96 Reference: https://github.com/sunyt32/torchscale/blob/main/torchscale/component/xpos_relative_position.py</p> if True, rotate pairs of even and odd dimensions (GPT-J style) instead <p>of 1st half and 2nd half (GPT-NeoX style).</p> <p>pos_idx_in_fp32: if True, the position indices [0.0, ..., seqlen - 1] are in fp32,     otherwise they might be in lower precision.     This option was added because previously (before 2023-07-02), when we construct     the position indices, we use the dtype of self.inv_freq. In most cases this would     be fp32, but if the model is trained in pure bf16 (not mixed precision), then     self.inv_freq would be bf16, and the position indices are also in bf16.     Because of the limited precision of bf16 (e.g. 1995.0 is rounded to 2000.0), the     embeddings for some positions will coincide.     To maintain compatibility with models previously trained in pure bf16,     we add this option. scaling_factor: RotaryEmbedding extended with linear scaling.</p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    base=10000.0,\n    interleaved=False,\n    scale_base=None,\n    scaling_factor=1.0,\n    pos_idx_in_fp32=True,\n    device=None,\n):\n    \"\"\"\n    interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n        of 1st half and 2nd half (GPT-NeoX style).\n    pos_idx_in_fp32: if True, the position indices [0.0, ..., seqlen - 1] are in fp32,\n        otherwise they might be in lower precision.\n        This option was added because previously (before 2023-07-02), when we construct\n        the position indices, we use the dtype of self.inv_freq. In most cases this would\n        be fp32, but if the model is trained in pure bf16 (not mixed precision), then\n        self.inv_freq would be bf16, and the position indices are also in bf16.\n        Because of the limited precision of bf16 (e.g. 1995.0 is rounded to 2000.0), the\n        embeddings for some positions will coincide.\n        To maintain compatibility with models previously trained in pure bf16,\n        we add this option.\n    scaling_factor: RotaryEmbedding extended with linear scaling.\n    \"\"\"\n    super().__init__()\n    self.dim = dim\n    self.base = float(base)\n    self.pos_idx_in_fp32 = pos_idx_in_fp32\n    # Generate and save the inverse frequency buffer (non trainable)\n    inv_freq = self._compute_inv_freq(device)\n    self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n    self.interleaved = interleaved\n    self.scale_base = scale_base\n    self.scaling_factor = scaling_factor\n    scale = (\n        (torch.arange(0, dim, 2, device=device, dtype=torch.float32) + 0.4 * dim)\n        / (1.4 * dim)\n        if scale_base is not None\n        else None\n    )\n    self.register_buffer(\"scale\", scale)\n\n    self._seq_len_cached = 0\n    self._cos_cached = None\n    self._sin_cached = None\n    self._cos_k_cached = None\n    self._sin_k_cached = None\n</code></pre> <code></code> dim <code>instance-attribute</code> <pre><code>dim = dim\n</code></pre> <code></code> base <code>instance-attribute</code> <pre><code>base = float(base)\n</code></pre> <code></code> pos_idx_in_fp32 <code>instance-attribute</code> <pre><code>pos_idx_in_fp32 = pos_idx_in_fp32\n</code></pre> <code></code> interleaved <code>instance-attribute</code> <pre><code>interleaved = interleaved\n</code></pre> <code></code> scale_base <code>instance-attribute</code> <pre><code>scale_base = scale_base\n</code></pre> <code></code> scaling_factor <code>instance-attribute</code> <pre><code>scaling_factor = scaling_factor\n</code></pre> <code></code> forward <pre><code>forward(q: Tensor, k: Tensor, seqlen_offset: int = 0, unpadded_lengths: Optional[tuple[Tensor]] = None) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>q: (batch, seqlen, nheads, headdim) k: (batch, seqlen, nheads, headdim) seqlen_offset: can be used in generation where the qkv being passed in is only the last token in the batch.</p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    seqlen_offset: int = 0,\n    unpadded_lengths: Optional[tuple[torch.Tensor]] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    q: (batch, seqlen, nheads, headdim)\n    k: (batch, seqlen, nheads, headdim)\n    seqlen_offset: can be used in generation where the qkv being passed in is only the last\n    token in the batch.\n    \"\"\"\n    if unpadded_lengths is not None:\n        cu_seqlens, max_seqlen = unpadded_lengths\n    else:\n        cu_seqlens, max_seqlen = None, q.shape[1]\n    self._update_cos_sin_cache(\n        max_seqlen + seqlen_offset, device=q.device, dtype=q.dtype\n    )\n\n    if self.scale is None:\n        return (\n            apply_rotary_emb_func(\n                q,\n                self._cos_cached[seqlen_offset:],\n                self._sin_cached[seqlen_offset:],\n                self.interleaved,\n                True,  # inplace=True,\n                cu_seqlens=cu_seqlens,\n                max_seqlen=max_seqlen,\n            ),\n            apply_rotary_emb_func(\n                k,\n                self._cos_cached[seqlen_offset:],\n                self._sin_cached[seqlen_offset:],\n                self.interleaved,\n                True,  # inplace=True\n                cu_seqlens=cu_seqlens,\n                max_seqlen=max_seqlen,\n            ),\n        )\n    else:\n        assert False\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaMLP","title":"LlamaMLP","text":"<pre><code>LlamaMLP(config)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, config):\n    super().__init__()\n    self.config = config\n    self.hidden_size = config.hidden_size\n    self.intermediate_size = config.intermediate_size\n    self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n    self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n    self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n    self.act_fn = ACT2FN[config.hidden_act]\n</code></pre> <code></code> config <code>instance-attribute</code> <pre><code>config = config\n</code></pre> <code></code> hidden_size <code>instance-attribute</code> <pre><code>hidden_size = hidden_size\n</code></pre> <code></code> intermediate_size <code>instance-attribute</code> <pre><code>intermediate_size = intermediate_size\n</code></pre> <code></code> gate_proj <code>instance-attribute</code> <pre><code>gate_proj = Linear(hidden_size, intermediate_size, bias=False)\n</code></pre> <code></code> up_proj <code>instance-attribute</code> <pre><code>up_proj = Linear(hidden_size, intermediate_size, bias=False)\n</code></pre> <code></code> down_proj <code>instance-attribute</code> <pre><code>down_proj = Linear(intermediate_size, hidden_size, bias=False)\n</code></pre> <code></code> act_fn <code>instance-attribute</code> <pre><code>act_fn = ACT2FN[hidden_act]\n</code></pre> <code></code> forward <pre><code>forward(x)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(self, x):\n    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaAttention","title":"LlamaAttention","text":"<pre><code>LlamaAttention(config: LlamaConfig)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Multi-headed attention from 'Attention Is All You Need' paper</p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, config: LlamaConfig):\n    super().__init__()\n    self.config = config\n    self.hidden_size = config.hidden_size\n    self.num_heads = config.num_attention_heads\n    self.head_dim = self.hidden_size // self.num_heads\n    self.num_key_value_heads = getattr(\n        config, \"num_key_value_heads\", self.num_heads\n    )\n    self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n    self.max_position_embeddings = config.max_position_embeddings\n\n    if (self.head_dim * self.num_heads) != self.hidden_size:\n        raise ValueError(\n            f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n            f\" and `num_heads`: {self.num_heads}).\"\n        )\n    self.q_proj = nn.Linear(\n        self.hidden_size, self.num_heads * self.head_dim, bias=False\n    )\n    self.k_proj = nn.Linear(\n        self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False\n    )\n    self.v_proj = nn.Linear(\n        self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False\n    )\n    self.o_proj = nn.Linear(\n        self.num_heads * self.head_dim, self.hidden_size, bias=False\n    )\n\n    self.register_buffer(\n        \"norm_factor\",\n        torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)).to(\n            torch.get_default_dtype()\n        ),\n        persistent=False,\n    )\n\n    if not getattr(self.config, \"rope_scaling\", None):\n        scaling_factor = 1\n    else:\n        scaling_type = self.config.rope_scaling[\"type\"]\n        scaling_factor = self.config.rope_scaling[\"factor\"]\n        assert scaling_type == \"linear\"\n    theta = getattr(self.config, \"rope_theta\", 10000)\n    self.rotary_emb = FlashRotaryEmbedding(\n        self.head_dim,\n        base=theta,\n        interleaved=False,\n        scaling_factor=scaling_factor,\n    )\n\n    self.distributed_attn_func = DistributedAttention(flash_attn_kvpacked_func)\n</code></pre> <code></code> config <code>instance-attribute</code> <pre><code>config = config\n</code></pre> <code></code> hidden_size <code>instance-attribute</code> <pre><code>hidden_size = hidden_size\n</code></pre> <code></code> num_heads <code>instance-attribute</code> <pre><code>num_heads = num_attention_heads\n</code></pre> <code></code> head_dim <code>instance-attribute</code> <pre><code>head_dim = hidden_size // num_heads\n</code></pre> <code></code> num_key_value_heads <code>instance-attribute</code> <pre><code>num_key_value_heads = getattr(config, 'num_key_value_heads', num_heads)\n</code></pre> <code></code> num_key_value_groups <code>instance-attribute</code> <pre><code>num_key_value_groups = num_heads // num_key_value_heads\n</code></pre> <code></code> max_position_embeddings <code>instance-attribute</code> <pre><code>max_position_embeddings = max_position_embeddings\n</code></pre> <code></code> q_proj <code>instance-attribute</code> <pre><code>q_proj = Linear(hidden_size, num_heads * head_dim, bias=False)\n</code></pre> <code></code> k_proj <code>instance-attribute</code> <pre><code>k_proj = Linear(hidden_size, num_key_value_heads * head_dim, bias=False)\n</code></pre> <code></code> v_proj <code>instance-attribute</code> <pre><code>v_proj = Linear(hidden_size, num_key_value_heads * head_dim, bias=False)\n</code></pre> <code></code> o_proj <code>instance-attribute</code> <pre><code>o_proj = Linear(num_heads * head_dim, hidden_size, bias=False)\n</code></pre> <code></code> rotary_emb <code>instance-attribute</code> <pre><code>rotary_emb = FlashRotaryEmbedding(head_dim, base=theta, interleaved=False, scaling_factor=scaling_factor)\n</code></pre> <code></code> distributed_attn_func <code>instance-attribute</code> <pre><code>distributed_attn_func = DistributedAttention(flash_attn_kvpacked_func)\n</code></pre> <code></code> forward <pre><code>forward(hidden_states: Tensor, attention_mask: Optional[Tensor] = None, position_ids: Optional[LongTensor] = None, past_key_value: Optional[tuple[Tensor]] = None, output_attentions: bool = False, use_cache: bool = False, unpadded_lengths: Optional[tuple[Tensor]] = None, seq_parallel_group: Optional[Any] = None) -&gt; tuple[Tensor, Optional[Tensor], Optional[tuple[Tensor]]]\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    unpadded_lengths: Optional[tuple[torch.Tensor]] = None,\n    seq_parallel_group: Optional[Any] = None,\n) -&gt; tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n    h_size = hidden_states.size(-1)\n\n    has_layer_past = past_key_value is not None and past_key_value[0] is not None\n\n    if has_layer_past:\n        past_kv = past_key_value[0]\n        past_len = past_key_value[1]\n    else:\n        past_len = 0\n\n    # NOTE: Hack to include position_ids, assuming they are increasing uniformly per block\n    if position_ids is not None:\n        past_len += position_ids.min()\n\n    q = self.q_proj(hidden_states)\n    k = self.k_proj(hidden_states)\n    v = self.v_proj(hidden_states)\n\n    q = q.view(*q.shape[:-1], self.num_heads, self.head_dim)\n    k = k.view(*k.shape[:-1], self.num_key_value_heads, self.head_dim)\n    v = v.view(*v.shape[:-1], self.num_key_value_heads, self.head_dim)\n\n    q, k = self.rotary_emb(q, k, past_len, unpadded_lengths)\n\n    kv = torch.stack([k, v], -3)\n    kv = repeat_kv(kv, self.num_key_value_groups)\n\n    # Cache QKV values\n    if has_layer_past:\n        new_len = past_len + q.size(1)\n        if new_len &gt; past_kv.size(1):\n            past_kv = torch.cat(\n                [\n                    past_kv,\n                    torch.empty(\n                        hidden_states.size(0),\n                        256,\n                        2,\n                        kv.size(3),\n                        kv.size(4),\n                        dtype=kv.dtype,\n                        device=kv.device,\n                    ),\n                ],\n                1,\n            )\n        past_kv[:, past_len:new_len] = kv\n        kv = past_kv[:, :new_len]\n    else:\n        past_kv = kv\n\n    past_key_value = (past_kv, past_len + q.size(1)) if use_cache else None\n\n    if dist.is_initialized() and dist.get_world_size(seq_parallel_group) &gt; 1:\n        # NOTE: we assume that padding tokens are at the end of the sequence and may ignore `attention_mask`\n        assert output_attentions is False\n        attn_outputs = self.distributed_attn_func(\n            q,\n            kv,\n            dropout_p=0.0,\n            softmax_scale=1.0 / self.norm_factor,\n            causal=True,\n            return_attn_probs=False,\n            group=seq_parallel_group,\n        )\n    else:\n        if unpadded_lengths is not None:\n            # varlen, ignore padding tokens, efficient for large batch with many paddings\n            assert attention_mask is not None\n            cu_seqlens, max_seqlen = unpadded_lengths\n\n            attn_outputs = flash_attn_varlen_kvpacked_func(\n                q,\n                kv,\n                cu_seqlens,\n                cu_seqlens,\n                max_seqlen,\n                max_seqlen,\n                dropout_p=0.0,\n                softmax_scale=1.0 / self.norm_factor,\n                causal=True,\n                return_attn_probs=output_attentions,\n            )\n        else:\n            attn_outputs = flash_attn_kvpacked_func(\n                q,\n                kv,\n                dropout_p=0.0,\n                softmax_scale=1.0 / self.norm_factor,\n                causal=True,\n                return_attn_probs=output_attentions,\n            )\n\n    attn_output = attn_outputs[0] if output_attentions else attn_outputs\n    attn_output = attn_output.reshape(*attn_output.shape[:-2], h_size)\n    attn_weights = attn_outputs[2] if output_attentions else None\n\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaDecoderLayer","title":"LlamaDecoderLayer","text":"<pre><code>LlamaDecoderLayer(config: LlamaConfig)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, config: LlamaConfig):\n    super().__init__()\n    self.hidden_size = config.hidden_size\n    self.self_attn = LlamaAttention(config=config)\n    self.mlp = LlamaMLP(config)\n    self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n    self.post_attention_layernorm = LlamaRMSNorm(\n        config.hidden_size, eps=config.rms_norm_eps\n    )\n    self._fsdp_wrap = True\n</code></pre> <code></code> hidden_size <code>instance-attribute</code> <pre><code>hidden_size = hidden_size\n</code></pre> <code></code> self_attn <code>instance-attribute</code> <pre><code>self_attn = LlamaAttention(config=config)\n</code></pre> <code></code> mlp <code>instance-attribute</code> <pre><code>mlp = LlamaMLP(config)\n</code></pre> <code></code> input_layernorm <code>instance-attribute</code> <pre><code>input_layernorm = LlamaRMSNorm(hidden_size, eps=rms_norm_eps)\n</code></pre> <code></code> post_attention_layernorm <code>instance-attribute</code> <pre><code>post_attention_layernorm = LlamaRMSNorm(hidden_size, eps=rms_norm_eps)\n</code></pre> <code></code> forward <pre><code>forward(hidden_states: Tensor, attention_mask: Optional[Tensor] = None, position_ids: Optional[LongTensor] = None, past_key_value: Optional[tuple[Tensor]] = None, unpadded_lengths: Optional[tuple[Tensor]] = None, output_attentions: Optional[bool] = False, use_cache: Optional[bool] = False, seq_parallel_group: Optional[Any] = None) -&gt; tuple[FloatTensor, Optional[tuple[FloatTensor, FloatTensor]]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>hidden_states</code> <code>`torch.FloatTensor`</code> <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p> required <code>attention_mask</code> <code>`torch.FloatTensor`, *optional*</code> <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very large negative values.</p> <code>None</code> <code>output_attentions</code> <code>`bool`, *optional*</code> <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more detail.</p> <code>False</code> <code>use_cache</code> <code>`bool`, *optional*</code> <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see <code>past_key_values</code>).</p> <code>False</code> <code>past_key_value</code> <code>`Tuple(torch.FloatTensor)`, *optional*</code> <p>cached past key and value projection states</p> <code>None</code> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[tuple[torch.Tensor]] = None,\n    unpadded_lengths: Optional[tuple[torch.Tensor]] = None,\n    output_attentions: Optional[bool] = False,\n    use_cache: Optional[bool] = False,\n    seq_parallel_group: Optional[Any] = None,\n) -&gt; tuple[\n    torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]\n]:\n    \"\"\"\n    Args:\n        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n            (see `past_key_values`).\n        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n    \"\"\"\n\n    residual = hidden_states\n\n    hidden_states = self.input_layernorm(hidden_states)\n\n    # Self Attention\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n        hidden_states=hidden_states,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_value=past_key_value,\n        output_attentions=output_attentions,\n        use_cache=use_cache,\n        unpadded_lengths=unpadded_lengths,\n        seq_parallel_group=seq_parallel_group,\n    )\n    hidden_states = residual + hidden_states\n\n    # Fully Connected\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = self.mlp(hidden_states)\n    hidden_states = residual + hidden_states\n\n    outputs = (hidden_states,)\n\n    if output_attentions:\n        outputs += (self_attn_weights,)\n\n    if use_cache:\n        outputs += (present_key_value,)\n\n    return outputs\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaPreTrainedModel","title":"LlamaPreTrainedModel","text":"<p>               Bases: <code>PreTrainedModel</code>, <code>GenerationMixin</code></p> <code></code> config_class <code>class-attribute</code> <code>instance-attribute</code> <pre><code>config_class = LlamaConfig\n</code></pre> <code></code> base_model_prefix <code>class-attribute</code> <code>instance-attribute</code> <pre><code>base_model_prefix = 'model'\n</code></pre> <code></code> supports_gradient_checkpointing <code>class-attribute</code> <code>instance-attribute</code> <pre><code>supports_gradient_checkpointing = True\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaModel","title":"LlamaModel","text":"<pre><code>LlamaModel(config: LlamaConfig)\n</code></pre> <p>               Bases: <code>LlamaPreTrainedModel</code></p> <p>Transformer decoder consisting of config.num_hidden_layers layers. Each layer is a [<code>LlamaDecoderLayer</code>]</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LlamaConfig</code> <p>LlamaConfig</p> required Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, config: LlamaConfig):\n    super().__init__(config)\n    self.padding_idx = config.pad_token_id\n    self.vocab_size = config.vocab_size\n\n    self.embed_tokens = nn.Embedding(\n        config.vocab_size, config.hidden_size, self.padding_idx\n    )\n    self.layers = nn.ModuleList(\n        [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n    )\n    self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    self.gradient_checkpointing = False\n    # Initialize weights and apply final processing\n    self.post_init()\n</code></pre> <code></code> padding_idx <code>instance-attribute</code> <pre><code>padding_idx = pad_token_id\n</code></pre> <code></code> vocab_size <code>instance-attribute</code> <pre><code>vocab_size = vocab_size\n</code></pre> <code></code> embed_tokens <code>instance-attribute</code> <pre><code>embed_tokens = Embedding(vocab_size, hidden_size, padding_idx)\n</code></pre> <code></code> layers <code>instance-attribute</code> <pre><code>layers = ModuleList([(LlamaDecoderLayer(config)) for _ in (range(num_hidden_layers))])\n</code></pre> <code></code> norm <code>instance-attribute</code> <pre><code>norm = LlamaRMSNorm(hidden_size, eps=rms_norm_eps)\n</code></pre> <code></code> gradient_checkpointing <code>instance-attribute</code> <pre><code>gradient_checkpointing = False\n</code></pre> <code></code> get_input_embeddings <pre><code>get_input_embeddings()\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def get_input_embeddings(self):\n    return self.embed_tokens\n</code></pre> <code></code> set_input_embeddings <pre><code>set_input_embeddings(value)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def set_input_embeddings(self, value):\n    self.embed_tokens = value\n</code></pre> <code></code> forward <pre><code>forward(input_ids: LongTensor = None, attention_mask: Optional[Tensor] = None, position_ids: Optional[LongTensor] = None, past_key_values: Optional[list[FloatTensor]] = None, inputs_embeds: Optional[FloatTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, seq_parallel_group: Optional[Any] = None) -&gt; Union[tuple, BaseModelOutputWithPast]\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[list[torch.FloatTensor]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    seq_parallel_group: Optional[Any] = None,\n) -&gt; Union[tuple, BaseModelOutputWithPast]:\n    output_attentions = (\n        output_attentions\n        if output_attentions is not None\n        else self.config.output_attentions\n    )\n    output_hidden_states = (\n        output_hidden_states\n        if output_hidden_states is not None\n        else self.config.output_hidden_states\n    )\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    # retrieve input_ids and inputs_embeds\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\n            \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n        )\n    elif input_ids is not None:\n        batch_size, seq_length = input_ids.shape\n    elif inputs_embeds is not None:\n        batch_size, seq_length, _ = inputs_embeds.shape\n    else:\n        raise ValueError(\n            \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n        )\n\n    # position_ids = None\n\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids)\n\n    hidden_states = inputs_embeds\n    bsz = hidden_states.size(0)\n\n    if self.gradient_checkpointing and self.training:\n        if use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n            )\n            use_cache = False\n\n    if (\n        ((attention_mask is not None) and (not attention_mask.all().item()))\n        and not use_cache\n        and not (\n            dist.is_initialized() and dist.get_world_size(seq_parallel_group) &gt; 1\n        )\n    ):\n        hidden_states, unpad_indices, cu_seqlens, max_seqlen = unpad_input(\n            hidden_states, attention_mask\n        )\n        unpadded_lengths = (cu_seqlens, max_seqlen)\n    else:\n        unpadded_lengths = None\n\n    # decoder layers\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    next_decoder_cache = () if use_cache else None\n\n    for idx, decoder_layer in enumerate(self.layers):\n        if output_hidden_states:\n            if unpadded_lengths is not None:\n                all_hidden_states += (\n                    pad_input(hidden_states, unpad_indices, bsz, max_seqlen),\n                )\n            else:\n                all_hidden_states += (hidden_states,)\n\n        past_key_value = (\n            past_key_values[idx] if past_key_values is not None and idx &lt; len(past_key_values) else None\n        )\n\n        if self.gradient_checkpointing and self.training:\n            layer_outputs = torch.utils.checkpoint.checkpoint(\n                decoder_layer,\n                hidden_states,\n                attention_mask,\n                position_ids,\n                None,\n                unpadded_lengths,\n                output_attentions,\n                False,\n                seq_parallel_group,\n            )\n        else:\n            layer_outputs = decoder_layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                unpadded_lengths=unpadded_lengths,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                seq_parallel_group=seq_parallel_group,\n            )\n\n        hidden_states = layer_outputs[0]\n\n        if use_cache:\n            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n\n    if unpadded_lengths is not None:\n        hidden_states = pad_input(hidden_states, unpad_indices, bsz, max_seqlen)\n    hidden_states = self.norm(hidden_states)\n\n    # add hidden states from the last decoder layer\n    if output_hidden_states:\n        all_hidden_states += (hidden_states,)\n\n    next_cache = next_decoder_cache if use_cache else None\n    if not return_dict:\n        return tuple(\n            v\n            for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n            if v is not None\n        )\n    return BaseModelOutputWithPast(\n        last_hidden_state=hidden_states,\n        past_key_values=next_cache,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attns,\n    )\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaForCausalLM","title":"LlamaForCausalLM","text":"<pre><code>LlamaForCausalLM(config)\n</code></pre> <p>               Bases: <code>LlamaPreTrainedModel</code>, <code>GenerationMixin</code></p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, config):\n    super().__init__(config)\n    self.model = LlamaModel(config)\n    self.vocab_size = config.vocab_size\n    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    # Initialize weights and apply final processing\n    self.post_init()\n</code></pre> <code></code> model <code>instance-attribute</code> <pre><code>model = LlamaModel(config)\n</code></pre> <code></code> vocab_size <code>instance-attribute</code> <pre><code>vocab_size = vocab_size\n</code></pre> <code></code> lm_head <code>instance-attribute</code> <pre><code>lm_head = Linear(hidden_size, vocab_size, bias=False)\n</code></pre> <code></code> get_input_embeddings <pre><code>get_input_embeddings()\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def get_input_embeddings(self):\n    return self.model.embed_tokens\n</code></pre> <code></code> set_input_embeddings <pre><code>set_input_embeddings(value)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def set_input_embeddings(self, value):\n    self.model.embed_tokens = value\n</code></pre> <code></code> get_output_embeddings <pre><code>get_output_embeddings()\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def get_output_embeddings(self):\n    return self.lm_head\n</code></pre> <code></code> set_output_embeddings <pre><code>set_output_embeddings(new_embeddings)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def set_output_embeddings(self, new_embeddings):\n    self.lm_head = new_embeddings\n</code></pre> <code></code> set_decoder <pre><code>set_decoder(decoder)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def set_decoder(self, decoder):\n    self.model = decoder\n</code></pre> <code></code> get_decoder <pre><code>get_decoder()\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def get_decoder(self):\n    return self.model\n</code></pre> <code></code> forward <pre><code>forward(input_ids: LongTensor = None, attention_mask: Optional[Tensor] = None, position_ids: Optional[LongTensor] = None, past_key_values: Optional[list[FloatTensor]] = None, inputs_embeds: Optional[FloatTensor] = None, labels: Optional[LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, unpadded_lengths: Optional[bool] = None, avg_valid_labels_per_chunk: Optional[float] = None, seq_parallel_group: Optional[Any] = None) -&gt; Union[tuple, CausalLMOutputWithPast]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*</code> <p>Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.</p> <code>None</code> <p>Returns:</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer, LlamaForCausalLM\n\n&gt;&gt;&gt; model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n&gt;&gt;&gt; prompt = \"Hey, are you conscious? Can you talk to me?\"\n&gt;&gt;&gt; inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n&gt;&gt;&gt; # Generate\n&gt;&gt;&gt; generate_ids = model.generate(inputs.input_ids, max_length=30)\n&gt;&gt;&gt; tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[list[torch.FloatTensor]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[torch.LongTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    unpadded_lengths: Optional[bool] = None,\n    avg_valid_labels_per_chunk: Optional[float] = None,\n    seq_parallel_group: Optional[Any] = None,\n) -&gt; Union[tuple, CausalLMOutputWithPast]:\n    r\"\"\"\n    Args:\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n    Returns:\n\n    Example:\n\n    ```python\n    &gt;&gt;&gt; from transformers import AutoTokenizer, LlamaForCausalLM\n\n    &gt;&gt;&gt; model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n    &gt;&gt;&gt; prompt = \"Hey, are you conscious? Can you talk to me?\"\n    &gt;&gt;&gt; inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n    &gt;&gt;&gt; # Generate\n    &gt;&gt;&gt; generate_ids = model.generate(inputs.input_ids, max_length=30)\n    &gt;&gt;&gt; tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n    ```\"\"\"\n\n    output_attentions = (\n        output_attentions\n        if output_attentions is not None\n        else self.config.output_attentions\n    )\n    output_hidden_states = (\n        output_hidden_states\n        if output_hidden_states is not None\n        else self.config.output_hidden_states\n    )\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_values=past_key_values,\n        inputs_embeds=inputs_embeds,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        seq_parallel_group=seq_parallel_group,\n    )\n\n    hidden_states = outputs[0]\n    loss = None\n    if labels is not None:\n        # Only compute loss on tokens that contribute to loss\n        valid_prediction = labels != -100\n        hidden_states_ = hidden_states[valid_prediction]\n        logits = self.lm_head(hidden_states_).float()\n\n        # NOTE: We don't shift the labels inside the model here!\n        labels_ = labels[valid_prediction]\n\n        if (\n            avg_valid_labels_per_chunk is not None\n            and avg_valid_labels_per_chunk &gt; 0\n        ):\n            # Don't take mean since this will give unequal weight to GPUs with unequal amount of padding\n            loss = F.cross_entropy(logits, labels_, reduction=\"mean\") * (\n                labels_.numel() / avg_valid_labels_per_chunk\n            )\n            if not valid_prediction.any():\n                loss.data = torch.zeros_like(loss)\n        else:\n            loss = F.cross_entropy(logits, labels_, reduction=\"mean\")\n    else:\n        logits = self.lm_head(hidden_states).float()\n\n    if not return_dict:\n        output = (logits,) + outputs[1:]\n        return (loss,) + output if loss is not None else output\n\n    return CausalLMOutputWithPast(\n        loss=loss,\n        logits=logits,\n        past_key_values=outputs.past_key_values,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre> <code></code> prepare_inputs_for_generation <pre><code>prepare_inputs_for_generation(input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def prepare_inputs_for_generation(\n    self,\n    input_ids,\n    past_key_values=None,\n    attention_mask=None,\n    inputs_embeds=None,\n    **kwargs,\n):\n    if past_key_values:\n        input_ids = input_ids[:, -1:]\n\n    # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n    if inputs_embeds is not None and past_key_values is None:\n        model_inputs = {\"inputs_embeds\": inputs_embeds}\n    else:\n        model_inputs = {\"input_ids\": input_ids}\n\n    model_inputs.update(\n        {\n            \"position_ids\": kwargs.get(\"position_ids\", None),\n            \"past_key_values\": past_key_values,\n            \"use_cache\": kwargs.get(\"use_cache\"),\n            \"attention_mask\": attention_mask,\n            \"unpadded_lengths\": (\n                (attention_mask is not None) and (not attention_mask.all().item())\n            ),\n            \"seq_parallel_group\": kwargs.get(\"seq_parallel_group\"),\n        }\n    )\n    return model_inputs\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.LlamaForSequenceClassification","title":"LlamaForSequenceClassification","text":"<pre><code>LlamaForSequenceClassification(config)\n</code></pre> <p>               Bases: <code>LlamaPreTrainedModel</code></p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def __init__(self, config):\n    super().__init__(config)\n    self.num_labels = config.num_labels\n    self.model = LlamaModel(config)\n    self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n    # Initialize weights and apply final processing\n    self.post_init()\n</code></pre> <code></code> num_labels <code>instance-attribute</code> <pre><code>num_labels = num_labels\n</code></pre> <code></code> model <code>instance-attribute</code> <pre><code>model = LlamaModel(config)\n</code></pre> <code></code> score <code>instance-attribute</code> <pre><code>score = Linear(hidden_size, num_labels, bias=False)\n</code></pre> <code></code> get_input_embeddings <pre><code>get_input_embeddings()\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def get_input_embeddings(self):\n    return self.model.embed_tokens\n</code></pre> <code></code> set_input_embeddings <pre><code>set_input_embeddings(value)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def set_input_embeddings(self, value):\n    self.model.embed_tokens = value\n</code></pre> <code></code> forward <pre><code>forward(input_ids: LongTensor = None, attention_mask: Optional[Tensor] = None, position_ids: Optional[LongTensor] = None, past_key_values: Optional[list[FloatTensor]] = None, inputs_embeds: Optional[FloatTensor] = None, labels: Optional[LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -&gt; Union[tuple, SequenceClassifierOutputWithPast]\n</code></pre> <p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[list[torch.FloatTensor]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[torch.LongTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[tuple, SequenceClassifierOutputWithPast]:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    transformer_outputs = self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_values=past_key_values,\n        inputs_embeds=inputs_embeds,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    hidden_states = transformer_outputs[0]\n    logits = self.score(hidden_states)\n\n    if input_ids is not None:\n        batch_size = input_ids.shape[0]\n    else:\n        batch_size = inputs_embeds.shape[0]\n\n    if self.config.pad_token_id is None and batch_size != 1:\n        raise ValueError(\n            \"Cannot handle batch sizes &gt; 1 if no padding token is defined.\"\n        )\n    if self.config.pad_token_id is None:\n        sequence_lengths = -1\n    else:\n        if input_ids is not None:\n            sequence_lengths = (\n                torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n            ).to(logits.device)\n        else:\n            sequence_lengths = -1\n\n    pooled_logits = logits[\n        torch.arange(batch_size, device=logits.device), sequence_lengths\n    ]\n\n    loss = None\n    if labels is not None:\n        labels = labels.to(logits.device)\n        if self.config.problem_type is None:\n            if self.num_labels == 1:\n                self.config.problem_type = \"regression\"\n            elif self.num_labels &gt; 1 and (\n                labels.dtype == torch.long or labels.dtype == torch.int\n            ):\n                self.config.problem_type = \"single_label_classification\"\n            else:\n                self.config.problem_type = \"multi_label_classification\"\n\n        if self.config.problem_type == \"regression\":\n            loss_fct = MSELoss()\n            if self.num_labels == 1:\n                loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n            else:\n                loss = loss_fct(pooled_logits, labels)\n        elif self.config.problem_type == \"single_label_classification\":\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(\n                pooled_logits.view(-1, self.num_labels), labels.view(-1)\n            )\n        elif self.config.problem_type == \"multi_label_classification\":\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(pooled_logits, labels)\n    if not return_dict:\n        output = (pooled_logits,) + transformer_outputs[1:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutputWithPast(\n        loss=loss,\n        logits=pooled_logits,\n        past_key_values=transformer_outputs.past_key_values,\n        hidden_states=transformer_outputs.hidden_states,\n        attentions=transformer_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.rmsnorm_func","title":"rmsnorm_func","text":"<pre><code>rmsnorm_func(hidden_states, weight, variance_epsilon)\n</code></pre> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>def rmsnorm_func(hidden_states, weight, variance_epsilon):\n    input_dtype = hidden_states.dtype\n    hidden_states = hidden_states.to(torch.float32)\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n    return (weight * hidden_states).to(input_dtype)\n</code></pre>"},{"location":"api/inference/#swebench.inference.llamao.modeling_flash_llama.repeat_kv","title":"repeat_kv","text":"<pre><code>repeat_kv(hidden_states: Tensor, n_rep: int) -&gt; Tensor\n</code></pre> <p>This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)</p> Source code in <code>swebench/inference/llamao/modeling_flash_llama.py</code> <pre><code>@torch.jit.script\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    if n_rep == 1:\n        return hidden_states\n    final_shape = list(hidden_states.shape[:-2]) + [-1] + [hidden_states.shape[-1]]\n    expand_shape = [-1] * (len(hidden_states.shape) - 1) + [n_rep] + [-1]\n    hidden_states = hidden_states.unsqueeze(-1).expand(expand_shape)\n    return hidden_states.reshape(final_shape)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets","title":"make_datasets","text":""},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval","title":"bm25_retrieval","text":""},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.DOCUMENT_ENCODING_FUNCTIONS","title":"DOCUMENT_ENCODING_FUNCTIONS  <code>module-attribute</code>","text":"<pre><code>DOCUMENT_ENCODING_FUNCTIONS = {'file_name_and_contents': file_name_and_contents, 'file_name_and_documentation': file_name_and_documentation, 'file_name_and_docs_jedi': file_name_and_docs_jedi}\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser()\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.ContextManager","title":"ContextManager","text":"<pre><code>ContextManager(repo_path, base_commit, verbose=False)\n</code></pre> <p>A context manager for managing a Git repository at a specific commit.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>str</code> <p>The path to the Git repository.</p> required <code>base_commit</code> <code>str</code> <p>The commit hash to switch to.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>repo_path</code> <code>str</code> <p>The path to the Git repository.</p> <code>base_commit</code> <code>str</code> <p>The commit hash to switch to.</p> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>repo</code> <code>Repo</code> <p>The Git repository object.</p> <p>Methods:</p> Name Description <code>__enter__</code> <p>Switches to the specified commit and returns the context manager object.</p> <code>get_readme_files</code> <p>Returns a list of filenames for all README files in the repository.</p> <code>__exit__</code> <p>Does nothing.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def __init__(self, repo_path, base_commit, verbose=False):\n    self.repo_path = Path(repo_path).resolve().as_posix()\n    self.base_commit = base_commit\n    self.verbose = verbose\n    self.repo = Repo(self.repo_path)\n</code></pre> <code></code> repo_path <code>instance-attribute</code> <pre><code>repo_path = as_posix()\n</code></pre> <code></code> base_commit <code>instance-attribute</code> <pre><code>base_commit = base_commit\n</code></pre> <code></code> verbose <code>instance-attribute</code> <pre><code>verbose = verbose\n</code></pre> <code></code> repo <code>instance-attribute</code> <pre><code>repo = Repo(repo_path)\n</code></pre> <code></code> __enter__ <pre><code>__enter__()\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def __enter__(self):\n    if self.verbose:\n        print(f\"Switching to {self.base_commit}\")\n    try:\n        self.repo.git.reset(\"--hard\", self.base_commit)\n        self.repo.git.clean(\"-fdxq\")\n    except Exception as e:\n        logger.error(f\"Failed to switch to {self.base_commit}\")\n        logger.error(e)\n        raise e\n    return self\n</code></pre> <code></code> get_readme_files <pre><code>get_readme_files()\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def get_readme_files(self):\n    files = os.listdir(self.repo_path)\n    files = list(filter(lambda x: os.path.isfile(x), files))\n    files = list(filter(lambda x: x.lower().startswith(\"readme\"), files))\n    return files\n</code></pre> <code></code> __exit__ <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    pass\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.file_name_and_contents","title":"file_name_and_contents","text":"<pre><code>file_name_and_contents(filename, relative_path)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def file_name_and_contents(filename, relative_path):\n    text = relative_path + \"\\n\"\n    with open(filename) as f:\n        text += f.read()\n    return text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.file_name_and_documentation","title":"file_name_and_documentation","text":"<pre><code>file_name_and_documentation(filename, relative_path)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def file_name_and_documentation(filename, relative_path):\n    text = relative_path + \"\\n\"\n    try:\n        with open(filename) as f:\n            node = ast.parse(f.read())\n        data = ast.get_docstring(node)\n        if data:\n            text += f\"{data}\"\n        for child_node in ast.walk(node):\n            if isinstance(\n                child_node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)\n            ):\n                data = ast.get_docstring(child_node)\n                if data:\n                    text += f\"\\n\\n{child_node.name}\\n{data}\"\n    except Exception as e:\n        logger.error(e)\n        logger.error(f\"Failed to parse file {str(filename)}. Using simple filecontent.\")\n        with open(filename) as f:\n            text += f.read()\n    return text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.file_name_and_docs_jedi","title":"file_name_and_docs_jedi","text":"<pre><code>file_name_and_docs_jedi(filename, relative_path)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def file_name_and_docs_jedi(filename, relative_path):\n    text = relative_path + \"\\n\"\n    with open(filename) as f:\n        source_code = f.read()\n    try:\n        script = jedi.Script(source_code, path=filename)\n        module = script.get_context()\n        docstring = module.docstring()\n        text += f\"{module.full_name}\\n\"\n        if docstring:\n            text += f\"{docstring}\\n\\n\"\n        abspath = Path(filename).absolute()\n        names = [\n            name\n            for name in script.get_names(\n                all_scopes=True, definitions=True, references=False\n            )\n            if not name.in_builtin_module()\n        ]\n        for name in names:\n            try:\n                origin = name.goto(follow_imports=True)[0]\n                if origin.module_name != module.full_name:\n                    continue\n                if name.parent().full_name != module.full_name:\n                    if name.type in {\"statement\", \"param\"}:\n                        continue\n                full_name = name.full_name\n                text += f\"{full_name}\\n\"\n                docstring = name.docstring()\n                if docstring:\n                    text += f\"{docstring}\\n\\n\"\n            except:\n                continue\n    except Exception as e:\n        logger.error(e)\n        logger.error(f\"Failed to parse file {str(filename)}. Using simple filecontent.\")\n        text = f\"{relative_path}\\n{source_code}\"\n        return text\n    return text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.clone_repo","title":"clone_repo","text":"<pre><code>clone_repo(repo, root_dir, token)\n</code></pre> <p>Clones a GitHub repository to a specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>str</code> <p>The GitHub repository to clone.</p> required <code>root_dir</code> <code>str</code> <p>The root directory to clone the repository to.</p> required <code>token</code> <code>str</code> <p>The GitHub personal access token to use for authentication.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <p>The path to the cloned repository directory.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def clone_repo(repo, root_dir, token):\n    \"\"\"\n    Clones a GitHub repository to a specified directory.\n\n    Args:\n        repo (str): The GitHub repository to clone.\n        root_dir (str): The root directory to clone the repository to.\n        token (str): The GitHub personal access token to use for authentication.\n\n    Returns:\n        Path: The path to the cloned repository directory.\n    \"\"\"\n    repo_dir = Path(root_dir, f\"repo__{repo.replace('/', '__')}\")\n\n    if not repo_dir.exists():\n        repo_url = f\"https://{token}@github.com/{repo}.git\"\n        logger.info(f\"Cloning {repo} {os.getpid()}\")\n        Repo.clone_from(repo_url, repo_dir)\n    return repo_dir\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.build_documents","title":"build_documents","text":"<pre><code>build_documents(repo_dir, commit, document_encoding_func)\n</code></pre> <p>Builds a dictionary of documents from a given repository directory and commit.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>str</code> <p>The path to the repository directory.</p> required <code>commit</code> <code>str</code> <p>The commit hash to use.</p> required <code>document_encoding_func</code> <code>function</code> <p>A function that takes a filename and a relative path and returns the encoded document text.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where the keys are the relative paths of the documents and the values are the encoded document text.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def build_documents(repo_dir, commit, document_encoding_func):\n    \"\"\"\n    Builds a dictionary of documents from a given repository directory and commit.\n\n    Args:\n        repo_dir (str): The path to the repository directory.\n        commit (str): The commit hash to use.\n        document_encoding_func (function): A function that takes a filename and a relative path and returns the encoded document text.\n\n    Returns:\n        dict: A dictionary where the keys are the relative paths of the documents and the values are the encoded document text.\n    \"\"\"\n    documents = dict()\n    with ContextManager(repo_dir, commit):\n        filenames = list_files(repo_dir, include_tests=False)\n        for relative_path in filenames:\n            filename = os.path.join(repo_dir, relative_path)\n            text = document_encoding_func(filename, relative_path)\n            documents[relative_path] = text\n    return documents\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.make_index","title":"make_index","text":"<pre><code>make_index(repo_dir, root_dir, query, commit, document_encoding_func, python, instance_id)\n</code></pre> <p>Builds an index for a given set of documents using Pyserini.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>str</code> <p>The path to the repository directory.</p> required <code>root_dir</code> <code>str</code> <p>The path to the root directory.</p> required <code>query</code> <code>str</code> <p>The query to use for retrieval.</p> required <code>commit</code> <code>str</code> <p>The commit hash to use for retrieval.</p> required <code>document_encoding_func</code> <code>function</code> <p>The function to use for encoding documents.</p> required <code>python</code> <code>str</code> <p>The path to the Python executable.</p> required <code>instance_id</code> <code>int</code> <p>The ID of the current instance.</p> required <p>Returns:</p> Name Type Description <code>index_path</code> <code>Path</code> <p>The path to the built index.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def make_index(\n    repo_dir,\n    root_dir,\n    query,\n    commit,\n    document_encoding_func,\n    python,\n    instance_id,\n):\n    \"\"\"\n    Builds an index for a given set of documents using Pyserini.\n\n    Args:\n        repo_dir (str): The path to the repository directory.\n        root_dir (str): The path to the root directory.\n        query (str): The query to use for retrieval.\n        commit (str): The commit hash to use for retrieval.\n        document_encoding_func (function): The function to use for encoding documents.\n        python (str): The path to the Python executable.\n        instance_id (int): The ID of the current instance.\n\n    Returns:\n        index_path (Path): The path to the built index.\n    \"\"\"\n    index_path = Path(root_dir, f\"index__{str(instance_id)}\", \"index\")\n    if index_path.exists():\n        return index_path\n    thread_prefix = f\"(pid {os.getpid()}) \"\n    documents_path = Path(root_dir, instance_id, \"documents.jsonl\")\n    if not documents_path.parent.exists():\n        documents_path.parent.mkdir(parents=True)\n    documents = build_documents(repo_dir, commit, document_encoding_func)\n    with open(documents_path, \"w\") as docfile:\n        for relative_path, contents in documents.items():\n            print(\n                json.dumps({\"id\": relative_path, \"contents\": contents}),\n                file=docfile,\n                flush=True,\n            )\n    cmd = [\n        python,\n        \"-m\",\n        \"pyserini.index\",\n        \"--collection\",\n        \"JsonCollection\",\n        \"--generator\",\n        \"DefaultLuceneDocumentGenerator\",\n        \"--threads\",\n        \"2\",\n        \"--input\",\n        documents_path.parent.as_posix(),\n        \"--index\",\n        index_path.as_posix(),\n        \"--storePositions\",\n        \"--storeDocvectors\",\n        \"--storeRaw\",\n    ]\n    try:\n        proc = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        output, error = proc.communicate()\n    except KeyboardInterrupt:\n        proc.kill()\n        raise KeyboardInterrupt\n    if proc.returncode == 130:\n        logger.warning(thread_prefix + \"Process killed by user\")\n        raise KeyboardInterrupt\n    if proc.returncode != 0:\n        logger.error(f\"return code: {proc.returncode}\")\n        raise Exception(\n            thread_prefix\n            + f\"Failed to build index for {instance_id} with error {error}\"\n        )\n    return index_path\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.get_remaining_instances","title":"get_remaining_instances","text":"<pre><code>get_remaining_instances(instances, output_file)\n</code></pre> <p>Filters a list of instances to exclude those that have already been processed and saved in a file.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>List[Dict]</code> <p>A list of instances, where each instance is a dictionary with an \"instance_id\" key.</p> required <code>output_file</code> <code>Path</code> <p>The path to the file where the processed instances are saved.</p> required <p>Returns:</p> Type Description <p>List[Dict]: A list of instances that have not been processed yet.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def get_remaining_instances(instances, output_file):\n    \"\"\"\n    Filters a list of instances to exclude those that have already been processed and saved in a file.\n\n    Args:\n        instances (List[Dict]): A list of instances, where each instance is a dictionary with an \"instance_id\" key.\n        output_file (Path): The path to the file where the processed instances are saved.\n\n    Returns:\n        List[Dict]: A list of instances that have not been processed yet.\n    \"\"\"\n    instance_ids = set()\n    remaining_instances = list()\n    if output_file.exists():\n        with FileLock(output_file.as_posix() + \".lock\"):\n            with open(output_file) as f:\n                for line in f:\n                    instance = json.loads(line)\n                    instance_id = instance[\"instance_id\"]\n                    instance_ids.add(instance_id)\n            logger.warning(\n                f\"Found {len(instance_ids)} existing instances in {output_file}. Will skip them.\"\n            )\n    else:\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n        return instances\n    for instance in instances:\n        instance_id = instance[\"instance_id\"]\n        if instance_id not in instance_ids:\n            remaining_instances.append(instance)\n    return remaining_instances\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.search","title":"search","text":"<pre><code>search(instance, index_path)\n</code></pre> <p>Searches for relevant documents in the given index for the given instance.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>dict</code> <p>The instance to search for.</p> required <code>index_path</code> <code>str</code> <p>The path to the index to search in.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the instance ID and a list of hits, where each hit is a dictionary containing the</p> <p>document ID and its score.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def search(instance, index_path):\n    \"\"\"\n    Searches for relevant documents in the given index for the given instance.\n\n    Args:\n        instance (dict): The instance to search for.\n        index_path (str): The path to the index to search in.\n\n    Returns:\n        dict: A dictionary containing the instance ID and a list of hits, where each hit is a dictionary containing the\n        document ID and its score.\n    \"\"\"\n    try:\n        instance_id = instance[\"instance_id\"]\n        searcher = LuceneSearcher(index_path.as_posix())\n        cutoff = len(instance[\"problem_statement\"])\n        while True:\n            try:\n                hits = searcher.search(\n                    instance[\"problem_statement\"][:cutoff],\n                    k=20,\n                    remove_dups=True,\n                )\n            except Exception as e:\n                if \"maxClauseCount\" in str(e):\n                    cutoff = int(round(cutoff * 0.8))\n                    continue\n                else:\n                    raise e\n            break\n        results = {\"instance_id\": instance_id, \"hits\": []}\n        for hit in hits:\n            results[\"hits\"].append({\"docid\": hit.docid, \"score\": hit.score})\n        return results\n    except Exception:\n        logger.error(f\"Failed to process {instance_id}\")\n        logger.error(traceback.format_exc())\n        return None\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.search_indexes","title":"search_indexes","text":"<pre><code>search_indexes(remaining_instance, output_file, all_index_paths)\n</code></pre> <p>Searches the indexes for the given instances and writes the results to the output file.</p> <p>Parameters:</p> Name Type Description Default <code>remaining_instance</code> <code>list</code> <p>A list of instances to search for.</p> required <code>output_file</code> <code>str</code> <p>The path to the output file to write the results to.</p> required <code>all_index_paths</code> <code>dict</code> <p>A dictionary mapping instance IDs to the paths of their indexes.</p> required Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def search_indexes(remaining_instance, output_file, all_index_paths):\n    \"\"\"\n    Searches the indexes for the given instances and writes the results to the output file.\n\n    Args:\n        remaining_instance (list): A list of instances to search for.\n        output_file (str): The path to the output file to write the results to.\n        all_index_paths (dict): A dictionary mapping instance IDs to the paths of their indexes.\n    \"\"\"\n    for instance in tqdm(remaining_instance, desc=\"Retrieving\"):\n        instance_id = instance[\"instance_id\"]\n        if instance_id not in all_index_paths:\n            continue\n        index_path = all_index_paths[instance_id]\n        results = search(instance, index_path)\n        if results is None:\n            continue\n        with FileLock(output_file.as_posix() + \".lock\"):\n            with open(output_file, \"a\") as out_file:\n                print(json.dumps(results), file=out_file, flush=True)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.get_missing_ids","title":"get_missing_ids","text":"<pre><code>get_missing_ids(instances, output_file)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def get_missing_ids(instances, output_file):\n    with open(output_file) as f:\n        written_ids = set()\n        for line in f:\n            instance = json.loads(line)\n            instance_id = instance[\"instance_id\"]\n            written_ids.add(instance_id)\n    missing_ids = set()\n    for instance in instances:\n        instance_id = instance[\"instance_id\"]\n        if instance_id not in written_ids:\n            missing_ids.add(instance_id)\n    return missing_ids\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.get_index_paths_worker","title":"get_index_paths_worker","text":"<pre><code>get_index_paths_worker(instance, root_dir_name, document_encoding_func, python, token)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def get_index_paths_worker(\n    instance,\n    root_dir_name,\n    document_encoding_func,\n    python,\n    token,\n):\n    index_path = None\n    repo = instance[\"repo\"]\n    commit = instance[\"base_commit\"]\n    instance_id = instance[\"instance_id\"]\n    try:\n        repo_dir = clone_repo(repo, root_dir_name, token)\n        query = instance[\"problem_statement\"]\n        index_path = make_index(\n            repo_dir=repo_dir,\n            root_dir=root_dir_name,\n            query=query,\n            commit=commit,\n            document_encoding_func=document_encoding_func,\n            python=python,\n            instance_id=instance_id,\n        )\n    except:\n        logger.error(f\"Failed to process {repo}/{commit} (instance {instance_id})\")\n        logger.error(traceback.format_exc())\n    return instance_id, index_path\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.get_index_paths","title":"get_index_paths","text":"<pre><code>get_index_paths(remaining_instances: list[dict[str, Any]], root_dir_name: str, document_encoding_func: Any, python: str, token: str, output_file: str) -&gt; dict[str, str]\n</code></pre> <p>Retrieves the index paths for the given instances using multiple processes.</p> <p>Parameters:</p> Name Type Description Default <code>remaining_instances</code> <code>list[dict[str, Any]]</code> <p>A list of instances for which to retrieve the index paths.</p> required <code>root_dir_name</code> <code>str</code> <p>The root directory name.</p> required <code>document_encoding_func</code> <code>Any</code> <p>A function for encoding documents.</p> required <code>python</code> <code>str</code> <p>The path to the Python executable.</p> required <code>token</code> <code>str</code> <p>The token to use for authentication.</p> required <code>output_file</code> <code>str</code> <p>The output file.</p> required <code>num_workers</code> <p>The number of worker processes to use.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dictionary mapping instance IDs to index paths.</p> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def get_index_paths(\n    remaining_instances: list[dict[str, Any]],\n    root_dir_name: str,\n    document_encoding_func: Any,\n    python: str,\n    token: str,\n    output_file: str,\n) -&gt; dict[str, str]:\n    \"\"\"\n    Retrieves the index paths for the given instances using multiple processes.\n\n    Args:\n        remaining_instances: A list of instances for which to retrieve the index paths.\n        root_dir_name: The root directory name.\n        document_encoding_func: A function for encoding documents.\n        python: The path to the Python executable.\n        token: The token to use for authentication.\n        output_file: The output file.\n        num_workers: The number of worker processes to use.\n\n    Returns:\n        A dictionary mapping instance IDs to index paths.\n    \"\"\"\n    all_index_paths = dict()\n    for instance in tqdm(remaining_instances, desc=\"Indexing\"):\n        instance_id, index_path = get_index_paths_worker(\n            instance=instance,\n            root_dir_name=root_dir_name,\n            document_encoding_func=document_encoding_func,\n            python=python,\n            token=token,\n        )\n        if index_path is None:\n            continue\n        all_index_paths[instance_id] = index_path\n    return all_index_paths\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.get_root_dir","title":"get_root_dir","text":"<pre><code>get_root_dir(dataset_name, output_dir, document_encoding_style)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def get_root_dir(dataset_name, output_dir, document_encoding_style):\n    root_dir = Path(output_dir, dataset_name, document_encoding_style + \"_indexes\")\n    if not root_dir.exists():\n        root_dir.mkdir(parents=True, exist_ok=True)\n    root_dir_name = root_dir\n    return root_dir, root_dir_name\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.bm25_retrieval.main","title":"main","text":"<pre><code>main(dataset_name_or_path, document_encoding_style, output_dir, shard_id, num_shards, splits, leave_indexes)\n</code></pre> Source code in <code>swebench/inference/make_datasets/bm25_retrieval.py</code> <pre><code>def main(\n    dataset_name_or_path,\n    document_encoding_style,\n    output_dir,\n    shard_id,\n    num_shards,\n    splits,\n    leave_indexes,\n):\n    document_encoding_func = DOCUMENT_ENCODING_FUNCTIONS[document_encoding_style]\n    token = os.environ.get(\"GITHUB_TOKEN\", \"git\")\n    if Path(dataset_name_or_path).exists():\n        dataset = load_from_disk(dataset_name_or_path)\n        dataset_name = os.path.basename(dataset_name_or_path)\n    else:\n        dataset = load_dataset(dataset_name_or_path)\n        dataset_name = dataset_name_or_path.replace(\"/\", \"__\")\n    if shard_id is not None:\n        for split in splits:\n            dataset[split] = dataset[split].shard(num_shards, shard_id)\n    instances = list()\n    if set(splits) - set(dataset.keys()) != set():\n        raise ValueError(f\"Unknown splits {set(splits) - set(dataset.keys())}\")\n    for split in splits:\n        instances += list(dataset[split])\n    python = subprocess.run(\"which python\", shell=True, capture_output=True)\n    python = python.stdout.decode(\"utf-8\").strip()\n    output_file = Path(\n        output_dir, dataset_name, document_encoding_style + \".retrieval.jsonl\"\n    )\n    remaining_instances = get_remaining_instances(instances, output_file)\n    root_dir, root_dir_name = get_root_dir(\n        dataset_name, output_dir, document_encoding_style\n    )\n    try:\n        all_index_paths = get_index_paths(\n            remaining_instances,\n            root_dir_name,\n            document_encoding_func,\n            python,\n            token,\n            output_file,\n        )\n    except KeyboardInterrupt:\n        logger.info(f\"Cleaning up {root_dir}\")\n        del_dirs = list(root_dir.glob(\"repo__*\"))\n        if leave_indexes:\n            index_dirs = list(root_dir.glob(\"index__*\"))\n            del_dirs += index_dirs\n        for dirname in del_dirs:\n            shutil.rmtree(dirname, ignore_errors=True)\n    logger.info(f\"Finished indexing {len(all_index_paths)} instances\")\n    search_indexes(remaining_instances, output_file, all_index_paths)\n    missing_ids = get_missing_ids(instances, output_file)\n    logger.warning(f\"Missing indexes for {len(missing_ids)} instances.\")\n    logger.info(f\"Saved retrieval results to {output_file}\")\n    del_dirs = list(root_dir.glob(\"repo__*\"))\n    logger.info(f\"Cleaning up {root_dir}\")\n    if leave_indexes:\n        index_dirs = list(root_dir.glob(\"index__*\"))\n        del_dirs += index_dirs\n    for dirname in del_dirs:\n        shutil.rmtree(dirname, ignore_errors=True)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance","title":"create_instance","text":""},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.PATCH_EXAMPLE","title":"PATCH_EXAMPLE  <code>module-attribute</code>","text":"<pre><code>PATCH_EXAMPLE = '--- a/file.py\\n+++ b/file.py\\n@@ -1,27 +1,35 @@\\n def euclidean(a, b):\\n-    while b:\\n-        a, b = b, a % b\\n-    return a\\n+    if b == 0:\\n+        return a\\n+    return euclidean(b, a % b)\\n \\n \\n def bresenham(x0, y0, x1, y1):\\n     points = []\\n     dx = abs(x1 - x0)\\n     dy = abs(y1 - y0)\\n-    sx = 1 if x0 &lt; x1 else -1\\n-    sy = 1 if y0 &lt; y1 else -1\\n-    err = dx - dy\\n+    x, y = x0, y0\\n+    sx = -1 if x0 &gt; x1 else 1\\n+    sy = -1 if y0 &gt; y1 else 1\\n \\n-    while True:\\n-        points.append((x0, y0))\\n-        if x0 == x1 and y0 == y1:\\n-            break\\n-        e2 = 2 * err\\n-        if e2 &gt; -dy:\\n+    if dx &gt; dy:\\n+        err = dx / 2.0\\n+        while x != x1:\\n+            points.append((x, y))\\n             err -= dy\\n-            x0 += sx\\n-        if e2 &lt; dx:\\n-            err += dx\\n-            y0 += sy\\n+            if err &lt; 0:\\n+                y += sy\\n+                err += dx\\n+            x += sx\\n+    else:\\n+        err = dy / 2.0\\n+        while y != y1:\\n+            points.append((x, y))\\n+            err -= dx\\n+            if err &lt; 0:\\n+                x += sx\\n+                err += dy\\n+            y += sy\\n \\n+    points.append((x, y))\\n     return points'\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.FULL_GENERATION_EXAMPLE","title":"FULL_GENERATION_EXAMPLE  <code>module-attribute</code>","text":"<pre><code>FULL_GENERATION_EXAMPLE = '[start of /src/this_file.py]\\nimport os\\n\\ndef euclidean(a, b):\\n    if b == 0:\\n        return a\\n    return euclidean(b, a % b)\\n[end of /src/this_file.py]\\n[start of /src/another_file.py]\\ndef bresenham(x0, y0, x1, y1):\\n    points = []\\n    dx = abs(x1 - x0)\\n    dy = abs(y1 - y0)\\n    x, y = x0, y0\\n    sx = -1 if x0 &gt; x1 else 1\\n    sy = -1 if y0 &gt; y1 else 1\\n    if dx &gt; dy:\\n        err = dx / 2.0\\n        while x != x1:\\n            points.append((x, y))\\n            err -= dy\\n            if err &lt; 0:\\n                y += sy\\n                err += dx\\n            x += sx\\n    else:\\n        err = dy / 2.0\\n        while y != y1:\\n            points.append((x\\n            err -= dx\\n            if err &lt; 0:\\n                x += sx\\n                err += dy\\n            y += sy\\n    points.append((x, y))\\n    return points\\n[end of /src/another_file.py]'\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.PROMPT_FUNCTIONS","title":"PROMPT_FUNCTIONS  <code>module-attribute</code>","text":"<pre><code>PROMPT_FUNCTIONS = {'style-2': prompt_style_2, 'style-3': prompt_style_3, 'full_file_gen': full_file_gen, 'style-2-edits-only': prompt_style_2_edits_only}\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.add_lines_list","title":"add_lines_list","text":"<pre><code>add_lines_list(content)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def add_lines_list(content):\n    content_with_lines = list()\n    for ix, line in enumerate(content.split(\"\\n\"), start=1):\n        content_with_lines.append(f\"{ix} {line}\")\n    return content_with_lines\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.add_lines","title":"add_lines","text":"<pre><code>add_lines(content)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def add_lines(content):\n    return \"\\n\".join(add_lines_list(content))\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.make_code_text","title":"make_code_text","text":"<pre><code>make_code_text(files_dict, add_line_numbers=True)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def make_code_text(files_dict, add_line_numbers=True):\n    all_text = \"\"\n    for filename, contents in sorted(files_dict.items()):\n        all_text += f\"[start of {filename}]\\n\"\n        if add_line_numbers:\n            all_text += add_lines(contents)\n        else:\n            all_text += contents\n        all_text += f\"\\n[end of {filename}]\\n\"\n    return all_text.strip(\"\\n\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.make_code_text_edits_only","title":"make_code_text_edits_only","text":"<pre><code>make_code_text_edits_only(files_dict, patch, add_line_numbers=True)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def make_code_text_edits_only(files_dict, patch, add_line_numbers=True):\n    files = dict()\n    patch = unidiff.PatchSet(patch)\n    for patched_file in patch:\n        source_file = patched_file.source_file.split(\"a/\", 1)[-1]\n        files[source_file] = list()\n        for hunk in patched_file:\n            start = hunk.source_start - 15\n            end = start + hunk.source_length + 15\n            files[source_file].append((start, end))\n    all_text = \"\"\n    for filename, content in files_dict.items():\n        all_text += f\"[start of {filename}]\\n\"\n        content_with_lines = add_lines_list(content)\n        for start, end in files[filename]:\n            if start &gt; 0:\n                all_text += \"...\\n\"\n            all_text += \"\\n\".join(content_with_lines[start:end])\n            all_text += \"\\n\"\n            if end &lt; len(content_with_lines):\n                all_text += \"...\\n\"\n        all_text = all_text.strip(\"\\n\")\n        all_text += f\"\\n[end of {filename}]\\n\"\n    return all_text.strip(\"\\n\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.prompt_style_2","title":"prompt_style_2","text":"<pre><code>prompt_style_2(instance)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def prompt_style_2(instance):\n    premise = \"You will be provided with a partial code base and an issue statement explaining a problem to resolve.\"\n    readmes_text = make_code_text(instance[\"readmes\"])\n    code_text = make_code_text(instance[\"file_contents\"])\n    instructions = (\n        \"I need you to solve this issue by generating a single patch file that I can apply \"\n        + \"directly to this repository using git apply. Please respond with a single patch \"\n        + \"file in the following format.\"\n    )\n    problem_statement = instance[\"problem_statement\"]\n    final_text = [\n        premise,\n        \"&lt;issue&gt;\",\n        problem_statement,\n        \"&lt;/issue&gt;\",\n        \"&lt;code&gt;\",\n        readmes_text,\n        code_text,\n        \"&lt;/code&gt;\",\n        instructions,\n        \"&lt;patch&gt;\",\n        PATCH_EXAMPLE,\n        \"&lt;/patch&gt;\",\n    ]\n    final_text = \"\\n\".join(final_text)\n    return final_text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.prompt_style_2_edits_only","title":"prompt_style_2_edits_only","text":"<pre><code>prompt_style_2_edits_only(instance)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def prompt_style_2_edits_only(instance):\n    premise = \"You will be provided with a partial code base and an issue statement explaining a problem to resolve.\"\n    readmes_text = make_code_text(instance[\"readmes\"])\n    code_text = make_code_text_edits_only(instance[\"file_contents\"], instance[\"patch\"])\n    instructions = (\n        \"I need you to solve this issue by generating a single patch file that I can apply \"\n        + \"directly to this repository using git apply. Please respond with a single patch \"\n        + \"file in the following format.\"\n    )\n    problem_statement = instance[\"problem_statement\"]\n    final_text = [\n        premise,\n        \"&lt;issue&gt;\",\n        problem_statement,\n        \"&lt;/issue&gt;\",\n        \"&lt;code&gt;\",\n        readmes_text,\n        code_text,\n        \"&lt;/code&gt;\",\n        instructions,\n        \"&lt;patch&gt;\",\n        PATCH_EXAMPLE,\n        \"&lt;/patch&gt;\",\n    ]\n    final_text = \"\\n\".join(final_text)\n    return final_text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.prompt_style_3","title":"prompt_style_3","text":"<pre><code>prompt_style_3(instance)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def prompt_style_3(instance):\n    premise = \"You will be provided with a partial code base and an issue statement explaining a problem to resolve.\"\n    readmes_text = make_code_text(instance[\"readmes\"])\n    code_text = make_code_text(instance[\"file_contents\"])\n    example_explanation = (\n        \"Here is an example of a patch file. It consists of changes to the code base. \"\n        + \"It specifies the file names, the line numbers of each change, and the removed and added lines. \"\n        + \"A single patch file can contain changes to multiple files.\"\n    )\n    final_instruction = (\n        \"I need you to solve the provided issue by generating a single patch file that I can apply \"\n        + \"directly to this repository using git apply. Please respond with a single patch \"\n        + \"file in the format shown above.\"\n    )\n    problem_statement = instance[\"problem_statement\"]\n    final_text = [\n        premise,\n        \"&lt;issue&gt;\",\n        problem_statement,\n        \"&lt;/issue&gt;\",\n        \"\",\n        \"&lt;code&gt;\",\n        readmes_text,\n        code_text,\n        \"&lt;/code&gt;\",\n        \"\",\n        example_explanation,\n        \"&lt;patch&gt;\",\n        PATCH_EXAMPLE,\n        \"&lt;/patch&gt;\",\n        \"\",\n        final_instruction,\n        \"Respond below:\",\n    ]\n    final_text = \"\\n\".join(final_text)\n    return final_text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.full_file_gen","title":"full_file_gen","text":"<pre><code>full_file_gen(instance)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def full_file_gen(instance):\n    premise = \"You will be provided with a partial code base and an issue statement explaining a problem to resolve.\"\n    readmes_text = make_code_text(instance[\"readmes\"], add_line_numbers=False)\n    code_text = make_code_text(instance[\"file_contents\"], add_line_numbers=False)\n    instructions = (\n        \"I need you to solve this issue by regenerating the full files in the code base that you would like to change. \"\n        + \"You can change as many files as you like. \"\n        + \"Please respond with a list of files and their revised contents in the following format.\"\n    )\n    problem_statement = instance[\"problem_statement\"]\n    final_text = [\n        premise,\n        \"&lt;issue&gt;\",\n        problem_statement,\n        \"&lt;/issue&gt;\",\n        \"&lt;code&gt;\",\n        readmes_text,\n        code_text,\n        \"&lt;/code&gt;\",\n        instructions,\n        \"&lt;example&gt;\",\n        FULL_GENERATION_EXAMPLE,\n        \"&lt;/example&gt;\",\n    ]\n    final_text = \"\\n\".join(final_text)\n    return final_text\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.ingest_files","title":"ingest_files","text":"<pre><code>ingest_files(filenames)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def ingest_files(filenames):\n    files_dict = dict()\n    for filename in filenames:\n        with open(filename) as f:\n            content = f.read()\n        files_dict[filename] = content\n    return files_dict\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.add_retrieval_results","title":"add_retrieval_results","text":"<pre><code>add_retrieval_results(input_instances, retrieval_file, k, file_source)\n</code></pre> <p>Adds retrieval results to input_instances in-place</p> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def add_retrieval_results(input_instances, retrieval_file, k, file_source):\n    \"\"\"\n    Adds retrieval results to input_instances in-place\n    \"\"\"\n    retrieval_results_path = Path(retrieval_file)\n    assert retrieval_results_path.exists(), (\n        f\"Retrieval results not found at {retrieval_results_path}\"\n    )\n    retrieval_results = [json.loads(line) for line in open(retrieval_results_path)]\n    retrieval_results = {x[\"instance_id\"]: x[\"hits\"] for x in retrieval_results}\n    for instance_id, instance in tqdm(\n        input_instances.items(),\n        total=len(input_instances),\n        desc=\"Adding retrieval results\",\n    ):\n        try:\n            instance[\"hits\"] = retrieval_results[instance_id][:k]\n        except KeyError:\n            logger.warning(f\"Instance {instance_id} not found in retrieval results\")\n            instance[\"hits\"] = list()\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.get_oracle_filenames","title":"get_oracle_filenames","text":"<pre><code>get_oracle_filenames(instance)\n</code></pre> <p>Returns the filenames that are changed in the patch</p> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def get_oracle_filenames(instance):\n    \"\"\"\n    Returns the filenames that are changed in the patch\n    \"\"\"\n    source_files = {\n        patch_file.source_file.split(\"a/\", 1)[-1]\n        for patch_file in unidiff.PatchSet(instance[\"patch\"])\n    }\n    gold_docs = set()\n    for source_file in source_files:\n        gold_docs.add(source_file)\n    return gold_docs\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_instance.add_text_inputs","title":"add_text_inputs","text":"<pre><code>add_text_inputs(instances, retrieval_file, k, prompt_style, file_source, max_context_len=None, tokenizer_name=None, verbose=False, progress_file=None) -&gt; None\n</code></pre> <p>Process instances and save results to progress file.</p> <p>Args: - instances: dictionary with unprocessed input instances - retrieval_file: if using retrieval method for file_contents, specify retrieval_file - k: if using retrieval, specifies the maximum number of files to include - prompt_style: specify the function to generate instructions and prompt - file_source: where to collect file_contents (e.g. oracle or bm25) - verbose: set ContextManager verbose to True - progress_file: required, path to save processed instances</p> Source code in <code>swebench/inference/make_datasets/create_instance.py</code> <pre><code>def add_text_inputs(\n    instances,\n    retrieval_file,\n    k,\n    prompt_style,\n    file_source,\n    max_context_len=None,\n    tokenizer_name=None,\n    verbose=False,\n    progress_file=None,\n) -&gt; None:\n    \"\"\"Process instances and save results to progress file.\n\n    Args:\n    - instances: dictionary with unprocessed input instances\n    - retrieval_file: if using retrieval method for file_contents, specify retrieval_file\n    - k: if using retrieval, specifies the maximum number of files to include\n    - prompt_style: specify the function to generate instructions and prompt\n    - file_source: where to collect file_contents (e.g. oracle or bm25)\n    - verbose: set ContextManager verbose to True\n    - progress_file: required, path to save processed instances\n    \"\"\"\n    assert progress_file is not None, \"progress_file is required\"\n\n    # Create progress file directory if it doesn't exist\n    progress_path = Path(progress_file)\n    progress_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Load already processed instances\n    processed_ids = set()\n    file_exists = os.path.exists(progress_file)\n\n    if file_exists:\n        with open(progress_file) as f:\n            for line in f:\n                instance = json.loads(line)\n                processed_ids.add(instance[\"instance_id\"])\n        logger.info(f\"Found {len(processed_ids)} already processed instances\")\n        progress_file_handle = open(progress_file, \"a\")\n    else:\n        progress_file_handle = open(progress_file, \"w\")\n\n    try:\n        if max_context_len is not None:\n            assert tokenizer_name is not None, (\n                \"Must specify tokenizer_name if using max_context_len\"\n            )\n            tokenizer, tokenizer_func = TOKENIZER_FUNCS[tokenizer_name]\n\n        # Add retrieval results if needed\n        if file_source in {\"bm25\"}:\n            instances = deepcopy(instances)\n            add_retrieval_results(instances, retrieval_file, k, file_source)\n\n        # Filter out already processed instances\n        instances_to_process = {\n            k: v for k, v in instances.items() if k not in processed_ids\n        }\n        logger.info(f\"Processing {len(instances_to_process)} instances\")\n\n        orig_dir = os.getcwd()\n        with TemporaryDirectory(\n            dir=\"/scratch\" if os.path.exists(\"/scratch\") else \"/tmp\"\n        ) as root_dir:\n            for instance_id, instance in tqdm(\n                instances_to_process.items(),\n                total=len(instances_to_process),\n                desc=\"Processing instances\",\n            ):\n                try:\n                    with AutoContextManager(instance, root_dir, verbose=verbose) as cm:\n                        # Process instance\n                        processed_instance = deepcopy(instance)\n\n                        # Add readmes\n                        readmes = cm.get_readme_files()\n                        processed_instance[\"readmes\"] = ingest_files(readmes)\n\n                        # Handle file contents based on configuration\n                        if max_context_len is not None:\n                            processed_instance[\"file_contents\"] = dict()\n                            base_text_inputs = PROMPT_FUNCTIONS[prompt_style](\n                                processed_instance\n                            )\n                            base_text_input_length = len(\n                                tokenizer_func(base_text_inputs, tokenizer)\n                            )\n\n                        if file_source == \"oracle\":\n                            processed_instance[\"file_contents\"] = ingest_files(\n                                get_oracle_filenames(processed_instance)\n                            )\n                        elif file_source == \"bm25\":\n                            processed_instance[\"file_contents\"] = ingest_files(\n                                [x[\"docid\"] for x in processed_instance[\"hits\"]]\n                            )\n                        elif file_source == \"all\":\n                            processed_instance[\"file_contents\"] = (\n                                ingest_directory_contents(cm.repo_path)\n                            )\n                        elif file_source == \"none\":\n                            processed_instance[\"file_contents\"] = dict()\n                        else:\n                            raise ValueError(f\"Invalid file source {file_source}\")\n\n                        # Handle context length limits\n                        if max_context_len is not None:\n                            cur_input_len = base_text_input_length\n                            include_files = []\n                            for filename in [\n                                x[\"docid\"] for x in processed_instance[\"hits\"]\n                            ]:\n                                content = make_code_text(\n                                    {\n                                        filename: processed_instance[\"file_contents\"][\n                                            filename\n                                        ]\n                                    }\n                                )\n                                if tokenizer_name == \"llama\":\n                                    tokens = tokenizer_func(\"\\n\" + content, tokenizer)\n                                    idx = tokens.index(13)\n                                    tokens = tokens[idx + 1 :]\n                                else:\n                                    tokens = tokenizer_func(content, tokenizer)\n                                if cur_input_len + len(tokens) &lt; max_context_len:\n                                    include_files.append(filename)\n                                    cur_input_len += len(tokens)\n                            processed_instance[\"file_contents\"] = {\n                                filename: processed_instance[\"file_contents\"][filename]\n                                for filename in include_files\n                            }\n\n                        # Generate final text inputs\n                        processed_instance[\"text_inputs\"] = PROMPT_FUNCTIONS[\n                            prompt_style\n                        ](processed_instance)\n\n                        # Save to progress file\n                        progress_file_handle.write(\n                            json.dumps(processed_instance) + \"\\n\"\n                        )\n                        progress_file_handle.flush()\n\n                except Exception as e:\n                    print(f\"Failed on instance {instance_id}\", e)\n                    traceback.print_exc()\n                    # Save failed instance\n                    failed_instance = {**instance, \"text_inputs\": None}\n                    progress_file_handle.write(json.dumps(failed_instance) + \"\\n\")\n                    progress_file_handle.flush()\n                finally:\n                    os.chdir(orig_dir)\n        os.chdir(orig_dir)\n    finally:\n        progress_file_handle.close()\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset","title":"create_text_dataset","text":"<p>Create a dataset for text-to-text training from the raw task instance outputs.</p>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description=__doc__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.load_jsonl_file","title":"load_jsonl_file","text":"<pre><code>load_jsonl_file(filename)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def load_jsonl_file(filename):\n    if type(filename) == str:\n        filename = Path(filename)\n    if filename.name.endswith(\".jsonl\") or filename.name.endswith(\".jsonl.all\"):\n        with open(filename) as f:\n            return [json.loads(line) for line in f]\n    elif filename.name.endswith(\".json\"):\n        with open(filename) as f:\n            return json.load(f)\n    else:\n        raise ValueError(f\"Unknown file type {filename}\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.instances_generator","title":"instances_generator","text":"<pre><code>instances_generator(files)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def instances_generator(files):\n    all_data = list()\n    for file in tqdm(files, desc=\"Loading instance files\"):\n        all_data.extend(load_jsonl_file(file))\n    return all_data\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.get_training_and_eval_instances","title":"get_training_and_eval_instances","text":"<pre><code>get_training_and_eval_instances(raw_files, test_dataset)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def get_training_and_eval_instances(raw_files, test_dataset):\n    logger.info(\"Loading instances\")\n    raw_instances = list(instances_generator(raw_files))\n    final_instances = list(test_dataset[\"test\"])\n    eval_repos = {x[\"repo\"] for x in final_instances}\n    train_instances = [x for x in raw_instances if x[\"repo\"] not in eval_repos]\n    train_instances = list(sorted(train_instances, key=lambda x: x[\"instance_id\"]))\n    eval_instances = list(sorted(final_instances, key=lambda x: x[\"instance_id\"]))\n    logger.info(f\"Found {len(train_instances)} training ids\")\n    logger.info(f\"Found {len(eval_instances)} eval ids\")\n    return train_instances, eval_instances\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.extract_fields","title":"extract_fields","text":"<pre><code>extract_fields(instance)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def extract_fields(instance):\n    instance_id = instance[\"instance_id\"]\n    if instance[\"text_inputs\"] is None or instance[\"patch\"] is None:\n        logger.warning(f\"No text for {instance_id}\")\n        return None\n    text_inputs = instance[\"text_inputs\"].strip() + \"\\n\\n\"\n    if text_inputs is None or instance[\"patch\"] is None:\n        logger.warning(f\"No inputs for {instance_id}\")\n        return None\n    patch = \"\\n\".join([\"&lt;patch&gt;\", instance[\"patch\"], \"&lt;/patch&gt;\"])\n    return {**instance, \"text\": text_inputs, \"patch\": patch}\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.validate_arguments","title":"validate_arguments","text":"<pre><code>validate_arguments(push_to_hub_user, output_dir, max_context_len, tokenizer_name, file_source, k)\n</code></pre> <p>Validate command line arguments and environment setup.</p> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def validate_arguments(\n    push_to_hub_user, output_dir, max_context_len, tokenizer_name, file_source, k\n):\n    \"\"\"Validate command line arguments and environment setup.\"\"\"\n    if push_to_hub_user is not None:\n        hub_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\", None)\n        assert hub_token is not None, (\n            \"Must provide HUGGING_FACE_HUB_TOKEN to push to the Hub\"\n        )\n        assert output_dir is None, \"Cannot provide output_dir if pushing to the Hub\"\n    if max_context_len is not None:\n        assert tokenizer_name is not None\n    if push_to_hub_user is None and not Path(output_dir).exists():\n        Path(output_dir).mkdir(parents=True)\n    if max_context_len is not None:\n        assert file_source not in {\"all\", \"oracle\"}, (\n            \"Cannot use max_context_len with oracle or all file sources\"\n        )\n        assert tokenizer_name is not None, (\n            \"Must provide tokenizer_name if max_context_len is not None\"\n        )\n    if k is not None:\n        assert file_source not in {\"all\", \"oracle\"}, (\n            \"Cannot use max_context_len with oracle or all file sources\"\n        )\n    return hub_token if push_to_hub_user is not None else None\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.construct_output_filename","title":"construct_output_filename","text":"<pre><code>construct_output_filename(dataset_name, prompt_style, file_source, k, max_context_len, tokenizer_name)\n</code></pre> <p>Construct the output filename based on parameters.</p> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def construct_output_filename(\n    dataset_name, prompt_style, file_source, k, max_context_len, tokenizer_name\n):\n    \"\"\"Construct the output filename based on parameters.\"\"\"\n    if dataset_name.startswith(\"princeton-nlp\"):\n        dataset_name = dataset_name.split(\"/\")[-1]\n    dataset_name = dataset_name.replace(\"/\", \"__\")\n    output_file = f\"{dataset_name}__{prompt_style}__fs-{file_source}\"\n    if k is not None:\n        output_file += f\"__k-{k}\"\n    if max_context_len is not None:\n        output_file += f\"__mcc-{max_context_len}-{tokenizer_name}\"\n    return output_file\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.create_text_dataset.main","title":"main","text":"<pre><code>main(dataset_name_or_path, splits, validation_ratio, output_dir, retrieval_file, prompt_style, file_source, k, max_context_len, tokenizer_name, push_to_hub_user)\n</code></pre> Source code in <code>swebench/inference/make_datasets/create_text_dataset.py</code> <pre><code>def main(\n    dataset_name_or_path,\n    splits,\n    validation_ratio,\n    output_dir,\n    retrieval_file,\n    prompt_style,\n    file_source,\n    k,\n    max_context_len,\n    tokenizer_name,\n    push_to_hub_user,\n):\n    # Validate arguments and setup\n    hub_token = validate_arguments(\n        push_to_hub_user, output_dir, max_context_len, tokenizer_name, file_source, k\n    )\n    output_file = construct_output_filename(\n        dataset_name_or_path,\n        prompt_style,\n        file_source,\n        k,\n        max_context_len,\n        tokenizer_name,\n    )\n    output_file = Path(output_dir, output_file)\n    if push_to_hub_user is None:\n        if output_file.exists():\n            existing_dataset = load_from_disk(output_file)\n            # if requested splits are in existing dataset, abort\n            for split in splits:\n                if split in existing_dataset:\n                    logger.info(\n                        f\"{output_file.absolute().as_posix()} already exists for split {split}. Aborting\"\n                    )\n                    return\n            del existing_dataset  # don't store in memory\n\n    # Load dataset\n    dataset = (\n        load_from_disk(dataset_name_or_path)\n        if Path(dataset_name_or_path).exists()\n        else load_dataset(dataset_name_or_path)\n    )\n    logger.info(f\"Found {set(dataset.keys())} splits\")\n    if set(splits) - set(dataset.keys()) != set():\n        raise ValueError(f\"Unknown splits {set(splits) - set(dataset.keys())}\")\n\n    # Define columns for final dataset\n    columns = [\n        \"instance_id\",\n        \"text\",\n        \"repo\",\n        \"base_commit\",\n        \"problem_statement\",\n        \"hints_text\",\n        \"created_at\",\n        \"patch\",\n        \"test_patch\",\n        \"version\",\n        \"FAIL_TO_PASS\",\n        \"PASS_TO_PASS\",\n        \"environment_setup_commit\",\n    ]\n\n    # Process each split\n    split_data = {}\n    progress_files = {}\n    for split in splits:\n        logger.info(f\"Processing {split} split\")\n        split_instances = {x[\"instance_id\"]: x for x in dataset[split]}\n        progress_file = f\"{output_file}.{split}.progress.jsonl\"\n        progress_files[split] = progress_file\n        # Process instances and save to progress file\n        add_text_inputs(\n            split_instances,\n            retrieval_file=retrieval_file,\n            k=k,\n            prompt_style=prompt_style,\n            file_source=file_source,\n            max_context_len=max_context_len,\n            tokenizer_name=tokenizer_name,\n            progress_file=progress_file,\n        )\n\n    logger.info(\"Creating final dataset\")\n    # Create final dataset\n    if output_file.exists():\n        final_dataset = load_from_disk(output_file)\n    else:\n        final_dataset = DatasetDict()\n    for split in splits:\n        split_data = {key: [] for key in columns}\n        valid_instance_ids = set(dataset[split][\"instance_id\"])\n        invalid_instances = []\n\n        with open(progress_files[split]) as f:\n            for line in f:\n                datum = extract_fields(json.loads(line))\n                if not datum:\n                    continue\n                if datum[\"instance_id\"] not in valid_instance_ids:\n                    invalid_instances.append(datum[\"instance_id\"])\n                    continue\n                for key in columns:\n                    split_data[key].append(datum.get(key, \"\"))\n\n        if invalid_instances:\n            logger.warning(\n                f\"Found {len(invalid_instances)} instances in progress file that are not in the {split} dataset: {invalid_instances}. These will be removed from the final dataset.\"\n            )\n\n        final_dataset[split] = Dataset.from_dict(split_data)\n\n    # Handle validation split\n    if validation_ratio &gt; 0 and \"train\" in final_dataset:\n        train_val = final_dataset[\"train\"].train_test_split(\n            test_size=validation_ratio, seed=42\n        )\n        final_dataset[\"train\"] = train_val[\"train\"]\n        final_dataset[\"validation\"] = train_val[\"test\"]\n\n    # Log final dataset sizes\n    for split in final_dataset:\n        logger.info(f\"Found {len(final_dataset[split])} {split} instances\")\n\n    # Save dataset\n    if push_to_hub_user is not None:\n        final_dataset.push_to_hub(\n            f\"{push_to_hub_user}/{output_file.name}\", use_auth_token=hub_token\n        )\n    else:\n        final_dataset.save_to_disk(output_file)\n\n    # Cleanup progress files\n    for progress_file in progress_files.values():\n        if os.path.exists(progress_file):\n            os.remove(progress_file)\n\n    logger.info(f\"Finished saving to {output_file}\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.eval_retrieval","title":"eval_retrieval","text":"<p>This script can be used to evaluate the BM25 retrieval results for a dataset created with create_text_dataset.py with the --retrieval_file option and --file_source bm25.</p>"},{"location":"api/inference/#swebench.inference.make_datasets.eval_retrieval.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.eval_retrieval.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description=__doc__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.eval_retrieval.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.eval_retrieval.main","title":"main","text":"<pre><code>main(dataset_name_or_path, split)\n</code></pre> Source code in <code>swebench/inference/make_datasets/eval_retrieval.py</code> <pre><code>def main(dataset_name_or_path, split):\n    try:\n        dataset = load_from_disk(dataset_name_or_path)[split]\n    except:\n        dataset = load_dataset(dataset_name_or_path, split=split)\n    print(\n        f\"Evaluating {len(dataset)} instances from {dataset_name_or_path} {split} split\"\n    )\n    instance_files_pattern = re.compile(\n        r\"\\[start of ([\\w\\.\\-\\/]+)\\]\\n(?:.+?)\\n\\[end of \\1\\]\", re.DOTALL\n    )\n    patch_files_pattern = re.compile(r\"\\-\\-\\- a/(.+)\")\n    patch_files = {instance[\"instance_id\"]: instance[\"patch\"] for instance in dataset}\n    recalls_any = list()\n    recalls_all = list()\n    recalls = list()\n    for datum in dataset:\n        instance_id = datum[\"instance_id\"]\n        retrieved_files = instance_files_pattern.findall(datum[\"text\"])\n        if retrieved_files and \"readme\" in retrieved_files[0].lower():\n            retrieved_files = retrieved_files[1:]\n        retrieved_files = set(retrieved_files)\n        gold_files = set(patch_files_pattern.findall(patch_files[instance_id]))\n        if len(gold_files) == 0:\n            print(f\"WARNING: Instance {datum['instance_id']} has no gold files\")\n            continue\n        if len(retrieved_files) == 0:\n            print(f\"WARNING: Instance {datum['instance_id']} has no retrieved files\")\n            recall = 0.0\n        else:\n            recall = len(retrieved_files.intersection(gold_files)) / len(gold_files)\n        recalls.append(recall)\n        recalls_any.append(int(recall &gt; 0))\n        recalls_all.append(int(recall == 1))\n    recalls = np.array(recalls)\n    recalls_any = np.array(recalls_any)\n    recalls_all = np.array(recalls_all)\n    print(f\"Avg Recall: {np.mean(recalls) * 100:.2f}\")\n    print(f\"All Recall: {np.mean(recalls_all) * 100:.2f}\")\n    print(f\"Any Recall: {np.mean(recalls_any) * 100:.2f}\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset","title":"tokenize_dataset","text":"<p>Provided a source (raw) directory and the final (eval) directory, create a training split by removing all instances that are in the final directory from the source directory.</p>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.TOKENIZER_FUNCS","title":"TOKENIZER_FUNCS  <code>module-attribute</code>","text":"<pre><code>TOKENIZER_FUNCS = {'cl100k': (get_encoding('cl100k_base'), cl100k), 'llama': (from_pretrained('togethercomputer/LLaMA-2-7B-32K'), llama)}\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description=__doc__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.cl100k","title":"cl100k","text":"<pre><code>cl100k(text, tokenizer)\n</code></pre> Source code in <code>swebench/inference/make_datasets/tokenize_dataset.py</code> <pre><code>def cl100k(text, tokenizer):\n    return tokenizer.encode(text, disallowed_special=())\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.llama","title":"llama","text":"<pre><code>llama(text, tokenizer)\n</code></pre> Source code in <code>swebench/inference/make_datasets/tokenize_dataset.py</code> <pre><code>def llama(text, tokenizer):\n    return tokenizer(text, add_special_tokens=False, return_attention_mask=False)[\n        \"input_ids\"\n    ]\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.extract_fields","title":"extract_fields","text":"<pre><code>extract_fields(instance, tokenizer_name, tokenizer, tokenizer_func, eos_token)\n</code></pre> Source code in <code>swebench/inference/make_datasets/tokenize_dataset.py</code> <pre><code>def extract_fields(instance, tokenizer_name, tokenizer, tokenizer_func, eos_token):\n    instance_id = instance[\"instance_id\"]\n    if instance[\"text\"] is None or instance[\"patch\"] is None:\n        print(f\"No text for {instance_id}\")\n        return {\"input_ids\": [], \"labels\": [], \"text\": \"\", \"patch\": \"\"}\n    text_inputs = instance[\"text\"].strip() + \"\\n\"\n    if text_inputs is None or instance[\"patch\"] is None:\n        print(f\"No inputs for {instance_id}\")\n        return None\n    patch = instance[\"patch\"].strip()\n    if len(eos_token) &gt; 0:\n        patch += f\"\\n{eos_token}\"\n    input_ids = tokenizer_func(text_inputs, tokenizer)\n    if tokenizer_name in {\"llama\"}:\n        label_ids = tokenizer_func(\n            \"\\n\" + patch, tokenizer\n        )  # add newline to tokenize patch\n        idx = label_ids.index(13)\n        assert idx &lt;= 2, (\n            \"Expected newline token id (13) to be one of the first three tokens\"\n        )\n        label_ids = label_ids[idx + 1 :]  # remove newline tokens\n    else:\n        label_ids = tokenizer_func(patch, tokenizer)\n    inputs = input_ids + label_ids[:-1]\n    cond_len = len(input_ids) - 1\n    labels = [-100] * cond_len + label_ids\n    assert len(inputs) == len(labels)\n    return {\n        **instance,\n        \"input_ids\": inputs,\n        \"labels\": labels,\n        \"text\": text_inputs,\n        \"patch\": patch,\n    }\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.extract_test_fields","title":"extract_test_fields","text":"<pre><code>extract_test_fields(instance, tokenizer_name, tokenizer, tokenizer_func, eos_token)\n</code></pre> Source code in <code>swebench/inference/make_datasets/tokenize_dataset.py</code> <pre><code>def extract_test_fields(instance, tokenizer_name, tokenizer, tokenizer_func, eos_token):\n    instance_id = instance[\"instance_id\"]\n    if instance[\"text\"] is None or instance[\"patch\"] is None:\n        print(f\"No text for {instance_id}\")\n        return None\n    text_inputs = instance[\"text\"].strip() + \"\\n\"\n    if text_inputs is None or instance[\"patch\"] is None:\n        print(f\"No inputs for {instance_id}\")\n        return None\n    patch = instance[\"patch\"].strip()\n    if len(eos_token) &gt; 0:\n        patch += f\"\\n{eos_token}\"\n    input_ids = tokenizer_func(text_inputs, tokenizer)\n    label_ids = tokenizer_func(patch, tokenizer)\n    inputs = input_ids\n    labels = label_ids\n    return {\n        **instance,\n        \"input_ids\": inputs,\n        \"labels\": labels,\n        \"text\": text_inputs,\n        \"patch\": patch,\n    }\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.add_columns_from_dict","title":"add_columns_from_dict","text":"<pre><code>add_columns_from_dict(dataset, dict_columns)\n</code></pre> <p>dict_columns is a list of dicts with keys that are columns in dataset</p> Source code in <code>swebench/inference/make_datasets/tokenize_dataset.py</code> <pre><code>def add_columns_from_dict(dataset, dict_columns):\n    \"\"\"dict_columns is a list of dicts with keys that are columns in dataset\"\"\"\n    for column in dict_columns[0].keys():\n        values = [d[column] for d in dict_columns]\n        if column in dataset.column_names:\n            dataset = dataset.remove_columns(column)\n        dataset = dataset.add_column(column, values)\n    return dataset\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.tokenize_dataset.main","title":"main","text":"<pre><code>main(dataset_name_or_path, output_dir, tokenizer_name, num_proc, push_to_hub_user)\n</code></pre> Source code in <code>swebench/inference/make_datasets/tokenize_dataset.py</code> <pre><code>def main(\n    dataset_name_or_path,\n    output_dir,\n    tokenizer_name,\n    num_proc,\n    push_to_hub_user,\n):\n    if push_to_hub_user is not None:\n        hub_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\", None)\n        if hub_token is None:\n            raise ValueError(\"Must provide HUGGING_FACE_HUB_TOKEN to push to the Hub\")\n    if not Path(output_dir).exists():\n        Path(output_dir).mkdir(parents=True)\n\n    if tokenizer_name is not None:\n        tokenizer, tokenizer_func = TOKENIZER_FUNCS[tokenizer_name]\n        eos_token = getattr(tokenizer, \"eos_token\", \"\")\n        if num_proc &gt; 0 and tokenizer_name == \"cl100k\":\n            logger.warning(\n                \"cl100k tokenizer does not support multiprocessing. Ignoring num_proc\"\n            )\n            num_proc = 0\n\n    if Path(dataset_name_or_path).exists():\n        dataset = load_from_disk(dataset_name_or_path)\n    else:\n        dataset = load_dataset(dataset_name_or_path)\n    dataset = dataset.filter(\n        lambda x: len(x[\"text\"]) &lt;= 5_000_000\n    )  # filter out superlong instances\n    for split in dataset.keys():\n        if split == \"test\":\n            continue\n        if num_proc &gt; 0:\n            dataset[split] = dataset[split].map(\n                lambda instance: extract_fields(\n                    instance,\n                    tokenizer_name,\n                    tokenizer,\n                    tokenizer_func,\n                    eos_token,\n                ),\n                num_proc=num_proc,\n                batched=False,\n                desc=f\"Tokenizing {split}\",\n            )\n        elif len(dataset[split]) &gt; 0:\n            new_values = list(\n                map(\n                    lambda x: extract_fields(\n                        x, tokenizer_name, tokenizer, tokenizer_func, eos_token\n                    ),\n                    tqdm(\n                        dataset[split],\n                        total=len(dataset[split]),\n                        desc=f\"Tokenizing {split}\",\n                    ),\n                )\n            )\n            dataset[split] = add_columns_from_dict(dataset[split], new_values)\n    for split in [\"test\"]:\n        if split not in dataset:\n            logger.warning(f\"Split {split} not in dataset. Skipping\")\n            continue\n        if num_proc &gt; 0:\n            dataset[split] = dataset[split].map(\n                lambda instance: extract_test_fields(\n                    instance,\n                    tokenizer_name,\n                    tokenizer,\n                    tokenizer_func,\n                    eos_token,\n                ),\n                num_proc=num_proc,\n                batched=False,\n                desc=f\"Tokenizing {split}\",\n            )\n        elif len(dataset[split]) &gt; 0:\n            new_values = list(\n                map(\n                    lambda x: extract_test_fields(\n                        x, tokenizer_name, tokenizer, tokenizer_func, eos_token\n                    ),\n                    tqdm(\n                        dataset[split],\n                        total=len(dataset[split]),\n                        desc=f\"Tokenizing {split}\",\n                    ),\n                )\n            )\n            dataset[split] = add_columns_from_dict(dataset[split], new_values)\n    output_file = Path(dataset_name_or_path).name + f\"__tok-{tokenizer_name}\"\n    if push_to_hub_user is not None:\n        output_file = f\"{push_to_hub_user}/{output_file}\"\n        dataset.push_to_hub(output_file, use_auth_token=hub_token)\n    else:\n        output_file = Path(output_dir) / output_file\n        dataset.save_to_disk(output_file)\n    logger.warning(f\"Saved to {output_file}\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils","title":"utils","text":""},{"location":"api/inference/#swebench.inference.make_datasets.utils.DIFF_PATTERN","title":"DIFF_PATTERN  <code>module-attribute</code>","text":"<pre><code>DIFF_PATTERN = compile('^diff(?:.*)')\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.PATCH_PATTERN","title":"PATCH_PATTERN  <code>module-attribute</code>","text":"<pre><code>PATCH_PATTERN = compile('(?:diff[\\\\w\\\\_\\\\.\\\\ \\\\/\\\\-]+\\\\n)?\\\\-\\\\-\\\\-\\\\s+a\\\\/(?:.*?)\\\\n\\\\+\\\\+\\\\+\\\\s+b\\\\/(?:.*?)(?=diff\\\\ |\\\\-\\\\-\\\\-\\\\ a\\\\/|\\\\Z)', DOTALL)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.PATCH_FILE_PATTERN","title":"PATCH_FILE_PATTERN  <code>module-attribute</code>","text":"<pre><code>PATCH_FILE_PATTERN = compile('\\\\-\\\\-\\\\-\\\\s+a\\\\/(?:.+)\\\\n\\\\+\\\\+\\\\+\\\\s+b\\\\/(?:.+)')\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.PATCH_HUNK_PATTERN","title":"PATCH_HUNK_PATTERN  <code>module-attribute</code>","text":"<pre><code>PATCH_HUNK_PATTERN = compile('\\\\@\\\\@\\\\s+\\\\-(\\\\d+),(\\\\d+)\\\\s+\\\\+(\\\\d+),(\\\\d+)\\\\s+\\\\@\\\\@(.+?)(?=diff\\\\ |\\\\-\\\\-\\\\-\\\\ a\\\\/|\\\\@\\\\@\\\\ \\\\-|\\\\Z)', DOTALL)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.ContextManager","title":"ContextManager","text":"<pre><code>ContextManager(repo_path, base_commit, verbose=False)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def __init__(self, repo_path, base_commit, verbose=False):\n    self.repo_path = Path(repo_path).resolve().as_posix()\n    self.old_dir = os.getcwd()\n    self.base_commit = base_commit\n    self.verbose = verbose\n</code></pre> <code></code> repo_path <code>instance-attribute</code> <pre><code>repo_path = as_posix()\n</code></pre> <code></code> old_dir <code>instance-attribute</code> <pre><code>old_dir = getcwd()\n</code></pre> <code></code> base_commit <code>instance-attribute</code> <pre><code>base_commit = base_commit\n</code></pre> <code></code> verbose <code>instance-attribute</code> <pre><code>verbose = verbose\n</code></pre> <code></code> __enter__ <pre><code>__enter__()\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def __enter__(self):\n    os.chdir(self.repo_path)\n    cmd = f\"git reset --hard {self.base_commit} &amp;&amp; git clean -fdxq\"\n    if self.verbose:\n        subprocess.run(cmd, shell=True, check=True)\n    else:\n        subprocess.run(\n            cmd,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    return self\n</code></pre> <code></code> get_environment <pre><code>get_environment()\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def get_environment(self):\n    raise NotImplementedError()  # TODO: activate conda environment and return the environment file\n</code></pre> <code></code> get_readme_files <pre><code>get_readme_files()\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def get_readme_files(self):\n    files = os.listdir(self.repo_path)\n    files = list(filter(lambda x: os.path.isfile(x), files))\n    files = list(filter(lambda x: x.lower().startswith(\"readme\"), files))\n    return files\n</code></pre> <code></code> __exit__ <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    os.chdir(self.old_dir)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.AutoContextManager","title":"AutoContextManager","text":"<pre><code>AutoContextManager(instance, root_dir=None, verbose=False, token=None)\n</code></pre> <p>               Bases: <code>ContextManager</code></p> <p>Automatically clones the repo if it doesn't exist</p> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def __init__(self, instance, root_dir=None, verbose=False, token=None):\n    if token is None:\n        token = os.environ.get(\"GITHUB_TOKEN\", \"git\")\n    self.tempdir = None\n    if root_dir is None:\n        self.tempdir = TemporaryDirectory()\n        root_dir = self.tempdir.name\n    self.root_dir = root_dir\n    repo_dir = os.path.join(self.root_dir, instance[\"repo\"].replace(\"/\", \"__\"))\n    if not os.path.exists(repo_dir):\n        repo_url = (\n            f\"https://{token}@github.com/swe-bench-repos/\"\n            + instance[\"repo\"].replace(\"/\", \"__\")\n            + \".git\"\n        )\n        if verbose:\n            print(f\"Cloning {instance['repo']} to {root_dir}\")\n        Repo.clone_from(repo_url, repo_dir)\n    super().__init__(repo_dir, instance[\"base_commit\"], verbose=verbose)\n    self.instance = instance\n</code></pre> <code></code> tempdir <code>instance-attribute</code> <pre><code>tempdir = None\n</code></pre> <code></code> root_dir <code>instance-attribute</code> <pre><code>root_dir = root_dir\n</code></pre> <code></code> instance <code>instance-attribute</code> <pre><code>instance = instance\n</code></pre> <code></code> __exit__ <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    if self.tempdir is not None:\n        self.tempdir.cleanup()\n    return super().__exit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.get_first_idx","title":"get_first_idx","text":"<pre><code>get_first_idx(charlist)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def get_first_idx(charlist):\n    first_min = charlist.index(\"-\") if \"-\" in charlist else len(charlist)\n    first_plus = charlist.index(\"+\") if \"+\" in charlist else len(charlist)\n    return min(first_min, first_plus)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.get_last_idx","title":"get_last_idx","text":"<pre><code>get_last_idx(charlist)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def get_last_idx(charlist):\n    char_idx = get_first_idx(charlist[::-1])\n    last_idx = len(charlist) - char_idx\n    return last_idx + 1\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.strip_content","title":"strip_content","text":"<pre><code>strip_content(hunk)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def strip_content(hunk):\n    first_chars = list(map(lambda x: None if not len(x) else x[0], hunk.split(\"\\n\")))\n    first_idx = get_first_idx(first_chars)\n    last_idx = get_last_idx(first_chars)\n    new_lines = list(map(lambda x: x.rstrip(), hunk.split(\"\\n\")[first_idx:last_idx]))\n    new_hunk = \"\\n\" + \"\\n\".join(new_lines) + \"\\n\"\n    return new_hunk, first_idx - 1\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.get_hunk_stats","title":"get_hunk_stats","text":"<pre><code>get_hunk_stats(pre_start, pre_len, post_start, post_len, hunk, total_delta)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def get_hunk_stats(pre_start, pre_len, post_start, post_len, hunk, total_delta):\n    stats = {\"context\": 0, \"added\": 0, \"subtracted\": 0}\n    hunk = hunk.split(\"\\n\", 1)[-1].strip(\"\\n\")\n    for line in hunk.split(\"\\n\"):\n        if line.startswith(\"-\"):\n            stats[\"subtracted\"] += 1\n        elif line.startswith(\"+\"):\n            stats[\"added\"] += 1\n        else:\n            stats[\"context\"] += 1\n    context = stats[\"context\"]\n    added = stats[\"added\"]\n    subtracted = stats[\"subtracted\"]\n    pre_len = context + subtracted\n    post_start = pre_start + total_delta\n    post_len = context + added\n    total_delta = total_delta + (post_len - pre_len)\n    return pre_start, pre_len, post_start, post_len, total_delta\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.repair_patch","title":"repair_patch","text":"<pre><code>repair_patch(model_patch)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def repair_patch(model_patch):\n    if model_patch is None:\n        return None\n    model_patch = model_patch.lstrip(\"\\n\")\n    new_patch = \"\"\n    for patch in PATCH_PATTERN.findall(model_patch):\n        total_delta = 0\n        diff_header = DIFF_PATTERN.findall(patch)\n        if diff_header:\n            new_patch += diff_header[0] + \"\\n\"\n        patch_header = PATCH_FILE_PATTERN.findall(patch)[0]\n        if patch_header:\n            new_patch += patch_header + \"\\n\"\n        for hunk in PATCH_HUNK_PATTERN.findall(patch):\n            pre_start, pre_len, post_start, post_len, content = hunk\n            pre_start, pre_len, post_start, post_len, total_delta = get_hunk_stats(\n                *list(map(lambda x: int(x) if x.isnumeric() else x, hunk)), total_delta\n            )\n            new_patch += (\n                f\"@@ -{pre_start},{pre_len} +{post_start},{post_len} @@{content}\"\n            )\n    return new_patch\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.extract_minimal_patch","title":"extract_minimal_patch","text":"<pre><code>extract_minimal_patch(model_patch)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def extract_minimal_patch(model_patch):\n    model_patch = model_patch.lstrip(\"\\n\")\n    new_patch = \"\"\n    for patch in PATCH_PATTERN.findall(model_patch):\n        total_delta = 0\n        diff_header = DIFF_PATTERN.findall(patch)\n        patch_header = PATCH_FILE_PATTERN.findall(patch)[0]\n        if patch_header:\n            new_patch += patch_header + \"\\n\"\n        for hunk in PATCH_HUNK_PATTERN.findall(patch):\n            pre_start, pre_len, post_start, post_len, content = hunk\n            pre_start, pre_len, post_start, post_len, content = list(\n                map(lambda x: int(x) if x.isnumeric() else x, hunk)\n            )\n            content, adjust_pre_start = strip_content(content)\n            pre_start += adjust_pre_start\n            pre_start, pre_len, post_start, post_len, total_delta = get_hunk_stats(\n                pre_start, pre_len, post_start, post_len, content, total_delta\n            )\n            new_patch += (\n                f\"@@ -{pre_start},{pre_len} +{post_start},{post_len} @@{content}\"\n            )\n    return new_patch\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.extract_diff","title":"extract_diff","text":"<pre><code>extract_diff(response)\n</code></pre> <p>Extracts the diff from a response formatted in different ways</p> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def extract_diff(response):\n    \"\"\"\n    Extracts the diff from a response formatted in different ways\n    \"\"\"\n    if response is None:\n        return None\n    diff_matches = []\n    other_matches = []\n    pattern = re.compile(r\"\\&lt;([\\w-]+)\\&gt;(.*?)\\&lt;\\/\\1\\&gt;\", re.DOTALL)\n    for code, match in pattern.findall(response):\n        if code in {\"diff\", \"patch\"}:\n            diff_matches.append(match)\n        else:\n            other_matches.append(match)\n    pattern = re.compile(r\"```(\\w+)?\\n(.*?)```\", re.DOTALL)\n    for code, match in pattern.findall(response):\n        if code in {\"diff\", \"patch\"}:\n            diff_matches.append(match)\n        else:\n            other_matches.append(match)\n    if diff_matches:\n        return diff_matches[0]\n    if other_matches:\n        return other_matches[0]\n    return response.split(\"&lt;/s&gt;\")[0]\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.is_test","title":"is_test","text":"<pre><code>is_test(name, test_phrases=None)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def is_test(name, test_phrases=None):\n    if test_phrases is None:\n        test_phrases = [\"test\", \"tests\", \"testing\"]\n    words = set(re.split(r\" |_|\\/|\\.\", name.lower()))\n    return any(word in words for word in test_phrases)\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.get_imported_modules","title":"get_imported_modules","text":"<pre><code>get_imported_modules(filename)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def get_imported_modules(filename):\n    with open(filename) as file:\n        tree = ast.parse(file.read(), filename)\n    return [\n        node\n        for node in ast.iter_child_nodes(tree)\n        if isinstance(node, (ast.Import, ast.ImportFrom))\n    ]\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.resolve_module_to_file","title":"resolve_module_to_file","text":"<pre><code>resolve_module_to_file(module, level, root_dir)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def resolve_module_to_file(module, level, root_dir):\n    components = module.split(\".\")\n    if level &gt; 0:\n        components = components[:-level]\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        if dirpath.endswith(os.sep.join(components)):\n            return [\n                os.path.join(dirpath, filename)\n                for filename in filenames\n                if filename.endswith(\".py\")\n            ]\n    return []\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.ingest_file_directory_contents","title":"ingest_file_directory_contents","text":"<pre><code>ingest_file_directory_contents(target_file, root_dir)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def ingest_file_directory_contents(target_file, root_dir):\n    imported_files = []\n    files_to_check = [target_file]\n    while files_to_check:\n        current_file = files_to_check.pop()\n        imported_files.append(current_file)\n        imports = get_imported_modules(current_file)\n        for node in imports:\n            if isinstance(node, ast.Import):\n                for alias in node.names:\n                    files = resolve_module_to_file(alias.name, 0, root_dir)\n                    for file in files:\n                        if file not in imported_files and file not in files_to_check:\n                            files_to_check.append(file)\n            elif isinstance(node, ast.ImportFrom):\n                files = resolve_module_to_file(node.module, node.level, root_dir)\n                for file in files:\n                    if file not in imported_files and file not in files_to_check:\n                        files_to_check.append(file)\n    return imported_files\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.detect_encoding","title":"detect_encoding","text":"<pre><code>detect_encoding(filename)\n</code></pre> <p>Detect the encoding of a file</p> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def detect_encoding(filename):\n    \"\"\"\n    Detect the encoding of a file\n    \"\"\"\n    with open(filename, \"rb\") as file:\n        rawdata = file.read()\n    return chardet.detect(rawdata)[\"encoding\"]\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.list_files","title":"list_files","text":"<pre><code>list_files(root_dir, include_tests=False)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def list_files(root_dir, include_tests=False):\n    files = []\n    for filename in Path(root_dir).rglob(\"*.py\"):\n        if not include_tests and is_test(filename.as_posix()):\n            continue\n        files.append(filename.relative_to(root_dir).as_posix())\n    return files\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.ingest_directory_contents","title":"ingest_directory_contents","text":"<pre><code>ingest_directory_contents(root_dir, include_tests=False)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def ingest_directory_contents(root_dir, include_tests=False):\n    files_content = {}\n    for relative_path in list_files(root_dir, include_tests=include_tests):\n        filename = os.path.join(root_dir, relative_path)\n        encoding = detect_encoding(filename)\n        if encoding is None:\n            content = \"[BINARY DATA FILE]\"\n        else:\n            try:\n                with open(filename, encoding=encoding) as file:\n                    content = file.read()\n            except (UnicodeDecodeError, LookupError):\n                content = \"[BINARY DATA FILE]\"\n        files_content[relative_path] = content\n    return files_content\n</code></pre>"},{"location":"api/inference/#swebench.inference.make_datasets.utils.string_to_bool","title":"string_to_bool","text":"<pre><code>string_to_bool(v)\n</code></pre> Source code in <code>swebench/inference/make_datasets/utils.py</code> <pre><code>def string_to_bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise ArgumentTypeError(\n            f\"Truthy value expected: got {v} but expected one of yes/no, true/false, t/f, y/n, 1/0 (case insensitive).\"\n        )\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api","title":"run_api","text":"<p>This python script is designed to run inference on a dataset using either the OpenAI or Anthropic API, depending on the model specified. It sorts instances by length and continually writes the outputs to a specified file, so that the script can be stopped and restarted without losing progress.</p>"},{"location":"api/inference/#swebench.inference.run_api.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.MODEL_LIMITS","title":"MODEL_LIMITS  <code>module-attribute</code>","text":"<pre><code>MODEL_LIMITS = {'claude-instant-1': 100000, 'claude-2': 100000, 'claude-3-opus-20240229': 200000, 'claude-3-sonnet-20240229': 200000, 'claude-3-haiku-20240307': 200000, 'gpt-3.5-turbo-16k-0613': 16385, 'gpt-3.5-turbo-0613': 4097, 'gpt-3.5-turbo-1106': 16385, 'gpt-4-32k-0613': 32768, 'gpt-4-0613': 8192, 'gpt-4-1106-preview': 128000, 'gpt-4-0125-preview': 128000}\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.MODEL_COST_PER_INPUT","title":"MODEL_COST_PER_INPUT  <code>module-attribute</code>","text":"<pre><code>MODEL_COST_PER_INPUT = {'claude-instant-1': 1.63e-06, 'claude-2': 1.102e-05, 'claude-3-opus-20240229': 1.5e-05, 'claude-3-sonnet-20240229': 3e-06, 'claude-3-haiku-20240307': 2.5e-07, 'gpt-3.5-turbo-16k-0613': 1.5e-06, 'gpt-3.5-turbo-0613': 1.5e-06, 'gpt-3.5-turbo-1106': 1e-06, 'gpt-35-turbo-0613': 1.5e-06, 'gpt-35-turbo': 1.5e-06, 'gpt-4-0613': 3e-05, 'gpt-4-32k-0613': 6e-05, 'gpt-4-32k': 6e-05, 'gpt-4-1106-preview': 1e-05, 'gpt-4-0125-preview': 1e-05}\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.MODEL_COST_PER_OUTPUT","title":"MODEL_COST_PER_OUTPUT  <code>module-attribute</code>","text":"<pre><code>MODEL_COST_PER_OUTPUT = {'claude-instant-1': 5.51e-06, 'claude-2': 3.268e-05, 'claude-3-opus-20240229': 7.5e-05, 'claude-3-sonnet-20240229': 1.5e-05, 'claude-3-haiku-20240307': 1.25e-06, 'gpt-3.5-turbo-16k-0613': 2e-06, 'gpt-3.5-turbo-16k': 2e-06, 'gpt-3.5-turbo-1106': 2e-06, 'gpt-35-turbo-0613': 2e-06, 'gpt-35-turbo': 2e-06, 'gpt-4-0613': 6e-05, 'gpt-4-32k-0613': 0.00012, 'gpt-4-32k': 0.00012, 'gpt-4-1106-preview': 3e-05, 'gpt-4-0125-preview': 3e-05}\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.ENGINES","title":"ENGINES  <code>module-attribute</code>","text":"<pre><code>ENGINES = {'gpt-3.5-turbo-16k-0613': 'gpt-35-turbo-16k', 'gpt-4-0613': 'gpt-4', 'gpt-4-32k-0613': 'gpt-4-32k'}\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description=__doc__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.calc_cost","title":"calc_cost","text":"<pre><code>calc_cost(model_name, input_tokens, output_tokens)\n</code></pre> <p>Calculates the cost of a response from the openai API.</p> <p>Args: response (openai.ChatCompletion): The response from the API.</p> <p>Returns: float: The cost of the response.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def calc_cost(model_name, input_tokens, output_tokens):\n    \"\"\"\n    Calculates the cost of a response from the openai API.\n\n    Args:\n    response (openai.ChatCompletion): The response from the API.\n\n    Returns:\n    float: The cost of the response.\n    \"\"\"\n    cost = (\n        MODEL_COST_PER_INPUT[model_name] * input_tokens\n        + MODEL_COST_PER_OUTPUT[model_name] * output_tokens\n    )\n    logger.info(\n        f\"input_tokens={input_tokens}, output_tokens={output_tokens}, cost={cost:.2f}\"\n    )\n    return cost\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.call_chat","title":"call_chat","text":"<pre><code>call_chat(model_name_or_path, inputs, use_azure, temperature, top_p, **model_args)\n</code></pre> <p>Calls the openai API to generate completions for the given inputs.</p> <p>Args: model_name_or_path (str): The name or path of the model to use. inputs (str): The inputs to generate completions for. use_azure (bool): Whether to use the azure API. temperature (float): The temperature to use. top_p (float): The top_p to use. **model_args (dict): A dictionary of model arguments.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>@retry(wait=wait_random_exponential(min=30, max=600), stop=stop_after_attempt(3))\ndef call_chat(model_name_or_path, inputs, use_azure, temperature, top_p, **model_args):\n    \"\"\"\n    Calls the openai API to generate completions for the given inputs.\n\n    Args:\n    model_name_or_path (str): The name or path of the model to use.\n    inputs (str): The inputs to generate completions for.\n    use_azure (bool): Whether to use the azure API.\n    temperature (float): The temperature to use.\n    top_p (float): The top_p to use.\n    **model_args (dict): A dictionary of model arguments.\n    \"\"\"\n    system_messages = inputs.split(\"\\n\", 1)[0]\n    user_message = inputs.split(\"\\n\", 1)[1]\n    try:\n        if use_azure:\n            response = openai.chat.completions.create(\n                engine=ENGINES[model_name_or_path] if use_azure else None,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_messages},\n                    {\"role\": \"user\", \"content\": user_message},\n                ],\n                temperature=temperature,\n                top_p=top_p,\n                **model_args,\n            )\n        else:\n            response = openai.chat.completions.create(\n                model=model_name_or_path,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_messages},\n                    {\"role\": \"user\", \"content\": user_message},\n                ],\n                temperature=temperature,\n                top_p=top_p,\n                **model_args,\n            )\n        input_tokens = response.usage.prompt_tokens\n        output_tokens = response.usage.completion_tokens\n        cost = calc_cost(response.model, input_tokens, output_tokens)\n        return response, cost\n    except openai.BadRequestError as e:\n        if e.code == \"context_length_exceeded\":\n            print(\"Context length exceeded\")\n            return None\n        raise e\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.gpt_tokenize","title":"gpt_tokenize","text":"<pre><code>gpt_tokenize(string: str, encoding) -&gt; int\n</code></pre> <p>Returns the number of tokens in a text string.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def gpt_tokenize(string: str, encoding) -&gt; int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.claude_tokenize","title":"claude_tokenize","text":"<pre><code>claude_tokenize(string: str, api) -&gt; int\n</code></pre> <p>Returns the number of tokens in a text string.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def claude_tokenize(string: str, api) -&gt; int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    num_tokens = api.count_tokens(string)\n    return num_tokens\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.openai_inference","title":"openai_inference","text":"<pre><code>openai_inference(test_dataset, model_name_or_path, output_file, model_args, existing_ids, max_cost)\n</code></pre> <p>Runs inference on a dataset using the openai API.</p> <p>Args: test_dataset (datasets.Dataset): The dataset to run inference on. model_name_or_path (str): The name or path of the model to use. output_file (str): The path to the output file. model_args (dict): A dictionary of model arguments. existing_ids (set): A set of ids that have already been processed. max_cost (float): The maximum cost to spend on inference.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def openai_inference(\n    test_dataset,\n    model_name_or_path,\n    output_file,\n    model_args,\n    existing_ids,\n    max_cost,\n):\n    \"\"\"\n    Runs inference on a dataset using the openai API.\n\n    Args:\n    test_dataset (datasets.Dataset): The dataset to run inference on.\n    model_name_or_path (str): The name or path of the model to use.\n    output_file (str): The path to the output file.\n    model_args (dict): A dictionary of model arguments.\n    existing_ids (set): A set of ids that have already been processed.\n    max_cost (float): The maximum cost to spend on inference.\n    \"\"\"\n    encoding = tiktoken.encoding_for_model(model_name_or_path)\n    test_dataset = test_dataset.filter(\n        lambda x: gpt_tokenize(x[\"text\"], encoding) &lt;= MODEL_LIMITS[model_name_or_path],\n        desc=\"Filtering\",\n        load_from_cache_file=False,\n    )\n    openai_key = os.environ.get(\"OPENAI_API_KEY\", None)\n    if openai_key is None:\n        raise ValueError(\n            \"Must provide an api key. Expected in OPENAI_API_KEY environment variable.\"\n        )\n    openai.api_key = openai_key\n    print(f\"Using OpenAI key {'*' * max(0, len(openai_key) - 5) + openai_key[-5:]}\")\n    use_azure = model_args.pop(\"use_azure\", False)\n    if use_azure:\n        openai.api_type = \"azure\"\n        openai.api_base = \"https://pnlpopenai3.openai.azure.com/\"\n        openai.api_version = \"2023-05-15\"\n    temperature = model_args.pop(\"temperature\", 0.2)\n    top_p = model_args.pop(\"top_p\", 0.95 if temperature &gt; 0 else 1)\n    print(f\"Using temperature={temperature}, top_p={top_p}\")\n    basic_args = {\n        \"model_name_or_path\": model_name_or_path,\n    }\n    total_cost = 0\n    print(f\"Filtered to {len(test_dataset)} instances\")\n    with open(output_file, \"a+\") as f:\n        for datum in tqdm(test_dataset, desc=f\"Inference for {model_name_or_path}\"):\n            instance_id = datum[\"instance_id\"]\n            if instance_id in existing_ids:\n                continue\n            output_dict = {\"instance_id\": instance_id}\n            output_dict.update(basic_args)\n            output_dict[\"text\"] = f\"{datum['text']}\\n\\n\"\n            response, cost = call_chat(\n                output_dict[\"model_name_or_path\"],\n                output_dict[\"text\"],\n                use_azure,\n                temperature,\n                top_p,\n            )\n            completion = response.choices[0].message.content\n            total_cost += cost\n            print(f\"Total Cost: {total_cost:.2f}\")\n            output_dict[\"full_output\"] = completion\n            output_dict[\"model_patch\"] = extract_diff(completion)\n            print(json.dumps(output_dict), file=f, flush=True)\n            if max_cost is not None and total_cost &gt;= max_cost:\n                print(f\"Reached max cost {max_cost}, exiting\")\n                break\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.call_anthropic","title":"call_anthropic","text":"<pre><code>call_anthropic(inputs, anthropic, model_name_or_path, temperature, top_p, **model_args)\n</code></pre> <p>Calls the anthropic API to generate completions for the given inputs.</p> <p>Args: inputs (str): The inputs to generate completions for. anthropic (Anthropic): The anthropic API object. model_name_or_path (str): The name or path of the model to use. temperature (float): The temperature to use. top_p (float): The top_p to use. model_args (dict): A dictionary of model arguments.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>@retry(wait=wait_random_exponential(min=60, max=600), stop=stop_after_attempt(6))\ndef call_anthropic(\n    inputs, anthropic, model_name_or_path, temperature, top_p, **model_args\n):\n    \"\"\"\n    Calls the anthropic API to generate completions for the given inputs.\n\n    Args:\n    inputs (str): The inputs to generate completions for.\n    anthropic (Anthropic): The anthropic API object.\n    model_name_or_path (str): The name or path of the model to use.\n    temperature (float): The temperature to use.\n    top_p (float): The top_p to use.\n    model_args (dict): A dictionary of model arguments.\n    \"\"\"\n    try:\n        completion = anthropic.completions.create(\n            model=model_name_or_path,\n            max_tokens_to_sample=6000,\n            prompt=inputs,\n            temperature=temperature,\n            top_p=top_p,\n            **model_args,\n        )\n        response = completion.completion\n        input_tokens = anthropic.count_tokens(inputs)\n        output_tokens = anthropic.count_tokens(response)\n        cost = calc_cost(model_name_or_path, input_tokens, output_tokens)\n        return completion, cost\n    except Exception as e:\n        logger.error(e)\n        logger.error(f\"Inputs: {inputs}\")\n        traceback.print_exc()\n        time.sleep(20)\n        return None\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.call_anthropic_v2","title":"call_anthropic_v2","text":"<pre><code>call_anthropic_v2(inputs, anthropic, model_name_or_path, temperature, top_p, **model_args)\n</code></pre> <p>Calls the anthropic API to generate completions for the given inputs.</p> <p>Args: inputs list(str): The inputs to generate completions for. anthropic (Anthropic): The anthropic API object. model_name_or_path (str): The name or path of the model to use. temperature (float): The temperature to use. top_p (float): The top_p to use. model_args (dict): A dictionary of model arguments.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>@retry(wait=wait_random_exponential(min=60, max=600), stop=stop_after_attempt(6))\ndef call_anthropic_v2(\n    inputs, anthropic, model_name_or_path, temperature, top_p, **model_args\n):\n    \"\"\"\n    Calls the anthropic API to generate completions for the given inputs.\n\n    Args:\n    inputs list(str): The inputs to generate completions for.\n    anthropic (Anthropic): The anthropic API object.\n    model_name_or_path (str): The name or path of the model to use.\n    temperature (float): The temperature to use.\n    top_p (float): The top_p to use.\n    model_args (dict): A dictionary of model arguments.\n    \"\"\"\n    system_messages = inputs.split(\"\\n\", 1)[0]\n    user_message = inputs.split(\"\\n\", 1)[1]\n    try:\n        messages = [\n            {\"role\": \"user\", \"content\": user_message},\n        ]\n        response = anthropic.messages.create(\n            messages=messages,\n            max_tokens=4096,\n            model=model_name_or_path,\n            temperature=temperature,\n            top_p=top_p,\n            system=system_messages,\n        )\n        input_tokens = response.usage.input_tokens\n        output_tokens = response.usage.output_tokens\n        cost = calc_cost(response.model, input_tokens, output_tokens)\n        return response, cost\n    except Exception as e:\n        logger.error(e)\n        logger.error(f\"Inputs: {inputs}\")\n        traceback.print_exc()\n        time.sleep(20)\n        return None\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.anthropic_inference","title":"anthropic_inference","text":"<pre><code>anthropic_inference(test_dataset, model_name_or_path, output_file, model_args, existing_ids, max_cost)\n</code></pre> <p>Runs inference on a dataset using the anthropic API.</p> <p>Args: test_dataset (datasets.Dataset): The dataset to run inference on. model_name_or_path (str): The name or path of the model to use. output_file (str): The path to the output file. model_args (dict): A dictionary of model arguments. existing_ids (set): A set of ids that have already been processed. max_cost (float): The maximum cost to spend on inference.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def anthropic_inference(\n    test_dataset,\n    model_name_or_path,\n    output_file,\n    model_args,\n    existing_ids,\n    max_cost,\n):\n    \"\"\"\n    Runs inference on a dataset using the anthropic API.\n\n    Args:\n    test_dataset (datasets.Dataset): The dataset to run inference on.\n    model_name_or_path (str): The name or path of the model to use.\n    output_file (str): The path to the output file.\n    model_args (dict): A dictionary of model arguments.\n    existing_ids (set): A set of ids that have already been processed.\n    max_cost (float): The maximum cost to spend on inference.\n    \"\"\"\n    api_key = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n    if api_key is None:\n        raise ValueError(\n            \"Must provide an api key. Expected in ANTHROPIC_API_KEY environment variable.\"\n        )\n    print(f\"Using Anthropic key {'*' * max(0, len(api_key) - 5) + api_key[-5:]}\")\n    anthropic = Anthropic(api_key=api_key)\n    test_dataset = test_dataset.filter(\n        lambda x: claude_tokenize(x[\"text\"], anthropic)\n        &lt;= MODEL_LIMITS[model_name_or_path],\n        desc=\"Filtering\",\n        load_from_cache_file=False,\n    )\n    temperature = model_args.pop(\"temperature\", 0.2)\n    top_p = model_args.pop(\"top_p\", 0.95 if temperature &gt; 0 else 1)\n    print(f\"Using temperature={temperature}, top_p={top_p}\")\n    basic_args = {\n        \"model_name_or_path\": model_name_or_path,\n    }\n    total_cost = 0\n    print(f\"Filtered to {len(test_dataset)} instances\")\n    if \"claude-3\" in model_name_or_path.lower():\n        call_api = call_anthropic_v2\n    else:\n        call_api = call_anthropic\n    with open(output_file, \"a+\") as f:\n        for datum in tqdm(test_dataset, desc=f\"Inference for {model_name_or_path}\"):\n            instance_id = datum[\"instance_id\"]\n            if instance_id in existing_ids:\n                continue\n            output_dict = {\"instance_id\": instance_id}\n            output_dict.update(basic_args)\n            if \"claude-3\" in model_name_or_path.lower():\n                output_dict[\"text_inputs\"] = f\"{datum['text']}\\n\"\n            else:\n                output_dict[\"text_inputs\"] = (\n                    f\"{HUMAN_PROMPT} {datum['text']}\\n\\n{AI_PROMPT}\"\n                )\n            try:\n                completion, cost = call_api(\n                    output_dict[\"text_inputs\"],\n                    anthropic,\n                    model_name_or_path,\n                    temperature,\n                    top_p,\n                    **model_args,\n                )\n            except Exception as e:\n                logger.error(e)\n                traceback.print_exc()\n                continue\n            total_cost += cost\n            print(f\"Total Cost: {total_cost:.2f}\")\n            if \"claude-3\" in model_name_or_path.lower():\n                output_dict[\"full_output\"] = completion.content[0].text\n            else:\n                output_dict[\"full_output\"] = completion.completion\n            output_dict[\"model_patch\"] = extract_diff(output_dict[\"full_output\"])\n            print(json.dumps(output_dict), file=f, flush=True)\n            if max_cost is not None and total_cost &gt;= max_cost:\n                print(f\"Reached max cost {max_cost}, exiting\")\n                break\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.parse_model_args","title":"parse_model_args","text":"<pre><code>parse_model_args(model_args)\n</code></pre> <p>Parses a string of model arguments and returns a dictionary of keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>str</code> <p>A string of comma-separated key-value pairs representing model arguments.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of keyword arguments parsed from the input string.</p> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def parse_model_args(model_args):\n    \"\"\"\n    Parses a string of model arguments and returns a dictionary of keyword arguments.\n\n    Args:\n        model_args (str): A string of comma-separated key-value pairs representing model arguments.\n\n    Returns:\n        dict: A dictionary of keyword arguments parsed from the input string.\n    \"\"\"\n    kwargs = dict()\n    if model_args is not None:\n        for arg in model_args.split(\",\"):\n            key, value = arg.split(\"=\")\n            # infer value type\n            if value in {\"True\", \"False\"}:\n                kwargs[key] = value == \"True\"\n            elif value.isnumeric():\n                kwargs[key] = int(value)\n            elif value.replace(\".\", \"\", 1).isnumeric():\n                kwargs[key] = float(value)\n            elif value in {\"None\"}:\n                kwargs[key] = None\n            elif value in {\"[]\"}:\n                kwargs[key] = []\n            elif value in {\"{}\"}:\n                kwargs[key] = {}\n            elif value.startswith(\"'\") and value.endswith(\"'\"):\n                kwargs[key] = value[1:-1]\n            elif value.startswith('\"') and value.endswith('\"'):\n                kwargs[key] = value[1:-1]\n            else:\n                kwargs[key] = value\n    return kwargs\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_api.main","title":"main","text":"<pre><code>main(dataset_name_or_path, split, model_name_or_path, shard_id, num_shards, output_dir, model_args, max_cost)\n</code></pre> Source code in <code>swebench/inference/run_api.py</code> <pre><code>def main(\n    dataset_name_or_path,\n    split,\n    model_name_or_path,\n    shard_id,\n    num_shards,\n    output_dir,\n    model_args,\n    max_cost,\n):\n    if shard_id is None and num_shards is not None:\n        logger.warning(\n            f\"Received num_shards={num_shards} but shard_id is None, ignoring\"\n        )\n    if shard_id is not None and num_shards is None:\n        logger.warning(f\"Received shard_id={shard_id} but num_shards is None, ignoring\")\n    model_args = parse_model_args(model_args)\n    model_nickname = model_name_or_path\n    if \"checkpoint\" in Path(model_name_or_path).name:\n        model_nickname = Path(model_name_or_path).parent.name\n    else:\n        model_nickname = Path(model_name_or_path).name\n    output_file = f\"{model_nickname}__{dataset_name_or_path.split('/')[-1]}__{split}\"\n    if shard_id is not None and num_shards is not None:\n        output_file += f\"__shard-{shard_id}__num_shards-{num_shards}\"\n    output_file = Path(output_dir, output_file + \".jsonl\")\n    logger.info(f\"Will write to {output_file}\")\n    existing_ids = set()\n    if os.path.exists(output_file):\n        with open(output_file) as f:\n            for line in f:\n                data = json.loads(line)\n                instance_id = data[\"instance_id\"]\n                existing_ids.add(instance_id)\n    logger.info(f\"Read {len(existing_ids)} already completed ids from {output_file}\")\n    if Path(dataset_name_or_path).exists():\n        dataset = load_from_disk(dataset_name_or_path)\n    else:\n        dataset = load_dataset(dataset_name_or_path)\n    if split not in dataset:\n        raise ValueError(f\"Invalid split {split} for dataset {dataset_name_or_path}\")\n    dataset = dataset[split]\n    lens = np.array(list(map(len, dataset[\"text\"])))\n    dataset = dataset.select(np.argsort(lens))\n    if len(existing_ids) &gt; 0:\n        dataset = dataset.filter(\n            lambda x: x[\"instance_id\"] not in existing_ids,\n            desc=\"Filtering out existing ids\",\n            load_from_cache_file=False,\n        )\n    if shard_id is not None and num_shards is not None:\n        dataset = dataset.shard(num_shards, shard_id, contiguous=True)\n    inference_args = {\n        \"test_dataset\": dataset,\n        \"model_name_or_path\": model_name_or_path,\n        \"output_file\": output_file,\n        \"model_args\": model_args,\n        \"existing_ids\": existing_ids,\n        \"max_cost\": max_cost,\n    }\n    if model_name_or_path.startswith(\"claude\"):\n        anthropic_inference(**inference_args)\n    elif model_name_or_path.startswith(\"gpt\"):\n        openai_inference(**inference_args)\n    else:\n        raise ValueError(f\"Invalid model name or path {model_name_or_path}\")\n    logger.info(\"Done!\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live","title":"run_live","text":"<p>This module contains functions for running a live inference session on a GitHub issue. It clones the repository associated with the issue, builds a BM25 retrieval index, and generates a prompt for the user to interact with the model. The output is saved to a specified directory.</p>"},{"location":"api/inference/#swebench.inference.run_live.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description=__doc__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.get_problem_statement","title":"get_problem_statement","text":"<pre><code>get_problem_statement(owner, repo, issue_num, ghapi, include_comments=False)\n</code></pre> Source code in <code>swebench/inference/run_live.py</code> <pre><code>def get_problem_statement(owner, repo, issue_num, ghapi, include_comments=False):\n    issue = ghapi.issues.get(owner, repo, issue_num)\n    issue_text = \"\\n\".join([issue.title, issue.body])\n    # Solved issues may include comments that give answers away too much\n    if include_comments:\n        all_comments = list(ghapi.issues.list_comments(owner, repo, issue_num))\n        comments = [comment.body for comment in all_comments]\n        comment_text = \"Comment: \" if comments else \"\" + \"\\nComment:\".join(comments)\n        issue_text += \"\\n\" + comment_text\n    return issue_text\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.get_readme_files","title":"get_readme_files","text":"<pre><code>get_readme_files(repo_path)\n</code></pre> Source code in <code>swebench/inference/run_live.py</code> <pre><code>def get_readme_files(repo_path):\n    files = list(Path(repo_path).iterdir())\n    files = list(filter(lambda x: x.is_file(), files))\n    files = list(filter(lambda x: x.name.lower().startswith(\"readme\"), files))\n    if files:\n        files = sorted(files, key=lambda x: len(x.name))\n        files = [files[0]]\n    return [Path(file).relative_to(repo_path).as_posix() for file in files]\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.make_instance","title":"make_instance","text":"<pre><code>make_instance(owner, repo, query, commit, root_dir, token, document_encoding_func, python, instance_id, tokenizer, tokenizer_func, prompt_style, max_context_len, include_readmes)\n</code></pre> <p>Creates an instance for a given query and repository.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str</code> <p>The owner of the repository.</p> required <code>repo</code> <code>str</code> <p>The name of the repository.</p> required <code>query</code> <code>str</code> <p>The query to search for.</p> required <code>commit</code> <code>str</code> <p>The commit hash to use.</p> required <code>root_dir</code> <code>str</code> <p>The root directory to clone the repository to.</p> required <code>token</code> <code>str</code> <p>The GitHub token to use for authentication.</p> required <code>document_encoding_func</code> <code>function</code> <p>The function to use for encoding documents.</p> required <code>python</code> <code>str</code> <p>The path to the Python executable.</p> required <code>instance_id</code> <code>int</code> <p>The ID of the instance.</p> required <code>tokenizer</code> <code>str</code> <p>The name of the tokenizer to use.</p> required <code>tokenizer_func</code> <code>function</code> <p>The function to use for tokenization.</p> required <code>prompt_style</code> <code>str</code> <p>The style of prompt to use.</p> required <code>max_context_len</code> <code>int</code> <p>The maximum length of the context.</p> required <code>include_readmes</code> <code>bool</code> <p>Whether to include README files in the instance.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The instance.</p> Source code in <code>swebench/inference/run_live.py</code> <pre><code>def make_instance(\n    owner,\n    repo,\n    query,\n    commit,\n    root_dir,\n    token,\n    document_encoding_func,\n    python,\n    instance_id,\n    tokenizer,\n    tokenizer_func,\n    prompt_style,\n    max_context_len,\n    include_readmes,\n):\n    \"\"\"\n    Creates an instance for a given query and repository.\n\n    Args:\n        owner (str): The owner of the repository.\n        repo (str): The name of the repository.\n        query (str): The query to search for.\n        commit (str): The commit hash to use.\n        root_dir (str): The root directory to clone the repository to.\n        token (str): The GitHub token to use for authentication.\n        document_encoding_func (function): The function to use for encoding documents.\n        python (str): The path to the Python executable.\n        instance_id (int): The ID of the instance.\n        tokenizer (str): The name of the tokenizer to use.\n        tokenizer_func (function): The function to use for tokenization.\n        prompt_style (str): The style of prompt to use.\n        max_context_len (int): The maximum length of the context.\n        include_readmes (bool): Whether to include README files in the instance.\n\n    Returns:\n        dict: The instance.\n    \"\"\"\n    thread_id = 0\n    instance = {\"instance_id\": instance_id, \"problem_statement\": query}\n    logger.info(f\"Cloning repo {owner}/{repo}\")\n    repo_dir = clone_repo(f\"{owner}/{repo}\", root_dir, token)\n    if commit is None:\n        commit = (\n            subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=repo_dir)\n            .decode(\"utf-8\")\n            .strip()\n        )\n    logger.info(f\"Building BM25 retrieval index for {owner}/{repo}@{commit}\")\n    index_dir = make_index(\n        repo_dir=repo_dir,\n        root_dir=root_dir,\n        query=query,\n        commit=commit,\n        document_encoding_func=document_encoding_func,\n        python=python,\n        instance_id=instance_id,\n    )\n    results = search(instance, index_dir)\n    hits = results[\"hits\"]\n    logger.info(f\"Retrieved {len(hits)} documents\")\n    with ContextManager(repo_dir, commit) as cm:\n        if include_readmes:\n            readmes = get_readme_files(cm.repo_path)\n        else:\n            readmes = list()\n        instance[\"readmes\"] = ingest_files(readmes)\n        for hit in hits:\n            hit[\"file_contents\"] = open(hit[\"docid\"]).read()\n        instance[\"file_contents\"] = dict()\n        base_text_inputs = PROMPT_FUNCTIONS[prompt_style](instance)\n        base_text_input_length = len(tokenizer_func(base_text_inputs, tokenizer))\n        instance[\"file_contents\"] = {x[\"docid\"]: x[\"file_contents\"] for x in hits}\n        cur_input_len = base_text_input_length\n        include_files = list()\n        for filename in [x[\"docid\"] for x in hits]:\n            content = make_code_text({filename: instance[\"file_contents\"][filename]})\n            tokens = tokenizer_func(content, tokenizer)\n            if cur_input_len + len(tokens) &lt; max_context_len:\n                include_files.append(filename)\n                cur_input_len += len(tokens)\n        logger.info(\n            f\"Including {len(include_files)} files in context with {cur_input_len} tokens:\\n\"\n            + \"\\n\\t\".join(sorted(include_files))\n        )\n        instance[\"file_contents\"] = {\n            filename: instance[\"file_contents\"][filename] for filename in include_files\n        }\n        instance[\"text_inputs\"] = PROMPT_FUNCTIONS[prompt_style](instance)\n        return instance\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.parse_issue_url","title":"parse_issue_url","text":"<pre><code>parse_issue_url(issue_url)\n</code></pre> Source code in <code>swebench/inference/run_live.py</code> <pre><code>def parse_issue_url(issue_url):\n    issue_pat = re.compile(r\"github\\.com\\/(.+?)\\/(.+?)\\/issues\\/(\\d+)\")\n    match = issue_pat.search(issue_url)\n    if not match:\n        raise ValueError(\n            f\"issue_url ({issue_url}) does not seem to be a valid issue url.\"\n            + \"\\nPlease use url like https://github.com/owner/repo/issues/12345\"\n        )\n    owner, repo, issue_num = match.groups()\n    return owner, repo, issue_num\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_live.main","title":"main","text":"<pre><code>main(model_name, prompt_style, issue_url, base_commit, max_context_length, document_encoding_func, output_dir, root_dir, include_readmes)\n</code></pre> Source code in <code>swebench/inference/run_live.py</code> <pre><code>def main(\n    model_name,\n    prompt_style,\n    issue_url,\n    base_commit,\n    max_context_length,\n    document_encoding_func,\n    output_dir,\n    root_dir,\n    include_readmes,\n):\n    if base_commit is not None and len(issue_url) != len(base_commit):\n        raise ValueError(\n            \"Must provide either no base commits or one base commit per issue url\"\n        )\n    if base_commit is None:\n        base_commit = [None] * len(issue_url)\n    gh_token = os.environ.get(\"GITHUB_TOKEN\", None)\n    if gh_token is not None:\n        logger.warning(f\"Using GitHub token: {'*' * 8}{gh_token[-4:]}\")\n    gh = GhApi(token=gh_token)\n    tokenizer, tokenizer_func = TOKENIZER_FUNCS[\"cl100k\"]\n    document_encoding_func = DOCUMENT_ENCODING_FUNCTIONS[document_encoding_func]\n    python = subprocess.check_output([\"which\", \"python\"]).decode(\"utf-8\").strip()\n    outputs = list()\n    for issue, commit in tqdm(zip(issue_url, base_commit), total=len(issue_url)):\n        owner, repo, issue_num = parse_issue_url(issue)\n        problem_statement = get_problem_statement(owner, repo, int(issue_num), gh)\n        instance_id = f\"{owner}__{repo}-{issue_num}\"\n        logger.info(f\"Creating instance {instance_id}\")\n        instance = make_instance(\n            owner=owner,\n            repo=repo,\n            query=problem_statement,\n            commit=commit,\n            root_dir=root_dir,\n            token=gh_token,\n            document_encoding_func=document_encoding_func,\n            python=python,\n            instance_id=instance_id,\n            tokenizer=tokenizer,\n            tokenizer_func=tokenizer_func,\n            prompt_style=prompt_style,\n            max_context_len=max_context_length,\n            include_readmes=include_readmes,\n        )\n        logger.info(f\"Calling model {model_name}\")\n        start = time.time()\n        if model_name.startswith(\"gpt\"):\n            inputs = instance[\"text_inputs\"]\n            response, _ = call_chat(\n                model_name, inputs, use_azure=False, temperature=0, top_p=1\n            )\n            completion = response.choices[0].message.content\n            logger.info(\n                f\"Generated {response.usage.completion_tokens} tokens in {(time.time() - start):.2f} seconds\"\n            )\n        else:\n            from anthropic import Anthropic\n\n            api_key = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n            anthropic = Anthropic(api_key=api_key)\n            response = call_anthropic(\n                inputs, anthropic, model_name, temperature=0, top_p=1\n            )\n            completion = response.completion\n        model_patch = extract_diff(completion)\n        minimal_patch = extract_minimal_patch(model_patch)\n        outputs.append(\n            {\n                \"instance_id\": instance_id,\n                \"response\": completion,\n                \"problem_statement\": problem_statement,\n                \"text_inputs\": inputs,\n                \"model_patch\": model_patch,\n                \"minimal_patch\": minimal_patch,\n            }\n        )\n    os.makedirs(output_dir, exist_ok=True)\n    output_file = Path(\n        output_dir,\n        f\"{model_name}__{prompt_style}__{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.jsonl\",\n    )\n    with open(output_file, \"+a\") as f:\n        for output in outputs:\n            print(json.dumps(output), file=f, flush=True)\n    logger.info(f\"Wrote output to {output_file}\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama","title":"run_llama","text":""},{"location":"api/inference/#swebench.inference.run_llama.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.DEVICE_MAPS","title":"DEVICE_MAPS  <code>module-attribute</code>","text":"<pre><code>DEVICE_MAPS = load(open(parent / 'codellama_device_maps.json'))\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser()\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.get_output_file","title":"get_output_file","text":"<pre><code>get_output_file(output_dir, model_name_or_path, peft_path, dataset_path, split, temperature, top_p, min_len, max_len, shard_id, num_shards)\n</code></pre> <p>Constructs the output file path based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The directory where the output file will be saved.</p> required <code>model_name_or_path</code> <code>str</code> <p>The name or path of the model.</p> required <code>peft_path</code> <code>str</code> <p>The path to the PEFT file.</p> required <code>dataset_path</code> <code>str</code> <p>The path to the dataset.</p> required <code>split</code> <code>str</code> <p>The dataset split.</p> required <code>temperature</code> <code>float</code> <p>The temperature value.</p> required <code>top_p</code> <code>float</code> <p>The top-p value.</p> required <code>min_len</code> <code>int</code> <p>The minimum length of the output.</p> required <code>max_len</code> <code>int</code> <p>The maximum length of the output.</p> required <code>shard_id</code> <code>int</code> <p>The shard ID.</p> required <code>num_shards</code> <code>int</code> <p>The total number of shards.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The constructed output file path.</p> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def get_output_file(\n    output_dir,\n    model_name_or_path,\n    peft_path,\n    dataset_path,\n    split,\n    temperature,\n    top_p,\n    min_len,\n    max_len,\n    shard_id,\n    num_shards,\n):\n    \"\"\"\n    Constructs the output file path based on the provided parameters.\n\n    Args:\n        output_dir (str): The directory where the output file will be saved.\n        model_name_or_path (str): The name or path of the model.\n        peft_path (str): The path to the PEFT file.\n        dataset_path (str): The path to the dataset.\n        split (str): The dataset split.\n        temperature (float): The temperature value.\n        top_p (float): The top-p value.\n        min_len (int): The minimum length of the output.\n        max_len (int): The maximum length of the output.\n        shard_id (int): The shard ID.\n        num_shards (int): The total number of shards.\n\n    Returns:\n        str: The constructed output file path.\n    \"\"\"\n    suffix = \"\"\n    if min_len is not None:\n        suffix += f\"__min-{min_len}\"\n    if max_len is not None:\n        suffix += f\"__max-{max_len}\"\n    if shard_id is not None and num_shards is not None:\n        suffix += f\"__shard-{shard_id}-{num_shards}\"\n    if Path(dataset_path).exists():\n        dset_nickname = Path(dataset_path).name + \"__\" + split\n    else:\n        dset_nickname = dataset_path.replace(\"/\", \"__\") + \"__\" + split\n    if peft_path is not None and \"checkpoint\" in Path(peft_path).name:\n        model_nickname = Path(peft_path).parent.name + \"__\" + Path(peft_path).name\n    elif peft_path is not None:\n        model_nickname = Path(peft_path).name\n    elif Path(model_name_or_path).exists():\n        if \"checkpoint\" in Path(model_name_or_path).name:\n            model_nickname = (\n                Path(model_name_or_path).parent.name\n                + \"__\"\n                + Path(model_name_or_path).name\n            )\n        else:\n            model_nickname = Path(model_name_or_path).name\n    else:\n        model_nickname = model_name_or_path.replace(\"/\", \"__\")\n    output_file = Path(\n        output_dir,\n        dset_nickname\n        + \"__\"\n        + model_nickname\n        + \"__temp-\"\n        + str(temperature)\n        + \"__top-p-\"\n        + str(top_p)\n        + suffix\n        + \".jsonl\",\n    )\n    if not output_file.parent.exists():\n        output_file.parent.mkdir(\n            parents=True, exist_ok=True\n        )  # exists_ok=True for parallel\n    return output_file\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.load_model","title":"load_model","text":"<pre><code>load_model(model_name_or_path, peft_path)\n</code></pre> <p>Loads a base model and optionally PEFT adapters.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The name or path of the base model.</p> required <code>peft_path</code> <code>str or None</code> <p>The path to the PEFT adapters. If None, no PEFT adapters will be loaded.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>The loaded model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no device map for the specified model_name_or_path.</p> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def load_model(model_name_or_path, peft_path):\n    \"\"\"\n    Loads a base model and optionally PEFT adapters.\n\n    Args:\n        model_name_or_path (str): The name or path of the base model.\n        peft_path (str or None): The path to the PEFT adapters. If None, no PEFT adapters will be loaded.\n\n    Returns:\n        model: The loaded model.\n\n    Raises:\n        ValueError: If there is no device map for the specified model_name_or_path.\n    \"\"\"\n    logger.info(f\"Loading base model from {model_name_or_path}\")\n    max_memory = {\n        **{\n            k: f\"{torch.cuda.get_device_properties(k).total_memory // 1_010_000_000:d}GIB\"\n            for k in range(torch.cuda.device_count())\n        },\n        \"cpu\": \"20GIB\",\n    }\n    logger.info(f\"Using max memory {max_memory}\")\n    if \"-7b\" in model_name_or_path:\n        device_map = DEVICE_MAPS[\"7b\"][str(torch.cuda.device_count())]\n    elif \"-13b\" in model_name_or_path:\n        device_map = DEVICE_MAPS[\"13b\"][str(torch.cuda.device_count())]\n    elif \"-34b\" in model_name_or_path:\n        device_map = DEVICE_MAPS[\"34b\"][str(torch.cuda.device_count())]\n    else:\n        raise ValueError(f\"No device map for {model_name_or_path}\")\n    logger.info(f\"Using device_map {device_map}\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name_or_path,\n        max_memory=max_memory,\n        device_map=device_map,\n        torch_dtype=torch.bfloat16,\n    ).eval()\n    if peft_path is None:\n        logger.info(\"No PEFT adapters to load\")\n        return model\n    logger.info(f\"Loading PEFT adapters from {peft_path}\")\n    model = PeftModel.from_pretrained(\n        model,\n        peft_path,\n        device_map=device_map,\n        torch_dtype=torch.bfloat16,\n        max_memory=max_memory,\n    )\n    return model\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.load_tokenizer","title":"load_tokenizer","text":"<pre><code>load_tokenizer(model_name_or_path)\n</code></pre> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def load_tokenizer(model_name_or_path):\n    logger.info(f\"Loading tokenizer {model_name_or_path}\")\n    tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)\n    return tokenizer\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.load_data","title":"load_data","text":"<pre><code>load_data(dataset_path, split, tokenizer, min_len, max_len, model_name_or_path, peft_path, existing_ids, shard_id, num_shards)\n</code></pre> <p>Load and preprocess the dataset for model inference.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset.</p> required <code>split</code> <code>str</code> <p>The split of the dataset to load.</p> required <code>tokenizer</code> <p>The tokenizer used to tokenize the text.</p> required <code>min_len</code> <code>int</code> <p>The minimum length of input sequences to include in the dataset.</p> required <code>max_len</code> <code>int</code> <p>The maximum length of input sequences to include in the dataset.</p> required <code>model_name_or_path</code> <code>str</code> <p>The name or path of the model.</p> required <code>peft_path</code> <code>str</code> <p>The path to the PEFT file.</p> required <code>existing_ids</code> <p>The list of existing instance IDs to filter out from the dataset.</p> required <code>shard_id</code> <code>int</code> <p>The ID of the shard to load.</p> required <code>num_shards</code> <code>int</code> <p>The total number of shards.</p> required <p>Returns:</p> Name Type Description <code>dataset</code> <p>The preprocessed dataset for model inference.</p> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def load_data(\n    dataset_path,\n    split,\n    tokenizer,\n    min_len,\n    max_len,\n    model_name_or_path,\n    peft_path,\n    existing_ids,\n    shard_id,\n    num_shards,\n):\n    \"\"\"\n    Load and preprocess the dataset for model inference.\n\n    Args:\n        dataset_path (str): The path to the dataset.\n        split (str): The split of the dataset to load.\n        tokenizer: The tokenizer used to tokenize the text.\n        min_len (int): The minimum length of input sequences to include in the dataset.\n        max_len (int): The maximum length of input sequences to include in the dataset.\n        model_name_or_path (str): The name or path of the model.\n        peft_path (str): The path to the PEFT file.\n        existing_ids: The list of existing instance IDs to filter out from the dataset.\n        shard_id (int): The ID of the shard to load.\n        num_shards (int): The total number of shards.\n\n    Returns:\n        dataset: The preprocessed dataset for model inference.\n    \"\"\"\n    logger.info(f\"Loading dataset from {dataset_path}\")\n    if not Path(dataset_path).exists():\n        dataset = load_dataset(dataset_path, split=split)\n    elif Path(dataset_path, split).exists():\n        dataset = load_from_disk(Path(dataset_path) / split)\n    else:\n        dataset = load_dataset(dataset_path)[split]\n    if peft_path is not None:\n        model_nickname = \"__\".join(peft_path.split(\"/\")[-2:])\n    else:\n        model_nickname = \"__\".join(model_name_or_path.split(\"/\")[-2:])\n    if \"input_ids\" not in dataset.column_names:\n        dataset = dataset.map(\n            lambda x: tokenizer(x[\"text\"], truncation=False),\n            batched=False,\n            desc=\"tokenizing\",\n        )\n    if \"SWE-Llama\" in model_name_or_path and dataset[0][\"input_ids\"][-2:] != [13, 13]:\n        # SWE-Llama needs two exactly two newlines at the end\n        dataset = dataset.map(\n            lambda x: {\"input_ids\": x[\"input_ids\"] + [13]}, batched=False\n        )\n    filter_func = None\n    if min_len is not None and max_len is None:\n        filter_func = lambda x: x &gt;= min_len\n    elif min_len is None and max_len is not None:\n        filter_func = lambda x: x &lt; max_len\n    elif min_len is not None and max_len is not None:\n        filter_func = lambda x: min_len &lt;= x &lt; max_len\n    if filter_func is not None:\n        dataset = dataset.filter(\n            lambda x: filter_func(len(x[\"input_ids\"])), desc=\"filtering for length\"\n        )\n    lens = torch.tensor(list(map(lambda x: len(x[\"input_ids\"]), dataset)))\n    dataset = dataset.select(lens.argsort())\n    if shard_id is not None and num_shards is not None:\n        dataset = dataset.shard(num_shards, shard_id, contiguous=True)\n    dataset = dataset.filter(\n        lambda x: x[\"instance_id\"] not in existing_ids,\n        desc=\"filtering for existing ids\",\n    )\n    lens = torch.tensor(list(map(lambda x: len(x[\"input_ids\"]), dataset)))  # recompute\n    if shard_id is not None and num_shards is not None:\n        logger.info(\n            f\"filtered dataset - {len(dataset)} examples, min length: {min(lens):_}, max length: {max(lens):_} (shard {shard_id} of {num_shards})\"\n        )\n    else:\n        logger.info(\n            f\"filtered dataset - {len(dataset)} examples, min length: {min(lens):_}, max length: {max(lens):_}\"\n        )\n    return dataset\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.generate","title":"generate","text":"<pre><code>generate(model, dataset, tokenizer, temperature, top_p, fileobj, model_name_or_path, peft_path)\n</code></pre> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def generate(\n    model,\n    dataset,\n    tokenizer,\n    temperature,\n    top_p,\n    fileobj,\n    model_name_or_path,\n    peft_path,\n):\n    class RepeatingTokensCriteria(StoppingCriteria):\n        \"\"\"\n        Stopping criteria based on repeating tokens in the generated sequence.\n\n        Attributes:\n            min_length (int): The minimum length of the generated sequence.\n            min_tokens (int): The minimum number of unique tokens required in the suffix of the generated sequence.\n        \"\"\"\n\n        def __init__(self, min_length=100, min_tokens=10):\n            super().__init__()\n            self.min_length = min_length\n            self.min_tokens = min_tokens\n\n        def __call__(self, input_ids, scores, **kwargs):\n            \"\"\"\n            Check if the stopping criteria is met based on repeating tokens.\n\n            Args:\n                input_ids (torch.Tensor): The input token IDs of the generated sequence.\n                scores (torch.Tensor): The scores of the generated sequence.\n                **kwargs: Additional keyword arguments.\n\n            Returns:\n                bool: True if the stopping criteria is met, False otherwise.\n            \"\"\"\n            if input_ids[0, -1].cpu().item() == tokenizer.eos_token_id:\n                return True\n            if input_ids.shape[-1] &lt; self.min_length:\n                return False\n            suffix = input_ids[0, -self.min_length :].cpu().tolist()\n            if len(set(suffix)) &lt;= self.min_tokens:\n                return True\n            return False\n\n    stopping_criteria = StoppingCriteriaList([RepeatingTokensCriteria()])\n    fail_count = 0\n    with torch.no_grad():\n        for ix, instance in enumerate(tqdm(dataset, desc=\"Generating patches\")):\n            try:\n                input_ids = instance[\"input_ids\"]\n                input_ids = torch.tensor(\n                    [input_ids], dtype=torch.long, device=model.device\n                )\n                logger.info(f\"Processing {input_ids.shape[-1]} tokens\")\n                start = datetime.now()\n                output = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=torch.ones_like(input_ids),\n                    temperature=1.0 if temperature == 0 else temperature,\n                    top_p=top_p,\n                    do_sample=False if temperature == 0 else True,\n                    max_new_tokens=200,\n                    stopping_criteria=stopping_criteria,\n                    use_cache=False,\n                )\n                total_len = output.shape[-1]\n                output = output[0].cpu()[input_ids.shape[-1] :]\n                new_len = len(output)\n                logger.info(\n                    f\"Generated {new_len} tokens ({total_len} total) in {(datetime.now() - start).total_seconds()} \"\n                    + f\"seconds (speed: {new_len / (datetime.now() - start).total_seconds()} tps)\"\n                )\n                output = tokenizer.decode(output, skip_special_tokens=False)\n                logger.info(output[:200])\n                diff = extract_diff(output)\n                model_name_or_path += f\"__{peft_path}\" if peft_path is not None else \"\"\n                res = {\n                    \"instance_id\": instance[\"instance_id\"],\n                    \"full_output\": output,\n                    \"model_patch\": diff,\n                    \"model_name_or_path\": model_name_or_path,\n                }\n                print(json.dumps(res), file=fileobj, flush=True)\n            except Exception as e:\n                logger.exception(e)\n                print(f\"failed on {ix} with {len(input_ids)} tokens\")\n                fail_count += 1\n                if fail_count &gt;= 3:\n                    raise ValueError(\"too many failures\")\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.get_all_existing_ids","title":"get_all_existing_ids","text":"<pre><code>get_all_existing_ids(output_file)\n</code></pre> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def get_all_existing_ids(output_file):\n    stub_pattern = re.compile(\n        r\"((?:[\\w\\-\\.]+)\\_\\_temp\\-((\\d+(\\.\\d+)?)|None)\\_\\_top\\-p\\-((\\d+(\\.\\d+)?)|None))(\\_\\_|\\.jsonl)\"\n    )\n    match = stub_pattern.match(output_file.name)\n    if not output_file.exists():\n        return set()\n    if match is None:\n        raise ValueError(f\"output_file {output_file} doesn't match pattern\")\n    stub = match[1]\n    existing_ids = set()\n    output_files = list(Path(output_file.parent).glob(stub + \"*\"))\n    for filename in output_files:\n        logger.info(f\"Loading existing ids from existing {filename}\")\n        with open(filename) as f:\n            for line in f:\n                datum = json.loads(line)\n                existing_ids.add(datum[\"instance_id\"])\n    logger.info(f\"Found {len(existing_ids)} existing ids\")\n    return existing_ids\n</code></pre>"},{"location":"api/inference/#swebench.inference.run_llama.main","title":"main","text":"<pre><code>main(model_name_or_path, peft_path, dataset_path, split, temperature, top_p, output_dir, min_len, max_len, shard_id, num_shards)\n</code></pre> Source code in <code>swebench/inference/run_llama.py</code> <pre><code>def main(\n    model_name_or_path,\n    peft_path,\n    dataset_path,\n    split,\n    temperature,\n    top_p,\n    output_dir,\n    min_len,\n    max_len,\n    shard_id,\n    num_shards,\n):\n    if shard_id is not None and num_shards is None:\n        raise ValueError(\"num_shards must be specified with shard_id\")\n    if shard_id is None and num_shards is not None:\n        raise ValueError(\"shard_id must be specified with num_shards\")\n    peft_config = None\n    if peft_path is not None:\n        peft_config = PeftConfig.from_pretrained(peft_path)\n        if peft_config.base_model_name_or_path != model_name_or_path:\n            logger.warning(\n                f\"model_name_or_path {model_name_or_path} does not match peft_path base_model {peft_config.base_model_name_or_path}\"\n            )\n    output_file = get_output_file(\n        output_dir=output_dir,\n        model_name_or_path=model_name_or_path,\n        peft_path=peft_path,\n        dataset_path=dataset_path,\n        split=split,\n        temperature=temperature,\n        top_p=top_p,\n        min_len=min_len,\n        max_len=max_len,\n        shard_id=shard_id,\n        num_shards=num_shards,\n    )\n    logger.warning(f\"output_file: {output_file}\")\n    model = load_model(model_name_or_path, peft_path)\n    tokenizer = load_tokenizer(model_name_or_path)\n    existing_ids = get_all_existing_ids(output_file)\n    dataset = load_data(\n        dataset_path=dataset_path,\n        split=split,\n        tokenizer=tokenizer,\n        min_len=min_len,\n        max_len=max_len,\n        model_name_or_path=model_name_or_path,\n        peft_path=peft_path,\n        existing_ids=existing_ids,\n        shard_id=shard_id,\n        num_shards=num_shards,\n    )\n    with open(output_file, \"a\") as f:\n        generate(\n            model=model,\n            dataset=dataset,\n            tokenizer=tokenizer,\n            temperature=temperature,\n            top_p=top_p,\n            fileobj=f,\n            model_name_or_path=model_name_or_path,\n            peft_path=peft_path,\n        )\n    logger.info(\"Done\")\n</code></pre>"},{"location":"api/versioning/","title":"Versioning API","text":""},{"location":"api/versioning/#swebench.versioning","title":"swebench.versioning","text":""},{"location":"api/versioning/#swebench.versioning.constants","title":"constants","text":""},{"location":"api/versioning/#swebench.versioning.constants.MAP_REPO_TO_VERSION_PATHS","title":"MAP_REPO_TO_VERSION_PATHS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_VERSION_PATHS = {'dbt-labs/dbt-core': ['core/dbt/version.py', 'core/dbt/__init__.py'], 'django/django': ['django/__init__.py'], 'huggingface/transformers': ['src/transformers/__init__.py'], 'marshmallow-code/marshmallow': ['src/marshmallow/__init__.py'], 'mwaskom/seaborn': ['seaborn/__init__.py'], 'pallets/flask': ['src/flask/__init__.py', 'flask/__init__.py'], 'psf/requests': ['requests/__version__.py', 'requests/__init__.py'], 'pyca/cryptography': ['src/cryptography/__about__.py', 'src/cryptography/__init__.py'], 'pylint-dev/astroid': ['astroid/__pkginfo__.py', 'astroid/__init__.py'], 'pylint-dev/pylint': ['pylint/__pkginfo__.py', 'pylint/__init__.py'], 'pytest-dev/pytest': ['src/_pytest/_version.py', '_pytest/_version.py'], 'pyvista/pyvista': ['pyvista/_version.py', 'pyvista/__init__.py'], 'Qiskit/qiskit': ['qiskit/VERSION.txt'], 'scikit-learn/scikit-learn': ['sklearn/__init__.py'], 'sphinx-doc/sphinx': ['sphinx/__init__.py'], 'sympy/sympy': ['sympy/release.py', 'sympy/__init__.py']}\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.constants.MAP_REPO_TO_VERSION_PATTERNS","title":"MAP_REPO_TO_VERSION_PATTERNS  <code>module-attribute</code>","text":"<pre><code>MAP_REPO_TO_VERSION_PATTERNS = {k: ['__version__ = [\\\\\\'\"](.*)[\\\\\\'\"]', 'VERSION = \\\\((.*)\\\\)']for k in ['dbt-labs/dbt-core', 'django/django', 'huggingface/transformers', 'marshmallow-code/marshmallow', 'mwaskom/seaborn', 'pallets/flask', 'psf/requests', 'pyca/cryptography', 'pylint-dev/astroid', 'pylint-dev/pylint', 'scikit-learn/scikit-learn', 'sphinx-doc/sphinx', 'sympy/sympy']}\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.constants.SWE_BENCH_URL_RAW","title":"SWE_BENCH_URL_RAW  <code>module-attribute</code>","text":"<pre><code>SWE_BENCH_URL_RAW = 'https://raw.githubusercontent.com/'\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions","title":"get_versions","text":""},{"location":"api/versioning/#swebench.versioning.get_versions.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.INSTALL_CMD","title":"INSTALL_CMD  <code>module-attribute</code>","text":"<pre><code>INSTALL_CMD = {'pytest-dev/pytest': 'pip install -e .', 'matplotlib/matplotlib': 'python -m pip install -e .', 'pydata/xarray': 'pip install -e .'}\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser()\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.args","title":"args  <code>module-attribute</code>","text":"<pre><code>args = parse_args()\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.get_version","title":"get_version","text":"<pre><code>get_version(instance, is_build=False, path_repo=None)\n</code></pre> <p>Function for looking up the version of a task instance.</p> <p>If is_build is True, then the version is looked up by 1. building the repo at the instance's base commit, 2. activating the conda environment, and 3. looking for the version according to a predefined list of paths.</p> <p>Otherwise, the version is looked up by searching GitHub at the instance's base commit for the version according to a predefined list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>dict</code> <p>Instance to find version for</p> required <code>is_build</code> <code>bool</code> <p>Whether to build the repo and look for the version</p> <code>False</code> <code>path_repo</code> <code>str</code> <p>Path to repo to build</p> <code>None</code> <p>Returns:     str: Version text, if found</p> Source code in <code>swebench/versioning/get_versions.py</code> <pre><code>def get_version(instance, is_build=False, path_repo=None):\n    \"\"\"\n    Function for looking up the version of a task instance.\n\n    If is_build is True, then the version is looked up by 1. building the repo\n    at the instance's base commit, 2. activating the conda environment, and 3.\n    looking for the version according to a predefined list of paths.\n\n    Otherwise, the version is looked up by searching GitHub at the instance's\n    base commit for the version according to a predefined list of paths.\n\n    Args:\n        instance (dict): Instance to find version for\n        is_build (bool): Whether to build the repo and look for the version\n        path_repo (str): Path to repo to build\n    Returns:\n        str: Version text, if found\n    \"\"\"\n    keep_major_minor = lambda x, sep: \".\".join(x.strip().split(sep)[:2])\n    paths_to_version = MAP_REPO_TO_VERSION_PATHS[instance[\"repo\"]]\n    version = None\n    for path_to_version in paths_to_version:\n        init_text = None\n        if is_build and path_repo is not None:\n            version_path_abs = os.path.join(path_repo, path_to_version)\n            if os.path.exists(version_path_abs):\n                logger.info(f\"Found version file at {path_to_version}\")\n                with open(path_to_version) as f:\n                    init_text = f.read()\n        else:\n            url = os.path.join(\n                SWE_BENCH_URL_RAW,\n                instance[\"repo\"],\n                instance[\"base_commit\"],\n                path_to_version,\n            )\n            init_text = requests.get(url).text\n        version = _find_version_in_text(init_text, instance)\n        if version is not None:\n            if \".\" in version:\n                version = keep_major_minor(version, \".\")\n            if \",\" in version:\n                version = keep_major_minor(version, \",\")\n            version = re.sub(r\"[^0-9\\.]\", \"\", version)\n            return version\n    return version\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.map_version_to_task_instances","title":"map_version_to_task_instances","text":"<pre><code>map_version_to_task_instances(task_instances: list) -&gt; dict\n</code></pre> <p>Create a map of version key to list of task instances</p> <p>Parameters:</p> Name Type Description Default <code>task_instances</code> <code>list</code> <p>List of task instances</p> required <p>Returns:     dict: Map of version key to list of task instances</p> Source code in <code>swebench/versioning/get_versions.py</code> <pre><code>def map_version_to_task_instances(task_instances: list) -&gt; dict:\n    \"\"\"\n    Create a map of version key to list of task instances\n\n    Args:\n        task_instances (list): List of task instances\n    Returns:\n        dict: Map of version key to list of task instances\n    \"\"\"\n    return_map = {}\n    if \"version\" in task_instances[0]:\n        for instance in task_instances:\n            version = instance[\"version\"]\n            if version not in return_map:\n                return_map[version] = []\n            return_map[version].append(instance)\n        return return_map\n    for instance in task_instances:\n        version = get_version(instance)\n        if version not in return_map:\n            return_map[version] = []\n        return_map[version].append(instance)\n    return return_map\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.get_versions_from_build","title":"get_versions_from_build","text":"<pre><code>get_versions_from_build(data: dict)\n</code></pre> <p>Logic for looking up versions by building the repo at the instance's base commit and looking for the version according to repo-specific paths.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary of data for building a repo for any task instance in a given list.</p> required Source code in <code>swebench/versioning/get_versions.py</code> <pre><code>def get_versions_from_build(data: dict):\n    \"\"\"\n    Logic for looking up versions by building the repo at the instance's base\n    commit and looking for the version according to repo-specific paths.\n\n    Args:\n        data (dict): Dictionary of data for building a repo for any task instance\n            in a given list.\n    \"\"\"\n    data_tasks, path_repo, conda_env, path_conda, save_path = (\n        data[\"data_tasks\"],\n        data[\"path_repo\"],\n        data[\"conda_env\"],\n        data[\"path_conda\"],\n        data[\"save_path\"],\n    )\n    # Activate conda environment and set installation command\n    cmd_activate = f\"source {os.path.join(path_conda, 'bin/activate')}\"\n    cmd_source = f\"source {os.path.join(path_conda, 'etc/profile.d/conda.sh')}\"\n    cmd_install = INSTALL_CMD[data_tasks[0][\"repo\"]]\n\n    # Change directory to repo testbed\n    cwd = os.getcwd()\n    os.chdir(path_repo)\n\n    for instance in data_tasks[::-1]:\n        # Reset repo to base commit\n        subprocess.run(\n            \"git restore .\", check=True, shell=True, stdout=subprocess.DEVNULL\n        )\n        subprocess.run(\n            \"git reset HEAD .\", check=True, shell=True, stdout=subprocess.DEVNULL\n        )\n        subprocess.run(\n            \"git clean -fd\", shell=True, check=True, stdout=subprocess.DEVNULL\n        )\n        out_check = subprocess.run(\n            f\"git -c advice.detachedHead=false checkout {instance['base_commit']}\",\n            shell=True,\n            stdout=subprocess.DEVNULL,\n        )\n        if out_check.returncode != 0:\n            logger.error(f\"[{instance['instance_id']}] Checkout failed\")\n            continue\n\n        # Run installation command in repo\n        out_install = subprocess.run(\n            f\"{cmd_source}; {cmd_activate} {conda_env}; {cmd_install}\",\n            shell=True,\n            stdout=subprocess.DEVNULL,\n        )\n        if out_install.returncode != 0:\n            logger.error(f\"[{instance['instance_id']}] Installation failed\")\n            continue\n\n        # Look up version according to repo-specific paths\n        version = get_version(instance, is_build=True, path_repo=path_repo)\n        instance[\"version\"] = version\n        logger.info(f\"For instance {instance['instance_id']}, version is {version}\")\n\n    # Save results\n    with open(save_path, \"w\") as f:\n        json.dump(data_tasks, fp=f)\n    os.chdir(cwd)\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.get_versions_from_web","title":"get_versions_from_web","text":"<pre><code>get_versions_from_web(data: dict)\n</code></pre> <p>Logic for looking up versions by searching GitHub at the instance's base commit and looking for the version according to repo-specific paths.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary of data for searching GitHub for any task instance in a given list.</p> required Source code in <code>swebench/versioning/get_versions.py</code> <pre><code>def get_versions_from_web(data: dict):\n    \"\"\"\n    Logic for looking up versions by searching GitHub at the instance's base\n    commit and looking for the version according to repo-specific paths.\n\n    Args:\n        data (dict): Dictionary of data for searching GitHub for any task instance\n            in a given list.\n    \"\"\"\n    data_tasks, save_path = data[\"data_tasks\"], data[\"save_path\"]\n    version_not_found = data[\"not_found_list\"]\n    for instance in data_tasks:\n        version = get_version(instance)\n        if version is not None:\n            instance[\"version\"] = version\n            logger.info(f\"For instance {instance['instance_id']}, version is {version}\")\n        elif version_not_found is not None:\n            logger.info(f\"[{instance['instance_id']}]: version not found\")\n            version_not_found.append(instance)\n    with open(save_path, \"w\") as f:\n        json.dump(data_tasks, fp=f)\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.merge_results","title":"merge_results","text":"<pre><code>merge_results(instances_path: str, repo_prefix: str, output_dir: str = None) -&gt; int\n</code></pre> <p>Helper function for merging JSON result files generated from multiple threads.</p> <p>Parameters:</p> Name Type Description Default <code>instances_path</code> <code>str</code> <p>Path to original task instances without versions</p> required <code>repo_prefix</code> <code>str</code> <p>Prefix of result files (repo name)</p> required <code>output_dir</code> <code>str</code> <p>Path to save merged results to</p> <code>None</code> <p>Returns:     int: Number of instances in merged results</p> Source code in <code>swebench/versioning/get_versions.py</code> <pre><code>def merge_results(instances_path: str, repo_prefix: str, output_dir: str = None) -&gt; int:\n    \"\"\"\n    Helper function for merging JSON result files generated from multiple threads.\n\n    Args:\n        instances_path (str): Path to original task instances without versions\n        repo_prefix (str): Prefix of result files (repo name)\n        output_dir (str): Path to save merged results to\n    Returns:\n        int: Number of instances in merged results\n    \"\"\"\n    # Merge values from result JSON files into a single list\n    merged = []\n    for task_with_version_path in glob.glob(f\"{repo_prefix}_versions_*.json\"):\n        with open(task_with_version_path) as f:\n            task_with_version = json.load(f)\n            merged.extend(task_with_version)\n        os.remove(task_with_version_path)\n\n    # Save merged results to original task instances file's path with `_versions` suffix\n    old_path_file = instances_path.split(\"/\")[-1]\n    instances_path_new = f\"{old_path_file.split('.')[0]}_versions.json\"\n    if output_dir is not None:\n        instances_path_new = os.path.join(output_dir, instances_path_new)\n    with open(f\"{instances_path_new}\", \"w\") as f:\n        json.dump(merged, fp=f)\n    logger.info(\n        f\"Saved merged results to {instances_path_new} ({len(merged)} instances)\"\n    )\n    return len(merged)\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.get_versions.main","title":"main","text":"<pre><code>main(args)\n</code></pre> <p>Main function for looking up versions for task instances.</p> Source code in <code>swebench/versioning/get_versions.py</code> <pre><code>def main(args):\n    \"\"\"\n    Main function for looking up versions for task instances.\n    \"\"\"\n    # Get task instances + split into groups for each thread\n    data_tasks = get_instances(args.instances_path)\n    data_task_lists = split_instances(data_tasks, args.num_workers)\n    repo_prefix = data_tasks[0][\"repo\"].replace(\"/\", \"__\")\n\n    logger.info(\n        f\"Getting versions for {len(data_tasks)} instances for {data_tasks[0]['repo']}\"\n    )\n    logger.info(\n        f\"Split instances into {len(data_task_lists)} groups with lengths {[len(x) for x in data_task_lists]}\"\n    )\n\n    # If retrieval method includes GitHub, then search GitHub for versions via parallel call\n    if any([x == args.retrieval_method for x in [\"github\", \"mix\"]]):\n        manager = Manager()\n        shared_result_list = manager.list()\n        pool = Pool(processes=args.num_workers)\n        pool.map(\n            get_versions_from_web,\n            [\n                {\n                    \"data_tasks\": data_task_list,\n                    \"save_path\": f\"{repo_prefix}_versions_{i}.json\"\n                    if args.retrieval_method == \"github\"\n                    else f\"{repo_prefix}_versions_{i}_web.json\",\n                    \"not_found_list\": shared_result_list\n                    if args.retrieval_method == \"mix\"\n                    else None,\n                }\n                for i, data_task_list in enumerate(data_task_lists)\n            ],\n        )\n        pool.close()\n        pool.join()\n\n        if args.retrieval_method == \"github\":\n            # If retrieval method is just GitHub, then merge results and return\n            assert len(data_tasks) == merge_results(\n                args.instances_path, repo_prefix, args.output_dir\n            )\n            return\n        elif args.retrieval_method == \"mix\":\n            # Otherwise, remove instances that were found via GitHub from the list\n            shared_result_list = list(shared_result_list)\n            total_web = len(data_tasks) - len(shared_result_list)\n            logger.info(f\"Retrieved {total_web} versions from web\")\n            data_task_lists = split_instances(shared_result_list, args.num_workers)\n            logger.info(\n                f\"Split instances into {len(data_task_lists)} groups with lengths {[len(x) for x in data_task_lists]} for build\"\n            )\n\n    # Check that all required arguments for installing task instances are present\n    assert any([x == args.retrieval_method for x in [\"build\", \"mix\"]])\n    assert all([x in args for x in [\"testbed\", \"path_conda\", \"conda_env\"]])\n    conda_exec = os.path.join(args.path_conda, \"bin/conda\")\n\n    cwd = os.getcwd()\n    os.chdir(args.testbed)\n    for x in range(0, args.num_workers):\n        # Clone git repo per thread\n        testbed_repo_name = f\"{repo_prefix}__{x}\"\n        if not os.path.exists(testbed_repo_name):\n            logger.info(\n                f\"Creating clone of {data_tasks[0]['repo']} at {testbed_repo_name}\"\n            )\n            cmd_clone = (\n                f\"git clone git@github.com:swe-bench/{repo_prefix} {testbed_repo_name}\"\n            )\n            subprocess.run(cmd_clone, shell=True, check=True, stdout=subprocess.DEVNULL)\n        else:\n            logger.info(\n                f\"Repo for {data_tasks[0]['repo']} exists: {testbed_repo_name}; skipping...\"\n            )\n        # Clone conda environment per thread\n        conda_env_name = f\"{args.conda_env}_clone_{x}\"\n        if not os.path.exists(os.path.join(args.path_conda, \"envs\", conda_env_name)):\n            logger.info(f\"Creating clone of {args.conda_env} at {conda_env_name}\")\n            cmd_clone_env = f\"{conda_exec} create --name {conda_env_name} --clone {args.conda_env} -y\"\n            subprocess.run(\n                cmd_clone_env, shell=True, check=True, stdout=subprocess.DEVNULL\n            )\n        else:\n            logger.info(\n                f\"Conda clone for thread {x} exists: {conda_env_name}; skipping...\"\n            )\n    os.chdir(cwd)\n\n    # Create pool tasks\n    pool_tasks = []\n    for i in range(0, args.num_workers):\n        testbed_repo_name = f\"{repo_prefix}__{i}\"\n        pool_tasks.append(\n            {\n                \"data_tasks\": data_task_lists[i],\n                \"path_repo\": os.path.join(args.testbed, testbed_repo_name),\n                \"conda_env\": f\"{args.conda_env}_clone_{i}\",\n                \"path_conda\": args.path_conda,\n                \"save_path\": os.path.join(cwd, f\"{repo_prefix}_versions_{i}.json\"),\n            }\n        )\n\n    # Parallelized call\n    pool = Pool(processes=args.num_workers)\n    pool.map(get_versions_from_build, pool_tasks)\n    pool.close()\n    pool.join()\n\n    # Check that correct number of instances were versioned\n    if args.retrieval_method == \"mix\":\n        assert (\n            len(data_tasks)\n            == merge_results(args.instances_path, repo_prefix, args.output_dir)\n            + total_web\n        )\n    elif args.retrieval_method == \"build\":\n        assert len(data_tasks) == merge_results(\n            args.instances_path, repo_prefix, args.output_dir\n        )\n\n    # Remove testbed repo and conda environments\n    if args.cleanup:\n        cwd = os.getcwd()\n        os.chdir(args.testbed)\n        for x in range(0, args.num_workers):\n            # Remove git repo\n            testbed_repo_name = f\"{repo_prefix}__{x}\"\n            subprocess.run(f\"rm -rf {testbed_repo_name}\", shell=True, check=True)\n\n            # Remove conda environment\n            cmd_rm_env = (\n                f\"{conda_exec} remove --name {args.conda_env}_clone_{x} --all -y\"\n            )\n            subprocess.run(cmd_rm_env, shell=True, check=True)\n        os.chdir(cwd)\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.utils","title":"utils","text":""},{"location":"api/versioning/#swebench.versioning.utils.get_instances","title":"get_instances","text":"<pre><code>get_instances(instance_path: str) -&gt; list\n</code></pre> <p>Get task instances from given path</p> <p>Parameters:</p> Name Type Description Default <code>instance_path</code> <code>str</code> <p>Path to task instances</p> required <p>Returns:     task_instances (list): List of task instances</p> Source code in <code>swebench/versioning/utils.py</code> <pre><code>def get_instances(instance_path: str) -&gt; list:\n    \"\"\"\n    Get task instances from given path\n\n    Args:\n        instance_path (str): Path to task instances\n    Returns:\n        task_instances (list): List of task instances\n    \"\"\"\n    if any([instance_path.endswith(x) for x in [\".jsonl\", \".jsonl.all\"]]):\n        task_instances = list()\n        with open(instance_path) as f:\n            for line in f.readlines():\n                task_instances.append(json.loads(line))\n        return task_instances\n\n    with open(instance_path) as f:\n        task_instances = json.load(f)\n    return task_instances\n</code></pre>"},{"location":"api/versioning/#swebench.versioning.utils.split_instances","title":"split_instances","text":"<pre><code>split_instances(input_list: list, n: int) -&gt; list\n</code></pre> <p>Split a list into n approximately equal length sublists</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>list</code> <p>List to split</p> required <code>n</code> <code>int</code> <p>Number of sublists to split into</p> required <p>Returns:     result (list): List of sublists</p> Source code in <code>swebench/versioning/utils.py</code> <pre><code>def split_instances(input_list: list, n: int) -&gt; list:\n    \"\"\"\n    Split a list into n approximately equal length sublists\n\n    Args:\n        input_list (list): List to split\n        n (int): Number of sublists to split into\n    Returns:\n        result (list): List of sublists\n    \"\"\"\n    avg_length = len(input_list) // n\n    remainder = len(input_list) % n\n    result, start = [], 0\n\n    for i in range(n):\n        length = avg_length + 1 if i &lt; remainder else avg_length\n        sublist = input_list[start : start + length]\n        result.append(sublist)\n        start += length\n\n    return result\n</code></pre>"},{"location":"assets/collection/","title":"Collecting Evaluation Tasks for SWE-Bench","text":"<p>John Yang \u2022 November 1, 2023</p> <p>[!IMPORTANT] (03/02/2025) At this time, we are temporarily not actively supporting queries around SWE-bench task instance creation. We have ongoing efforts that will standardize and fully open source the process of generating SWE-bench task instances. These efforts will be released soon (within 1-2 months), so stay tuned!</p> <p>Therefore, we kindly request that at this time, please do not create more issues in this repository around creating new task instances.</p> <p>If you are thinking of a making a substantive, open-source research contribution that requires knowledge of SWE-bench task instance collection, please reach out to Carlos and John and include 4-5 sentences detailing what you'd like to build. In our current capacity, we've found that we're more successful in supporting such efforts when working more closely.</p> <p>If you'd like to create instances but prefer to keep these efforts private, at this time, we recommend reverting the repository to commit <code>b4a40501b4d604dea28ad03418671cc597570bb9</code>, which is the latest public version of the validation code. However, as mentioned, we are temporarily not responding to task creation questions to focus on our ongoing efforts.</p> <p>In this tutorial, we explain how to use the SWE-Bench repository to collect evaluation task instances from GitHub repositories.</p> <p>SWE-bench's collection pipeline is currently designed to target PyPI packages. We hope to expand SWE-bench to more repositories and languages in the future.</p>"},{"location":"assets/collection/#selecting-a-repository","title":"\ud83d\udd0d Selecting a Repository","text":"<p>SWE-bench constructs task instances from issues and pull requests. A good repository to source evaluation instances from should have many issues and pull requests. A point of reference for repositories that fit this bill would be the Top PyPI packages website.</p> <p>Once you've selected a repository, use the <code>/collect/make_repo/make_repo.sh</code> script to create a mirror of the repository, like so: <pre><code>./collect/make_repo/make_repo.sh scikit-learn/scikit-learn\n</code></pre></p>"},{"location":"assets/collection/#collecting-candidate-tasks","title":"\u26cf\ufe0f Collecting Candidate Tasks","text":"<p>Once you have cloned the repository, you can then use the <code>collect/get_tasks_pipeline.py</code> script to collect pull requests and convert them to candidate task instances. Supply the repository name(s) and logging folders as arguments to the <code>run_get_tasks_pipeline.sh</code> script, then run it like so: <pre><code>./collect/run_get_tasks_pipeline.sh \n</code></pre></p> <p>At this point, for a repository, you should have... * A mirror clone of the repository under the SWE-bench organization. * A <code>&lt;repo name&gt;-prs.jsonl</code> file containing all the repository's PRs. * A <code>&lt;repo name&gt;-task-instances.jsonl</code> file containing all the candidate task instances.</p>"},{"location":"assets/collection/#specify-execution-parameters","title":"\ud83d\udcd9 Specify Execution Parameters","text":"<p>This step is the most manual of all parts. To create an appropriate execution environment for task instances from a new repository, you must do the following steps: * Assign a repository-specific version (i.e. <code>1.2</code>) to every task instance. * Specify repository+version-specific installation commands in <code>harness/constants.py</code>.</p>"},{"location":"assets/collection/#part-a-versioning","title":"Part A: Versioning","text":"<p>Determining a version for each task instance can be accomplished in a number of ways, depending on the availability + feasability with respect to each repository. * Scrape from code: A version is explicitly specified in the codebase (in <code>__init__.py</code> or <code>_version.py</code> for PyPI packages). * Scrape from web: Repositories with websites (i.e. xarray.dev) have a \"Releases\" or \"What's New\" page (i.e. release page for xarray). This can be scraped for information. * Build from code: Sometimes, version-related files (i.e. <code>_version.py</code>) are purposely omitted by a developer (check <code>.gitignore</code> to verify). In this case, per task instance you can build the repository source code locally and extract the version number from the built codebase.</p> <p>Examples and technical details for each are included in <code>/versioning/</code>. Please refer to them as needed.</p>"},{"location":"assets/collection/#part-b-installation-configurations","title":"Part B: Installation Configurations","text":"<p>Per repository, you must provide installation instructions per version. In <code>constants.py</code>... 1. In <code>MAP_VERSION_TO_INSTALL</code>, declare a <code>&lt;repo owner/name&gt;: MAP_VERSION_TO_INSTALL_&lt;repo name&gt;</code> key/value pair. 2. Define a <code>MAP_VERSION_TO_INSTALL_&lt;repo name&gt;</code>, where the key is a version as a string, and the value is a dictionary of installation fields that include the following information: <pre><code>{\n    \"python\": \"3.x\", # Required\n    \"packages\": \"numpy pandas tensorflow\",\n    \"install\": \"pip install -e .\", # Required\n    \"pip_packages\": [\"pytest\"],\n}\n</code></pre> These instructions can typically be inferred from the companion website or <code>CONTRIBUTING.md</code> doc that many open source repositories have.</p>"},{"location":"assets/collection/#execution-based-validation","title":"\u2699\ufe0f Execution-based Validation","text":"<p>Congrats, you got through the trickiest part! It's smooth sailing from here on out.</p> <p>We now need to check that the task instances install properly + the problem solved by the task instance is non-trivial. This is taken care of by the <code>engine_validation.py</code> code. Run <code>./harness/run_validation.sh</code> and supply the following arguments: * <code>instances_path</code>: Path to versioned candidate task instances * <code>log_dir</code>: Path to folder to store task instance-specific execution logs * <code>temp_dir</code>: Path to directory to perform execution * <code>verbose</code>: Whether to print logging traces to standard output.</p> <p>In practice, you may have to iterate between this step and Installation Configurations a couple times. If your instructions are incorrect/under-specified, it may result in candidate task instances not being installed properly.</p>"},{"location":"assets/collection/#convert-to-task-instances","title":"\ud83d\udd04 Convert to Task Instances","text":"<p>At this point, we now have all the information necessary to determine if task instances can be used for evaluation with SWE-bench, and save them if they do.</p> <p>We provide the <code>validation.ipynb</code> Jupyter notebook provided in this folder to make the remaining steps easier. At a high level, it enables the following: * In Monitor Validation, check the results of the <code>./run_validation.sh</code> step. * In Get [FP]2[FP] Tests, determine which task instances are non-trivial (solves at least one test) * In Create Task Instances <code>.json</code> file, perform some final preprocessing and save your task instances to a <code>.json</code> file.</p> <p>Thanks for reading! If you have any questions or comments about the details in the article, please feel free to follow up with an issue.</p>"},{"location":"assets/evaluation/","title":"SWE-bench Evaluation","text":"<p>John Yang \u2022 November 6, 2023 (Updated Feb. 4, 2025)</p> <p>In this tutorial, we will explain how to evaluate models and methods using SWE-bench.</p>"},{"location":"assets/evaluation/#creating-predictions","title":"\ud83e\udd16 Creating Predictions","text":"<p>For each task instance of the SWE-bench dataset, given an issue (<code>problem_statement</code>) + codebase (<code>repo</code> + <code>base_commit</code>), your model should attempt to write a diff patch prediction. For full details on the SWE-bench task, please refer to Section 2 of the main paper.</p> <p>Each prediction must be formatted as follows: <pre><code>{\n    \"instance_id\": \"&lt;Unique task instance ID&gt;\",\n    \"model_patch\": \"&lt;.patch file content string&gt;\",\n    \"model_name_or_path\": \"&lt;Model name here (i.e. SWE-Llama-13b)&gt;\",\n}\n</code></pre></p> <p>Store multiple predictions in a <code>.json</code> file formatted as <code>[&lt;prediction 1&gt;, &lt;prediction 2&gt;,... &lt;prediction n&gt;]</code>. It is not necessary to generate predictions for every task instance.</p> <p>If you'd like examples, the swe-bench/experiments GitHub repository contains many examples of well formed patches.</p>"},{"location":"assets/evaluation/#running-evaluation","title":"\ud83d\udd04 Running Evaluation","text":"<p>Evaluate model predictions on the test split of SWE-bench Lite using the evaluation harness with the following command: <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --max_workers &lt;num_workers&gt; \\\n    --run_id &lt;run_id&gt;\n    # use --predictions_path 'gold' to verify the gold patches\n    # use --run_id to name the run, logs will be written to ./logs/run_evaluation/&lt;run_id&gt;\n    # use --split to specify which split to evaluate on, usually `dev` or `test`\n    # use --modal true to run on Modal\n</code></pre></p> <p>You can run evaluation for the following (<code>dataset_name</code>, <code>split</code>) * <code>princeton-nlp/SWE-bench_Lite</code>, <code>test</code> (300 task instances) * <code>princeton-nlp/SWE-bench_Verified</code>, <code>test</code> (500) * <code>princeton-nlp/SWE-bench</code>, <code>dev</code> (225) * <code>princeton-nlp/SWE-bench</code>, <code>test</code> (2294) * <code>princeton-nlp/SWE-bench_Multimodal</code>, <code>dev</code> (102)</p> <p>You cannot run evaluation on the <code>test</code> split of <code>princeton-nlp/SWE-bench_Multimodal</code> using this repository (517 instances). To encourage less intentional climbing of the leaderboard, we have intentionally made specifications for evaluating the test split private. Use sb-cli for SWE-bench Multimodal evaluation.</p>"},{"location":"assets/evaluation/#evaluation-with-modal","title":"\ud83c\udf29\ufe0f Evaluation with Modal","text":"<p>You can also run evaluations entirely on the cloud using Modal to avoid local setup and resource constraints: <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path gold \\\n    --run_id validate-gold-modal \\\n    --instance_ids sympy__sympy-20590 \\\n    --modal true\n</code></pre> This will execute the evaluation harness on Modal's cloud infrastructure, eliminating the need for local Docker setup and resource management.</p> <p>[!NOTE] Modal for SWE-bench Multimodal is currently experimental and may not be fully supported yet.</p>"},{"location":"guides/create_rag_datasets/","title":"Creating RAG Datasets for SWE-bench","text":"<p>This guide explains how to create custom datasets for SWE-bench with your own prompts, contexts, and tokenizers, particularly for Retrieval-Augmented Generation (RAG) approaches.</p>"},{"location":"guides/create_rag_datasets/#overview","title":"Overview","text":"<p>The <code>swebench.inference.make_datasets</code> sub-package provides tools to create and process datasets for SWE-bench:</p> Tool Purpose <code>swebench.inference.make_datasets.create_text_dataset</code> Creates a text dataset with custom prompts and context sources <code>swebench.inference.make_datasets.tokenize_dataset</code> Tokenizes datasets with various tokenizers <code>swebench.inference.make_datasets.bm25_retrieval</code> Performs BM25 retrieval on SWE-bench datasets <code>swebench.inference.make_datasets.eval_retrieval</code> Evaluates retrieval results"},{"location":"guides/create_rag_datasets/#creating-text-datasets","title":"Creating Text Datasets","text":"<p>The <code>swebench.inference.make_datasets.create_text_dataset</code> script generates text datasets from SWE-bench with specified prompts and context sources.</p>"},{"location":"guides/create_rag_datasets/#prompt-styles","title":"Prompt Styles","text":"<p>Several prompt styles are available: - <code>style-2</code> and <code>style-3</code>: Suitable for API models (only <code>style-2</code> works with SWE-Llama) - <code>full_file_gen</code>: Used for full file generation ablation - <code>style-2-edits-only</code>: Used for oracle-collapsed ablation</p>"},{"location":"guides/create_rag_datasets/#example-usage","title":"Example Usage","text":"<pre><code>export GITHUB_TOKEN=&lt;your token&gt;\npython -m swebench.inference.make_datasets.create_text_dataset \\\n    --dataset_name_or_path princeton-nlp/SWE-bench \\\n    --output_dir ./base_datasets \\\n    --prompt_style style-3 \\\n    --file_source oracle\n</code></pre>"},{"location":"guides/create_rag_datasets/#options","title":"Options","text":"<ul> <li><code>--splits</code>: Dataset splits to process (default: all)</li> <li><code>--validation_ratio</code>: Ratio of training set for validation (creates a new <code>validation</code> split from <code>train</code>)</li> <li><code>--max_context_len</code>: Maximum tokens for context</li> <li><code>--tokenizer_name</code>: Tokenizer to use for measuring context length</li> <li><code>--push_to_hub_user</code>: HF Hub username to publish dataset (requires <code>HUGGING_FACE_HUB_TOKEN</code>)</li> <li><code>--retrieval_file</code>: File with BM25 retrieval results (use with <code>--file_source bm25</code>)</li> </ul>"},{"location":"guides/create_rag_datasets/#tokenizing-datasets","title":"Tokenizing Datasets","text":"<p>The <code>swebench.inference.make_datasets.tokenize_dataset</code> script tokenizes text datasets with specified tokenizers. The script will create a new tokenized dataset in the specified output directory.</p>"},{"location":"guides/create_rag_datasets/#example-usage_1","title":"Example Usage","text":"<pre><code>python -m swebench.inference.make_datasets.tokenize_dataset \\\n    --dataset_name_or_path ./base_datasets/DATASET_NAME \\\n    --output_dir ./tokenized_datasets \\\n    --tokenizer_name llama \\\n    --num_proc 20\n</code></pre> <p>Note: The <code>cl100k</code> tokenizer does not support multiprocessing.</p>"},{"location":"guides/create_rag_datasets/#bm25-retrieval","title":"BM25 Retrieval","text":"<p>The <code>swebench.inference.make_datasets.bm25_retrieval</code> script performs BM25 retrieval on SWE-bench datasets.</p>"},{"location":"guides/create_rag_datasets/#example-usage_2","title":"Example Usage","text":"<pre><code>python -m swebench.inference.make_datasets.bm25_retrieval \\\n    --dataset_name_or_path princeton-nlp/SWE-bench \\\n    --output_dir ./retrieval_results \\\n    --splits test\n</code></pre> <p>Note: Requires the <code>pyserini</code> package. See installation instructions.</p>"},{"location":"guides/create_rag_datasets/#evaluating-retrieval-results","title":"Evaluating Retrieval Results","text":"<p>The <code>swebench.inference.make_datasets.eval_retrieval</code> script evaluates BM25 retrieval results.</p>"},{"location":"guides/create_rag_datasets/#example-usage_3","title":"Example Usage","text":"<pre><code>python -m swebench.inference.make_datasets.eval_retrieval \\\n    --dataset_name_or_path princeton-nlp/SWE-bench_bm25_13K \\\n    --split test\n</code></pre> <p>Note: This script assumes file specifications use the \"[start of filename]\" and \"[end of filename]\" tags from the default <code>DOCUMENT_ENCODING_FUNCTIONS</code> in <code>bm25_retrieval.py</code>.</p>"},{"location":"guides/datasets/","title":"SWE-bench Datasets","text":"<p>SWE-bench offers multiple datasets for evaluating language models on software engineering tasks. This guide explains the different datasets and how to use them.</p>"},{"location":"guides/datasets/#available-datasets","title":"Available Datasets","text":"<p>SWE-bench provides several dataset variants:</p> Dataset Description Size Use Case SWE-bench Full benchmark with diverse repositories 2,294 instances Comprehensive evaluation SWE-bench Lite Smaller subset for quick evaluations 534 instances Faster iteration, development SWE-bench Verified Expert-verified solvable problems 500 instances High-quality evaluation SWE-bench Multimodal Includes screenshots and UI elements 100 dev instances (500 test) Testing multimodal capabilities SWE-bench Multilingual 9 programming languages, 42 repositories 300 instances Cross-lingual evaluation"},{"location":"guides/datasets/#accessing-datasets","title":"Accessing Datasets","text":"<p>All datasets are available on Hugging Face:</p> <pre><code>from datasets import load_dataset\n\n# Load main dataset\nsbf = load_dataset('SWE-bench/SWE-bench')\n\n# Load lite variant\nsbl = load_dataset('SWE-bench/SWE-bench_Lite')\n\n# Load verified variant\nsbv = load_dataset('SWE-bench/SWE-bench_Verified', split='test')\n\n# Load multimodal variant\nsbm_dev = load_dataset('SWE-bench/SWE-bench_Multimodal', split='dev')\nsbm_test = load_dataset('SWE-bench/SWE-bench_Multimodal', split='test')\n\n# Load multilingual variant\nsbml = load_dataset('SWE-bench/SWE-bench_Multilingual', split='test')\n</code></pre>"},{"location":"guides/datasets/#dataset-structure","title":"Dataset Structure","text":"<p>Each instance in the datasets has the following structure:</p> <pre><code>{\n    \"instance_id\": \"owner__repo-pr_number\",\n    \"repo\": \"owner/repo\",\n    \"issue_id\": issue_number,\n    \"base_commit\": \"commit_hash\",\n    \"problem_statement\": \"Issue description...\",\n    \"version\": \"Repository package version\",\n    \"issue_url\": \"GitHub issue URL\",\n    \"pr_url\": \"GitHub pull request URL\",\n    \"patch\": \"Gold solution patch (don't look at this if you're trying to solve the problem)\",\n    \"test_patch\": \"Test patch\",\n    \"created_at\": \"Date of creation\",\n    \"FAIL_TO_PASS\": \"Fail to pass test cases\",\n    \"PASS_TO_PASS\": \"Pass test cases\"\n}\n</code></pre> <p>SWE-bench Verified also includes:</p> <pre><code>{\n    # ... standard fields above ...\n    \"difficulty\": \"Difficulty level\"\n}\n</code></pre> <p>The multimodal dataset also includes:</p> <pre><code>{\n    # ... standard fields above ...\n    \"image_assets\": {\n        \"problem_statement\": [\"url1\", \"url2\", ...],\n        \"patch\": [\"url1\", \"url2\", ...],\n        \"test_patch\": [\"url1\", \"url2\", ...]\n    }\n}\n</code></pre> <p>Note that for the <code>test</code> split of the multimodal dataset, the <code>patch</code>, <code>test_patch</code>, and <code>image_assets</code> fields will be empty.</p>"},{"location":"guides/datasets/#papers-retrieval-datasets","title":"Paper's Retrieval Datasets","text":"<p>For the BM25 retrieval datasets used in the SWE-bench paper, you can load the datasets as follows:</p> <pre><code># Load oracle retrieval dataset\noracle_retrieval = load_dataset('princeton-nlp/SWE-bench_oracle', split='test')\n\n# Load BM25 retrieval dataset\nsbf_bm25_13k = load_dataset('princeton-nlp/SWE-bench_bm25_13K', split='test')\nsbf_bm25_27k = load_dataset('princeton-nlp/SWE-bench_bm25_27K', split='test')\nsbf_bm25_40k = load_dataset('princeton-nlp/SWE-bench_bm25_40K', split='test')\n</code></pre>"},{"location":"guides/docker_setup/","title":"Docker Setup Guide","text":""},{"location":"guides/docker_setup/#introduction","title":"Introduction","text":"<p>SWE-bench uses a Docker-based evaluation harness to ensure consistent, reproducible results across different platforms. This containerized approach eliminates environment discrepancies and provides isolated environments for each evaluation task.</p>"},{"location":"guides/docker_setup/#prerequisites","title":"Prerequisites","text":"<p>Before setting up Docker for SWE-bench, ensure you have:</p> <ul> <li>Docker installed on your system (Docker installation guide)</li> <li>For Linux users, follow the post-installation steps</li> <li>Sufficient disk space (at least 120GB free)</li> <li>Adequate system resources (16GB+ RAM recommended)</li> </ul>"},{"location":"guides/docker_setup/#docker-installation","title":"Docker Installation","text":""},{"location":"guides/docker_setup/#macos","title":"macOS","text":"<ol> <li>Download and install Docker Desktop for Mac from the official website</li> <li>Increase resource allocation in Docker Desktop settings:    - Open Docker Desktop preferences    - Go to Resources &gt; Advanced    - Allocate at least 8 CPUs and 16GB RAM    - Set disk image size to at least 120GB</li> </ol>"},{"location":"guides/docker_setup/#linux","title":"Linux","text":"<ol> <li>Install Docker using your distribution's package manager or follow the official guide</li> <li>Add your user to the docker group to run Docker without sudo:    <pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker  # Apply changes without logging out\n</code></pre></li> </ol>"},{"location":"guides/docker_setup/#windows","title":"Windows","text":"<ol> <li>Install Docker Desktop for Windows from the official website</li> <li>Ensure WSL 2 is installed and configured</li> <li>Increase resource allocation in Docker Desktop settings:    - Open Docker Desktop settings    - Go to Resources &gt; Advanced    - Allocate at least 8 CPUs and 16GB RAM    - Set disk image size to at least 120GB</li> </ol>"},{"location":"guides/docker_setup/#testing-your-docker-installation","title":"Testing Your Docker Installation","text":"<p>Verify your Docker installation with these commands:</p> <pre><code># Check Docker version\ndocker --version\n\n# Run a simple test container\ndocker run hello-world\n\n# Check available disk space\ndocker system df\n</code></pre>"},{"location":"guides/docker_setup/#docker-resource-management","title":"Docker Resource Management","text":""},{"location":"guides/docker_setup/#understanding-swe-benchs-docker-usage","title":"Understanding SWE-bench's Docker Usage","text":"<p>The SWE-bench evaluation harness builds Docker images in three layers:</p> <ol> <li>Base image: Common dependencies for all evaluations</li> <li>Environment images: Python environments for different configurations (~60 images)</li> <li>Instance images: Specific dependencies for each evaluation task</li> </ol> <p>These images require significant disk space, so it's important to understand how to manage them.</p>"},{"location":"guides/docker_setup/#resource-management-commands","title":"Resource Management Commands","text":"<p>Useful commands for managing Docker resources:</p> <pre><code># View Docker disk usage\ndocker system df\n\n# Remove unused containers\ndocker container prune\n\n# Remove unused images\ndocker image prune\n\n# Remove all unused Docker objects (containers, images, networks, volumes)\ndocker system prune\n\n# Remove all stopped containers\ndocker container prune\n\n# Remove all dangling images\ndocker image prune\n</code></pre>"},{"location":"guides/docker_setup/#cache-level-configuration","title":"Cache Level Configuration","text":"<p>SWE-bench provides different caching options to balance speed vs. storage:</p> Cache Level Description Storage Impact Performance <code>none</code> No image caching Minimal (~120GB during run) Slowest <code>base</code> Cache only base image Minimal (~120GB during run) Slow <code>env</code> (default) Cache base and environment images Moderate (~100GB) Moderate <code>instance</code> Cache all images High (~2,000GB) Fastest <p>Set the cache level when running the evaluation:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --cache_level env \\\n    --clean True\n</code></pre> <p>For most users, the default <code>env</code> setting provides a good balance between evaluation speed and disk usage.</p>"},{"location":"guides/docker_setup/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/docker_setup/#setting-the-right-number-of-workers","title":"Setting the Right Number of Workers","text":"<p>The optimal number of workers depends on your system resources:</p> <ul> <li>Use fewer than <code>min(0.75 * os.cpu_count(), 24)</code> workers</li> <li>For an 8-core machine, 6 workers is typically appropriate</li> <li>For a 16-core machine, 12 workers is typically appropriate</li> </ul> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --max_workers 8\n</code></pre> <p>Increasing worker count beyond your system's capabilities can actually slow down evaluation due to resource contention.</p>"},{"location":"guides/docker_setup/#troubleshooting-docker-issues","title":"Troubleshooting Docker Issues","text":""},{"location":"guides/docker_setup/#common-problems-and-solutions","title":"Common Problems and Solutions","text":"<ol> <li> <p>Insufficient disk space:    - Free up disk space or increase Docker Desktop's disk image size    - Use <code>--cache_level=env</code> or <code>--cache_level=base</code> to reduce storage needs</p> </li> <li> <p>Docker build failures:    - Check network connectivity    - Inspect build logs in <code>logs/build_images</code></p> </li> <li> <p>Permission issues:    - Ensure your user is in the docker group (Linux)    - Run with elevated privileges if necessary</p> </li> <li> <p>Slow evaluation times:    - Reduce the number of parallel workers    - Check CPU and memory usage during evaluation    - Consider using a more powerful machine</p> </li> <li> <p>Network-related issues:    - Check Docker network settings:      <pre><code>docker network ls\ndocker network inspect bridge\n</code></pre></p> </li> </ol>"},{"location":"guides/docker_setup/#cleaning-up-after-evaluation","title":"Cleaning Up After Evaluation","text":"<p>To reclaim disk space after running evaluations:</p> <pre><code># Remove all unused Docker resources\ndocker system prune -a\n\n# Or for more control, remove specific resources\ndocker container prune  # Remove all stopped containers\ndocker image prune      # Remove unused images\n</code></pre> <p>You can also set <code>--clean=True</code> when running the evaluation to automatically clean up instance-specific resources. </p>"},{"location":"guides/evaluation/","title":"Evaluation Guide","text":"<p>This guide explains how to evaluate model predictions on SWE-bench tasks.</p>"},{"location":"guides/evaluation/#overview","title":"Overview","text":"<p>SWE-bench evaluates models by applying their generated patches to real-world repositories and running the repository's tests to verify if the issue is resolved. The evaluation is performed in a containerized Docker environment to ensure consistent results across different platforms.</p>"},{"location":"guides/evaluation/#basic-evaluation","title":"Basic Evaluation","text":"<p>The main entry point for evaluation is the <code>swebench.harness.run_evaluation</code> module:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --max_workers 8 \\\n    --run_id my_evaluation_run\n</code></pre>"},{"location":"guides/evaluation/#evaluation-on-swe-bench-lite","title":"Evaluation on SWE-bench Lite","text":"<p>For beginners, we recommend starting with SWE-bench Lite, which is a smaller subset of SWE-bench:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --max_workers 8 \\\n    --run_id my_first_evaluation\n</code></pre>"},{"location":"guides/evaluation/#evaluation-on-the-full-swe-bench","title":"Evaluation on the Full SWE-bench","text":"<p>Once you're familiar with the evaluation process, you can evaluate on the full SWE-bench dataset:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --max_workers 12 \\\n    --run_id full_evaluation\n</code></pre>"},{"location":"guides/evaluation/#prediction-format","title":"Prediction Format","text":"<p>Your predictions should be in JSONL format, with each line containing a JSON object with the following fields:</p> <pre><code>{\n  \"instance_id\": \"repo_owner__repo_name-issue_number\",\n  \"model_name_or_path\": \"your-model-name\",\n  \"model_patch\": \"the patch content as a string\"\n}\n</code></pre> <p>Example:</p> <pre><code>{\"instance_id\": \"sympy__sympy-20590\", \"model_name_or_path\": \"gpt-4\", \"model_patch\": \"diff --git a/sympy/core/sympify.py b/sympy/core/sympify.py\\nindex 6a73a83..fb90e1a 100644\\n--- a/sympy/core/sympify.py\\n+++ b/sympy/core/sympify.py\\n@@ -508,7 +508,7 @@ def sympify(a, locals=None, convert_xor=True, strict=False, rational=False,\\n         converter[type(a)],\\n         (SympifyError,\\n          OverflowError,\\n-         ValueError)):\\n+         ValueError, AttributeError)):\\n     return a\\n\"}\n</code></pre>"},{"location":"guides/evaluation/#cloud-based-evaluation","title":"Cloud-Based Evaluation","text":"<p>You can run SWE-bench evaluations in the cloud using Modal:</p> <pre><code># Install Modal\npip install modal swebench[modal]\n\n# Set up Modal (first time only)\nmodal setup\n\n# Run evaluation on Modal\npython -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --parallelism 10 \\\n    --modal true\n</code></pre>"},{"location":"guides/evaluation/#running-with-sb-cli","title":"Running with sb-cli","text":"<p>For a simpler cloud evaluation experience, you can use the sb-cli tool:</p> <pre><code># Install sb-cli\npip install sb-cli\n\n# Authenticate (first time only)\nsb login\n\n# Submit evaluation\nsb submit --predictions &lt;path_to_predictions&gt;\n</code></pre>"},{"location":"guides/evaluation/#advanced-evaluation-options","title":"Advanced Evaluation Options","text":""},{"location":"guides/evaluation/#evaluating-specific-instances","title":"Evaluating Specific Instances","text":"<p>To evaluate only specific instances, use the <code>--instance_ids</code> parameter:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --instance_ids httpie-cli__httpie-1088,sympy__sympy-20590 \\\n    --max_workers 2\n</code></pre>"},{"location":"guides/evaluation/#controlling-cache-usage","title":"Controlling Cache Usage","text":"<p>Control how Docker images are cached between runs with the <code>--cache_level</code> parameter:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --cache_level env \\  # Options: none, base, env, instance\n    --max_workers 8\n</code></pre>"},{"location":"guides/evaluation/#cleaning-up-resources","title":"Cleaning Up Resources","text":"<p>To automatically clean up resources after evaluation, use the <code>--clean</code> parameter:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path &lt;path_to_predictions&gt; \\\n    --clean True\n</code></pre>"},{"location":"guides/evaluation/#understanding-evaluation-results","title":"Understanding Evaluation Results","text":"<p>The evaluation results are stored in the <code>evaluation_results</code> directory, with subdirectories for each run. Each run produces:</p> <ul> <li><code>results.json</code>: Overall evaluation metrics</li> <li><code>instance_results.jsonl</code>: Detailed results for each instance</li> <li><code>run_logs/</code>: Logs for each evaluation instance</li> </ul> <p>Key metrics include:</p> <ul> <li>Total instances: Number of instances in the dataset</li> <li>Instances submitted: Number of instances the model attempted</li> <li>Instances completed: Number of instances that completed evaluation</li> <li>Instances resolved: Number of instances where the patch fixed the issue</li> <li>Resolution rate: Percentage of submitted instances successfully resolved</li> </ul>"},{"location":"guides/evaluation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during evaluation:</p> <ol> <li>Check the Docker setup (Docker Setup Guide)</li> <li>Verify that your predictions file is correctly formatted</li> <li>Examine the log files in <code>logs/</code> for error messages</li> <li>Try reducing the number of workers with <code>--max_workers</code></li> <li>Increase available disk space or use <code>--cache_level=base</code> to reduce storage needs</li> </ol>"},{"location":"guides/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with SWE-bench, from installation to running your first evaluation.</p>"},{"location":"guides/quickstart/#setup","title":"Setup","text":"<p>First, install SWE-bench and its dependencies:</p> <pre><code>git clone https://github.com/princeton-nlp/SWE-bench.git\ncd SWE-bench\npip install -e .\n</code></pre> <p>Make sure Docker is installed and running on your system.</p>"},{"location":"guides/quickstart/#accessing-the-datasets","title":"Accessing the Datasets","text":"<p>SWE-bench datasets are available on Hugging Face:</p> <pre><code>from datasets import load_dataset\n\n# Load the main SWE-bench dataset\nswebench = load_dataset('princeton-nlp/SWE-bench', split='test')\n\n# Load other variants as needed\nswebench_lite = load_dataset('princeton-nlp/SWE-bench_Lite', split='test')\nswebench_verified = load_dataset('princeton-nlp/SWE-bench_Verified', split='test')\nswebench_multimodal_dev = load_dataset('princeton-nlp/SWE-bench_Multimodal', split='dev')\nswebench_multimodal_test = load_dataset('princeton-nlp/SWE-bench_Multimodal', split='test')\nswebench_multilingual = load_dataset('princeton-nlp/SWE-bench_Multilingual', split='test')\n</code></pre>"},{"location":"guides/quickstart/#running-a-basic-evaluation","title":"Running a Basic Evaluation","text":"<p>To evaluate an LLM's performance on SWE-bench:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --num_workers 4 \\\n    --predictions_path &lt;path_to_predictions&gt;\n</code></pre>"},{"location":"guides/quickstart/#check-that-your-evaluation-setup-is-correct","title":"Check That Your Evaluation Setup Is Correct","text":"<p>To verify that your evaluation setup is correct, you can evaluate the ground truth (\"gold\") patches:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --max_workers 1 \\\n    --instance_ids sympy__sympy-20590 \\\n    --predictions_path gold \\\n    --run_id validate-gold\n</code></pre>"},{"location":"guides/quickstart/#using-swe-llama-models","title":"Using SWE-Llama Models","text":"<p>SWE-bench provides specialized models for software engineering tasks:</p> <pre><code>python -m swebench.inference.run_llama \\\n    --model_name_or_path princeton-nlp/SWE-Llama-13b \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --max_instances 10 \\\n    --output_dir &lt;path_to_output&gt;\n</code></pre>"},{"location":"guides/quickstart/#using-the-docker-environment-directly","title":"Using the Docker Environment Directly","text":"<p>SWE-bench uses Docker containers for consistent evaluation environments:</p> <pre><code>from swebench.harness.docker_build import build_instance_image\nfrom swebench.harness.docker_utils import exec_run_with_timeout\n\n# Build a Docker image for a specific instance\ninstance_id = \"httpie-cli/httpie#1088\"\nimage_tag = build_instance_image(instance_id)\n\n# Run commands in the container\nresult = exec_run_with_timeout(\n    container_name=f\"container_{instance_id}\",\n    image_tag=image_tag,\n    cmd=\"cd /workspace &amp;&amp; pytest\",\n    timeout=300\n)\n</code></pre>"},{"location":"guides/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about running evaluations with more advanced options</li> <li>Explore dataset options to find the right fit for your needs</li> <li>Try cloud-based evaluations for large-scale testing</li> </ul>"},{"location":"other_languages/README_CN/","title":"README CN","text":"| [\u65e5\u672c\u8a9e](README_JP.md) | [English](https://github.com/swe-bench/SWE-bench) | [\u4e2d\u6587\u7b80\u4f53](README_CN.md) | [\u4e2d\u6587\u7e41\u9ad4](README_TW.md) |   <p> \u60a8\u53ef\u4ee5\u5728\u6211\u4eec\u7684ICLR 2024\u7684\u8bba\u6587\u300aSWE-bench: Can Language Models Resolve Real-World GitHub Issues?\u300b\u4e2d\u627e\u5230\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e      </p> <p>\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u7f51\u7ad9\u67e5\u770b\u516c\u5171\u6392\u884c\u699c\uff0c\u5e76\u67e5\u770b\u66f4\u6539\u65e5\u5fd7\u4ee5\u83b7\u53d6\u6709\u5173 SWE-bench \u57fa\u51c6\u6700\u65b0\u66f4\u65b0\u7684\u4fe1\u606f\u3002</p>"},{"location":"other_languages/README_CN/#_1","title":"\ud83d\udd0d \u76f8\u5173\u9879\u76ee","text":"<p>\u67e5\u770bSWE-bench\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5176\u4ed6\u9879\u76ee\uff01</p>"},{"location":"other_languages/README_CN/#_2","title":"\ud83d\udc4b \u6982\u8ff0","text":"<p>SWE-bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u95ee\u9898\u4e0a\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5,\u8fd9\u4e9b\u95ee\u9898\u6536\u96c6\u81eaGitHub\u3002 \u7ed9\u5b9a\u4e00\u4e2a\u4ee3\u7801\u5e93\u548c\u4e00\u4e2a\u95ee\u9898,\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u662f\u751f\u6210\u4e00\u4e2a\u8865\u4e01\u6765\u89e3\u51b3\u63cf\u8ff0\u7684\u95ee\u9898\u3002</p> <p></p> <p>\u8981\u8bbf\u95eeSWE-bench,\u590d\u5236\u5e76\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801: <pre><code>from datasets import load_dataset\nswebench = load_dataset('princeton-nlp/SWE-bench', split='test')\n</code></pre></p>"},{"location":"other_languages/README_CN/#_3","title":"\ud83d\ude80 \u8bbe\u7f6e","text":"<p>SWE-bench\u4f7f\u7528Docker\u8fdb\u884c\u53ef\u91cd\u73b0\u7684\u8bc4\u4f30\u3002 \u6309\u7167Docker\u5b89\u88c5\u6307\u5357\u4e2d\u7684\u8bf4\u660e\u5728\u4f60\u7684\u673a\u5668\u4e0a\u5b89\u88c5Docker\u3002 \u5982\u679c\u4f60\u5728Linux\u4e0a\u8fdb\u884c\u8bbe\u7f6e,\u6211\u4eec\u5efa\u8bae\u4f60\u4e5f\u67e5\u770b\u5b89\u88c5\u540e\u6b65\u9aa4\u3002</p> <p>\u6700\u540e,\u8981\u4ece\u6e90\u4ee3\u7801\u6784\u5efaSWE-bench,\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c: <pre><code>git clone git@github.com:princeton-nlp/SWE-bench.git\ncd SWE-bench\npip install -e .\n</code></pre></p> <p>\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6d4b\u8bd5\u4f60\u7684\u5b89\u88c5: <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path gold \\\n    --max_workers 1 \\\n    --instance_ids sympy__sympy-20590 \\\n    --run_id validate-gold\n</code></pre></p>"},{"location":"other_languages/README_CN/#_4","title":"\ud83d\udcbd \u4f7f\u7528","text":"<p>[!\u8b66\u544a] \u5728SWE-bench\u4e0a\u8fd0\u884c\u5feb\u901f\u8bc4\u4f30\u53ef\u80fd\u4f1a\u6d88\u8017\u5927\u91cf\u8d44\u6e90 \u6211\u4eec\u5efa\u8bae\u5728\u4e00\u53f0\u5177\u6709\u81f3\u5c11120GB\u53ef\u7528\u5b58\u50a8\u7a7a\u95f4\u300116GB RAM\u548c8\u4e2aCPU\u6838\u5fc3\u7684<code>x86_64</code>\u673a\u5668\u4e0a\u8fd0\u884c\u8bc4\u4f30\u5de5\u5177\u3002 \u4f60\u53ef\u80fd\u9700\u8981\u5c1d\u8bd5\u8c03\u6574<code>--max_workers</code>\u53c2\u6570\u4ee5\u627e\u5230\u9002\u5408\u4f60\u673a\u5668\u7684\u6700\u4f73\u5de5\u4f5c\u8fdb\u7a0b\u6570,\u4f46\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u5c11\u4e8e<code>min(0.75 * os.cpu_count(), 24)</code>\u7684\u6570\u503c\u3002</p> <p>\u5982\u679c\u4f7f\u7528docker desktop\u8fd0\u884c,\u8bf7\u786e\u4fdd\u589e\u52a0\u4f60\u7684\u865a\u62df\u78c1\u76d8\u7a7a\u95f4\u4ee5\u6709\u7ea6120GB\u7684\u53ef\u7528\u7a7a\u95f4,\u5e76\u6839\u636e\u4e0a\u8ff0\u5efa\u8bae\u4e3adocker\u8bbe\u7f6e\u53ef\u7528\u7684CPU\u6570\u91cf\u6765\u8bbe\u7f6emax_workers\u3002</p> <p>\u5bf9<code>arm64</code>\u673a\u5668\u7684\u652f\u6301\u4ecd\u5904\u4e8e\u5b9e\u9a8c\u9636\u6bb5\u3002</p> <p>\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u901a\u8fc7\u8bc4\u4f30\u5de5\u5177\u5728SWE-bench Lite\u4e0a\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b: <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;\u9884\u6d4b\u7ed3\u679c\u8def\u5f84&gt; \\\n    --max_workers &lt;\u5de5\u4f5c\u8fdb\u7a0b\u6570&gt; \\\n    --run_id &lt;\u8fd0\u884cID&gt;\n    # \u4f7f\u7528 --predictions_path 'gold' \u6765\u9a8c\u8bc1\u9ec4\u91d1\u8865\u4e01\n    # \u4f7f\u7528 --run_id \u6765\u547d\u540d\u8bc4\u4f30\u8fd0\u884c\n    # \u4f7f\u7528 --modal true \u6765\u5728 Modal \u4e0a\u4f7f\u7528\n</code></pre></p> <p>\u8fd9\u4e2a\u547d\u4ee4\u5c06\u5728\u5f53\u524d\u76ee\u5f55\u4e2d\u751f\u6210docker\u6784\u5efa\u65e5\u5fd7(<code>logs/build_images</code>)\u548c\u8bc4\u4f30\u65e5\u5fd7(<code>logs/run_evaluation</code>)\u3002</p> <p>\u6700\u7ec8\u7684\u8bc4\u4f30\u7ed3\u679c\u5c06\u5b58\u50a8\u5728<code>evaluation_results</code>\u76ee\u5f55\u4e2d\u3002</p> <p>\u8981\u67e5\u770b\u8bc4\u4f30\u5de5\u5177\u7684\u5b8c\u6574\u53c2\u6570\u5217\u8868,\u8bf7\u8fd0\u884c: <pre><code>python -m swebench.harness.run_evaluation --help\n</code></pre></p> <p>\u6b64\u5916,SWE-Bench\u4ed3\u5e93\u8fd8\u53ef\u4ee5\u5e2e\u52a9\u4f60: * \u5728\u6211\u4eec\u9884\u5904\u7406\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4f60\u81ea\u5df1\u7684\u6a21\u578b * \u5728\u73b0\u6709\u6a21\u578b\u4e0a\u8fd0\u884c\u63a8\u7406(\u65e0\u8bba\u662f\u4f60\u672c\u5730\u7684\u6a21\u578b\u5982LLaMA,\u8fd8\u662f\u4f60\u901a\u8fc7API\u8bbf\u95ee\u7684\u6a21\u578b\u5982GPT-4)\u3002\u63a8\u7406\u6b65\u9aa4\u662f\u6307\u7ed9\u5b9a\u4e00\u4e2a\u4ed3\u5e93\u548c\u4e00\u4e2a\u95ee\u9898,\u8ba9\u6a21\u578b\u5c1d\u8bd5\u751f\u6210\u4fee\u590d\u65b9\u6848\u3002 * \u5728\u4f60\u81ea\u5df1\u7684\u4ed3\u5e93\u4e0a\u8fd0\u884cSWE-bench\u7684\u6570\u636e\u6536\u96c6\u7a0b\u5e8f,\u4ee5\u521b\u5efa\u65b0\u7684SWE-Bench\u4efb\u52a1\u3002</p>"},{"location":"other_languages/README_CN/#_5","title":"\u2b07\ufe0f \u4e0b\u8f7d","text":"\u6570\u636e\u96c6 \u6a21\u578b \ud83e\udd17 SWE-bench \ud83e\udd99 SWE-Llama 13b \ud83e\udd17 \"Oracle\" \u68c0\u7d22 \ud83e\udd99 SWE-Llama 13b (PEFT) \ud83e\udd17 BM25 \u68c0\u7d22 13K \ud83e\udd99 SWE-Llama 7b \ud83e\udd17 BM25 \u68c0\u7d22 27K \ud83e\udd99 SWE-Llama 7b (PEFT) \ud83e\udd17 BM25 \u68c0\u7d22 40K \ud83e\udd17 BM25 \u68c0\u7d22 50K (Llama tokens)"},{"location":"other_languages/README_CN/#_6","title":"\ud83c\udf4e \u6559\u7a0b","text":"<p>\u6211\u4eec\u8fd8\u7f16\u5199\u4e86\u4ee5\u4e0b\u535a\u5ba2\u6587\u7ae0,\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528SWE-bench\u7684\u4e0d\u540c\u90e8\u5206\u3002 \u5982\u679c\u4f60\u60f3\u770b\u5230\u5173\u4e8e\u7279\u5b9a\u4e3b\u9898\u7684\u6587\u7ae0,\u8bf7\u901a\u8fc7issue\u544a\u8bc9\u6211\u4eec\u3002 * [2023\u5e7411\u67081\u65e5] \u4e3aSWE-Bench\u6536\u96c6\u8bc4\u4f30\u4efb\u52a1 (\ud83d\udd17) * [2023\u5e7411\u67086\u65e5] \u5728SWE-bench\u4e0a\u8fdb\u884c\u8bc4\u4f30 (\ud83d\udd17)</p>"},{"location":"other_languages/README_CN/#_7","title":"\ud83d\udcab \u8d21\u732e","text":"<p>\u6211\u4eec\u5f88\u4e50\u610f\u542c\u53d6\u66f4\u5e7f\u6cdb\u7684NLP\u3001\u673a\u5668\u5b66\u4e60\u548c\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u793e\u533a\u7684\u610f\u89c1,\u6211\u4eec\u6b22\u8fce\u4efb\u4f55\u8d21\u732e\u3001\u62c9\u53d6\u8bf7\u6c42\u6216\u95ee\u9898\uff01 \u8981\u8fd9\u6837\u505a,\u8bf7\u63d0\u4ea4\u65b0\u7684\u62c9\u53d6\u8bf7\u6c42\u6216\u95ee\u9898,\u5e76\u76f8\u5e94\u5730\u586b\u5199\u5bf9\u5e94\u7684\u6a21\u677f\u3002\u6211\u4eec\u4f1a\u5c3d\u5feb\u8ddf\u8fdb\uff01</p> <p>\u8054\u7cfb\u4eba: Carlos E. Jimenez \u548c John Yang (\u90ae\u7bb1: carlosej@princeton.edu, johnby@stanford.edu)\u3002</p>"},{"location":"other_languages/README_CN/#_8","title":"\u270d\ufe0f \u5f15\u7528","text":"<p>\u5982\u679c\u4f60\u89c9\u5f97\u6211\u4eec\u7684\u5de5\u4f5c\u6709\u5e2e\u52a9,\u8bf7\u4f7f\u7528\u4ee5\u4e0b\u5f15\u7528\u3002 <pre><code>@inproceedings{\n    jimenez2024swebench,\n    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},\n    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=VTF8yNQM66}\n}\n</code></pre></p>"},{"location":"other_languages/README_CN/#_9","title":"\ud83e\udeaa \u8bb8\u53ef\u8bc1","text":"<p>MIT\u3002\u67e5\u770b<code>LICENSE.md</code>\u3002</p>"},{"location":"other_languages/README_JP/","title":"README JP","text":"<p> paste.txt <p> </p>    | [\u65e5\u672c\u8a9e](README_JP.md) | [English](https://github.com/swe-bench/SWE-bench) | [\u4e2d\u6587\u7b80\u4f53](README_CN.md) | [\u4e2d\u6587\u7e41\u9ad4](README_TW.md) |   <p> ICLR 2024 \u306e\u8ad6\u6587 SWE-bench: Can Language Models Resolve Real-World GitHub Issues? \u306e\u30b3\u30fc\u30c9\u3068\u30c7\u30fc\u30bf      </p> <p>\u30d1\u30d6\u30ea\u30c3\u30af\u30ea\u30fc\u30c0\u30fc\u30dc\u30fc\u30c9\u306f\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3092\u3001SWE-bench \u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u6700\u65b0\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u60c5\u5831\u306f change log \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"other_languages/README_JP/#_1","title":"\ud83d\udd0d \u95a2\u9023\u30d7\u30ed\u30b8\u30a7\u30af\u30c8","text":"<p>SWE-bench\u30a8\u30b3\u30b7\u30b9\u30c6\u30e0\u306e\u4ed6\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3082\u30c1\u30a7\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\uff01</p>"},{"location":"other_languages/README_JP/#_2","title":"\ud83d\udc4b \u6982\u8981","text":"<p>SWE-bench\u306f\u3001GitHub\u304b\u3089\u53ce\u96c6\u3057\u305f\u5b9f\u4e16\u754c\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u554f\u984c\u306b\u5bfe\u3059\u308b\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u306e\u305f\u3081\u306e\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3067\u3059\u3002 \u30b3\u30fc\u30c9\u30d9\u30fc\u30b9\u3068\u8ab2\u984c\u304c\u4e0e\u3048\u3089\u308c\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u306f\u8a18\u8ff0\u3055\u308c\u305f\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u30d1\u30c3\u30c1\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p> <p></p> <p>SWE-bench\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3057\u3066\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\uff1a <pre><code>from datasets import load_dataset\nswebench = load_dataset('princeton-nlp/SWE-bench', split='test')\n</code></pre></p>"},{"location":"other_languages/README_JP/#_3","title":"\ud83d\ude80 \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7","text":"<p>SWE-bench\u306f\u518d\u73fe\u53ef\u80fd\u306a\u8a55\u4fa1\u306e\u305f\u3081\u306bDocker\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 Docker\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u30ac\u30a4\u30c9\u306e\u6307\u793a\u306b\u5f93\u3063\u3066\u3001\u30de\u30b7\u30f3\u306bDocker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Linux\u3067\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\u5834\u5408\u306f\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c\u306e\u624b\u9806\u3082\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002</p> <p>\u6700\u5f8c\u306b\u3001SWE-bench\u3092\u30bd\u30fc\u30b9\u304b\u3089\u30d3\u30eb\u30c9\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u306b\u5f93\u3063\u3066\u304f\u3060\u3055\u3044\uff1a <pre><code>git clone git@github.com:princeton-nlp/SWE-bench.git\ncd SWE-bench\npip install -e .\n</code></pre></p> <p>\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u30c6\u30b9\u30c8\u3057\u3066\u304f\u3060\u3055\u3044\uff1a <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path gold \\\n    --max_workers 1 \\\n    --instance_ids sympy__sympy-20590 \\\n    --run_id validate-gold\n</code></pre></p>"},{"location":"other_languages/README_JP/#_4","title":"\ud83d\udcbd \u4f7f\u7528\u65b9\u6cd5","text":"<p>[!\u8b66\u544a] SWE-bench\u3067\u306e\u9ad8\u901f\u8a55\u4fa1\u306e\u5b9f\u884c\u306f\u30ea\u30bd\u30fc\u30b9\u96c6\u7d04\u7684\u3067\u3042\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059 \u8a55\u4fa1\u30cf\u30fc\u30cd\u30b9\u306f\u3001\u5c11\u306a\u304f\u3068\u3082120GB\u306e\u7a7a\u304d\u5bb9\u91cf\u300116GB\u306eRAM\u30018 CPU\u30b3\u30a2\u3092\u6301\u3064<code>x86_64</code>\u30de\u30b7\u30f3\u3067\u5b9f\u884c\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002 <code>--max_workers</code>\u5f15\u6570\u3092\u8abf\u6574\u3057\u3066\u3001\u30de\u30b7\u30f3\u306b\u6700\u9069\u306a\u30ef\u30fc\u30ab\u30fc\u6570\u3092\u898b\u3064\u3051\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001<code>min(0.75 * os.cpu_count(), 24)</code>\u672a\u6e80\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002</p> <p>docker desktop\u3067\u5b9f\u884c\u3059\u308b\u5834\u5408\u306f\u3001\u4eee\u60f3\u30c7\u30a3\u30b9\u30af\u5bb9\u91cf\u3092\u5897\u3084\u3057\u3066\u7d04120GB\u306e\u7a7a\u304d\u5bb9\u91cf\u3092\u78ba\u4fdd\u3057\u3001\u4e0a\u8a18\u306b\u5f93\u3063\u3066docker\u3067\u5229\u7528\u53ef\u80fd\u306aCPU\u6570\u306b\u5408\u308f\u305b\u3066max_workers\u3092\u8a2d\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p><code>arm64</code>\u30de\u30b7\u30f3\u306e\u30b5\u30dd\u30fc\u30c8\u306f\u5b9f\u9a13\u7684\u3067\u3059\u3002</p> <p>\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u8a55\u4fa1\u30cf\u30fc\u30cd\u30b9\u3067SWE-bench Lite\u306e\u30e2\u30c7\u30eb\u4e88\u6e2c\u3092\u8a55\u4fa1\u3057\u307e\u3059\uff1a <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;\u4e88\u6e2c\u7d50\u679c\u306e\u30d1\u30b9&gt; \\\n    --max_workers &lt;\u30ef\u30fc\u30ab\u30fc\u6570&gt; \\\n    --run_id &lt;\u5b9f\u884cID&gt;\n    # \u30b4\u30fc\u30eb\u30c9\u30d1\u30c3\u30c1\u3092\u691c\u8a3c\u3059\u308b\u306b\u306f --predictions_path 'gold' \u3092\u4f7f\u7528\n    # --run_id \u3067\u8a55\u4fa1\u5b9f\u884c\u306b\u540d\u524d\u3092\u4ed8\u3051\u308b\n    # --modal true Modal \u3067\u4f7f\u7528\u3059\u308b\n</code></pre></p> <p>\u3053\u306e\u30b3\u30de\u30f3\u30c9\u306f\u3001\u73fe\u5728\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306bdocker\u30d3\u30eb\u30c9\u30ed\u30b0\uff08<code>logs/build_images</code>\uff09\u3068\u8a55\u4fa1\u30ed\u30b0\uff08<code>logs/run_evaluation</code>\uff09\u3092\u751f\u6210\u3057\u307e\u3059\u3002</p> <p>\u6700\u7d42\u7684\u306a\u8a55\u4fa1\u7d50\u679c\u306f<code>evaluation_results</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002</p> <p>\u8a55\u4fa1\u30cf\u30fc\u30cd\u30b9\u306e\u5f15\u6570\u306e\u5b8c\u5168\u306a\u30ea\u30b9\u30c8\u3092\u78ba\u8a8d\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\uff1a <pre><code>python -m swebench.harness.run_evaluation --help\n</code></pre></p> <p>\u3055\u3089\u306b\u3001SWE-Bench\u30ea\u30dd\u30b8\u30c8\u30ea\u306f\u4ee5\u4e0b\u306e\u3053\u3068\u3092\u652f\u63f4\u3067\u304d\u307e\u3059\uff1a * \u4e8b\u524d\u51e6\u7406\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u72ec\u81ea\u306e\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b * \u65e2\u5b58\u306e\u30e2\u30c7\u30eb\uff08\u30c7\u30a3\u30b9\u30af\u4e0a\u306eLLaMA\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u3084\u3001API\u3092\u901a\u3058\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u306aGPT-4\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\uff09\u3067\u63a8\u8ad6\u3092\u5b9f\u884c\u3059\u308b\u3002\u63a8\u8ad6\u30b9\u30c6\u30c3\u30d7\u306f\u3001\u30ea\u30dd\u30b8\u30c8\u30ea\u3068\u8ab2\u984c\u304c\u4e0e\u3048\u3089\u308c\u3001\u30e2\u30c7\u30eb\u304c\u305d\u308c\u306b\u5bfe\u3059\u308b\u4fee\u6b63\u3092\u751f\u6210\u3057\u3088\u3046\u3068\u3059\u308b\u3068\u3053\u308d\u3067\u3059\u3002 * \u72ec\u81ea\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3067SWE-bench\u306e\u30c7\u30fc\u30bf\u53ce\u96c6\u624b\u9806\u3092\u5b9f\u884c\u3057\u3001\u65b0\u3057\u3044SWE-Bench\u30bf\u30b9\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002</p>"},{"location":"other_languages/README_JP/#_5","title":"\u2b07\ufe0f \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u30e2\u30c7\u30eb \ud83e\udd17 SWE-bench \ud83e\udd99 SWE-Llama 13b \ud83e\udd17 \"Oracle\" \u691c\u7d22 \ud83e\udd99 SWE-Llama 13b (PEFT) \ud83e\udd17 BM25 \u691c\u7d22 13K \ud83e\udd99 SWE-Llama 7b \ud83e\udd17 BM25 \u691c\u7d22 27K \ud83e\udd99 SWE-Llama 7b (PEFT) \ud83e\udd17 BM25 \u691c\u7d22 40K \ud83e\udd17 BM25 \u691c\u7d22 50K (Llama\u30c8\u30fc\u30af\u30f3)"},{"location":"other_languages/README_JP/#_6","title":"\ud83c\udf4e \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb","text":"<p>SWE-bench\u306e\u69d8\u3005\u306a\u90e8\u5206\u306e\u4f7f\u7528\u65b9\u6cd5\u306b\u95a2\u3059\u308b\u4ee5\u4e0b\u306e\u30d6\u30ed\u30b0\u8a18\u4e8b\u3082\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 \u7279\u5b9a\u306e\u30c8\u30d4\u30c3\u30af\u306b\u95a2\u3059\u308b\u8a18\u4e8b\u3092\u898b\u305f\u3044\u5834\u5408\u306f\u3001issue\u3092\u901a\u3058\u3066\u304a\u77e5\u3089\u305b\u304f\u3060\u3055\u3044\u3002 * [2023\u5e7411\u67081\u65e5] SWE-Bench\u306e\u8a55\u4fa1\u30bf\u30b9\u30af\u306e\u53ce\u96c6 (\ud83d\udd17) * [2023\u5e7411\u67086\u65e5] SWE-bench\u3067\u306e\u8a55\u4fa1 (\ud83d\udd17)</p>"},{"location":"other_languages/README_JP/#_7","title":"\ud83d\udcab \u8ca2\u732e","text":"<p>\u3088\u308a\u5e83\u7bc4\u306aNLP\u3001\u6a5f\u68b0\u5b66\u7fd2\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u7814\u7a76\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u304b\u3089\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u6b53\u8fce\u3057\u3001\u3042\u3089\u3086\u308b\u8ca2\u732e\u3001\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3001\u307e\u305f\u306f\u554f\u984c\u63d0\u8d77\u3092\u6b53\u8fce\u3057\u307e\u3059\uff01 \u305d\u306e\u305f\u3081\u306b\u306f\u3001\u65b0\u3057\u3044\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u307e\u305f\u306f\u554f\u984c\u3092\u63d0\u51fa\u3057\u3001\u5bfe\u5fdc\u3059\u308b\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306b\u8a18\u5165\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3059\u3050\u306b\u30d5\u30a9\u30ed\u30fc\u30a2\u30c3\u30d7\u3044\u305f\u3057\u307e\u3059\uff01</p> <p>\u9023\u7d61\u5148: Carlos E. Jimenez \u304a\u3088\u3073 John Yang (\u30e1\u30fc\u30eb: carlosej@princeton.edu, johnby@stanford.edu)</p>"},{"location":"other_languages/README_JP/#_8","title":"\u270d\ufe0f \u5f15\u7528","text":"<p>\u79c1\u305f\u3061\u306e\u7814\u7a76\u304c\u5f79\u7acb\u3064\u3068\u611f\u3058\u305f\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u5f15\u7528\u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002 <pre><code>@inproceedings{\n    jimenez2024swebench,\n    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},\n    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=VTF8yNQM66}\n}\n</code></pre></p>"},{"location":"other_languages/README_JP/#_9","title":"\ud83e\udeaa \u30e9\u30a4\u30bb\u30f3\u30b9","text":"<p>MIT\u3002<code>LICENSE.md</code>\u3092\u3054\u78ba\u8a8d\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"other_languages/README_TW/","title":"README TW","text":"| [\u65e5\u672c\u8a9e](README_JP.md) | [English](https://github.com/swe-bench/SWE-bench) | [\u4e2d\u6587\u7b80\u4f53](README_CN.md) | [\u4e2d\u6587\u7e41\u9ad4](README_TW.md) |   <p> \u4f60\u53ef\u4ee5\u5728\u6211\u5011\u7684ICLR 2024\u7684\u8ad6\u6587\u300aSWE-bench: Can Language Models Resolve Real-World GitHub Issues?\u300b\u4e2d\u627e\u5230\u6211\u5011\u7684\u4ee3\u78bc\u548c\u6578\u64da      </p> <p>\u8acb\u8a2a\u554f\u6211\u5011\u7684\u7db2\u7ad9\u67e5\u770b\u516c\u5171\u6392\u884c\u699c\uff0c\u4e26\u67e5\u770b\u66f4\u6539\u65e5\u8a8c\u4ee5\u7372\u53d6\u6709\u95dc SWE-bench \u57fa\u6e96\u6700\u65b0\u66f4\u65b0\u7684\u4fe1\u606f\u3002</p>"},{"location":"other_languages/README_TW/#_1","title":"\ud83d\udd0d \u76f8\u95dc\u9805\u76ee","text":"<p>\u67e5\u770bSWE-bench\u751f\u614b\u7cfb\u7d71\u4e2d\u7684\u5176\u4ed6\u9805\u76ee\uff01</p>"},{"location":"other_languages/README_TW/#_2","title":"\ud83d\udc4b \u6982\u8ff0","text":"<p>SWE-bench\u662f\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u771f\u5be6\u4e16\u754c\u8edf\u9ad4\u554f\u984c\u4e0a\u8868\u73fe\u7684\u57fa\u6e96\u6e2c\u8a66,\u9019\u4e9b\u554f\u984c\u6536\u96c6\u81eaGitHub\u3002 \u7d66\u5b9a\u4e00\u500b\u7a0b\u5f0f\u78bc\u5eab\u548c\u4e00\u500b\u554f\u984c,\u8a9e\u8a00\u6a21\u578b\u7684\u4efb\u52d9\u662f\u751f\u6210\u4e00\u500b\u88dc\u4e01\u4f86\u89e3\u6c7a\u63cf\u8ff0\u7684\u554f\u984c\u3002</p> <p></p> <p>\u8981\u8a2a\u554fSWE-bench,\u8907\u88fd\u4e26\u904b\u884c\u4ee5\u4e0b\u7a0b\u5f0f\u78bc: <pre><code>from datasets import load_dataset\nswebench = load_dataset('princeton-nlp/SWE-bench', split='test')\n</code></pre></p>"},{"location":"other_languages/README_TW/#_3","title":"\ud83d\ude80 \u8a2d\u7f6e","text":"<p>SWE-bench\u4f7f\u7528Docker\u9032\u884c\u53ef\u91cd\u73fe\u7684\u8a55\u4f30\u3002 \u6309\u7167Docker\u5b89\u88dd\u6307\u5357\u4e2d\u7684\u8aaa\u660e\u5728\u4f60\u7684\u6a5f\u5668\u4e0a\u5b89\u88ddDocker\u3002 \u5982\u679c\u4f60\u5728Linux\u4e0a\u9032\u884c\u8a2d\u7f6e,\u6211\u5011\u5efa\u8b70\u4f60\u4e5f\u67e5\u770b\u5b89\u88dd\u5f8c\u6b65\u9a5f\u3002</p> <p>\u6700\u5f8c,\u8981\u5f9e\u6e90\u4ee3\u78bc\u69cb\u5efaSWE-bench,\u8acb\u6309\u7167\u4ee5\u4e0b\u6b65\u9a5f\u64cd\u4f5c: <pre><code>git clone git@github.com:princeton-nlp/SWE-bench.git\ncd SWE-bench\npip install -e .\n</code></pre></p> <p>\u901a\u904e\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u6e2c\u8a66\u4f60\u7684\u5b89\u88dd: <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path gold \\\n    --max_workers 1 \\\n    --instance_ids sympy__sympy-20590 \\\n    --run_id validate-gold\n</code></pre></p>"},{"location":"other_languages/README_TW/#_4","title":"\ud83d\udcbd \u4f7f\u7528","text":"<p>[!\u8b66\u544a] \u5728SWE-bench\u4e0a\u904b\u884c\u5feb\u901f\u8a55\u4f30\u53ef\u80fd\u6703\u6d88\u8017\u5927\u91cf\u8cc7\u6e90 \u6211\u5011\u5efa\u8b70\u5728\u4e00\u53f0\u5177\u6709\u81f3\u5c11120GB\u53ef\u7528\u5b58\u5132\u7a7a\u9593\u300116GB RAM\u548c8\u500bCPU\u6838\u5fc3\u7684<code>x86_64</code>\u6a5f\u5668\u4e0a\u904b\u884c\u8a55\u4f30\u5de5\u5177\u3002 \u4f60\u53ef\u80fd\u9700\u8981\u5617\u8a66\u8abf\u6574<code>--max_workers</code>\u53c3\u6578\u4ee5\u627e\u5230\u9069\u5408\u4f60\u6a5f\u5668\u7684\u6700\u4f73\u5de5\u4f5c\u9032\u7a0b\u6578,\u4f46\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u5c11\u65bc<code>min(0.75 * os.cpu_count(), 24)</code>\u7684\u6578\u503c\u3002</p> <p>\u5982\u679c\u4f7f\u7528docker desktop\u904b\u884c,\u8acb\u78ba\u4fdd\u589e\u52a0\u4f60\u7684\u865b\u64ec\u78c1\u76e4\u7a7a\u9593\u4ee5\u6709\u7d04120GB\u7684\u53ef\u7528\u7a7a\u9593,\u4e26\u6839\u64da\u4e0a\u8ff0\u5efa\u8b70\u70badocker\u8a2d\u7f6e\u53ef\u7528\u7684CPU\u6578\u91cf\u4f86\u8a2d\u7f6emax_workers\u3002</p> <p>\u5c0d<code>arm64</code>\u6a5f\u5668\u7684\u652f\u6301\u4ecd\u8655\u65bc\u5be6\u9a57\u968e\u6bb5\u3002</p> <p>\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u901a\u904e\u8a55\u4f30\u5de5\u5177\u5728SWE-bench Lite\u4e0a\u8a55\u4f30\u6a21\u578b\u9810\u6e2c: <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path &lt;\u9810\u6e2c\u7d50\u679c\u8def\u5f91&gt; \\\n    --max_workers &lt;\u5de5\u4f5c\u9032\u7a0b\u6578&gt; \\\n    --run_id &lt;\u904b\u884cID&gt;\n    # \u4f7f\u7528 --predictions_path 'gold' \u4f86\u9a57\u8b49\u9ec3\u91d1\u88dc\u4e01\n    # \u4f7f\u7528 --run_id \u4f86\u547d\u540d\u8a55\u4f30\u904b\u884c\n    # \u4f7f\u7528 --modal true \u6765\u5728 Modal \u4e0a\u4f7f\u7528\n</code></pre></p> <p>\u9019\u500b\u547d\u4ee4\u5c07\u5728\u7576\u524d\u76ee\u9304\u4e2d\u751f\u6210docker\u69cb\u5efa\u65e5\u8a8c(<code>logs/build_images</code>)\u548c\u8a55\u4f30\u65e5\u8a8c(<code>logs/run_evaluation</code>)\u3002</p> <p>\u6700\u7d42\u7684\u8a55\u4f30\u7d50\u679c\u5c07\u5b58\u5132\u5728<code>evaluation_results</code>\u76ee\u9304\u4e2d\u3002</p> <p>\u8981\u67e5\u770b\u8a55\u4f30\u5de5\u5177\u7684\u5b8c\u6574\u53c3\u6578\u5217\u8868,\u8acb\u904b\u884c: <pre><code>python -m swebench.harness.run_evaluation --help\n</code></pre></p> <p>\u6b64\u5916,SWE-Bench\u5132\u5b58\u5eab\u9084\u53ef\u4ee5\u5e6b\u52a9\u4f60: * \u5728\u6211\u5011\u9810\u8655\u7406\u7684\u6578\u64da\u96c6\u4e0a\u8a13\u7df4\u4f60\u81ea\u5df1\u7684\u6a21\u578b * \u5728\u73fe\u6709\u6a21\u578b\u4e0a\u904b\u884c\u63a8\u7406(\u7121\u8ad6\u662f\u4f60\u672c\u5730\u7684\u6a21\u578b\u5982LLaMA,\u9084\u662f\u4f60\u901a\u904eAPI\u8a2a\u554f\u7684\u6a21\u578b\u5982GPT-4)\u3002\u63a8\u7406\u6b65\u9a5f\u662f\u6307\u7d66\u5b9a\u4e00\u500b\u5132\u5b58\u5eab\u548c\u4e00\u500b\u554f\u984c,\u8b93\u6a21\u578b\u5617\u8a66\u751f\u6210\u4fee\u5fa9\u65b9\u6848\u3002 * \u5728\u4f60\u81ea\u5df1\u7684\u5132\u5b58\u5eab\u4e0a\u904b\u884cSWE-bench\u7684\u6578\u64da\u6536\u96c6\u7a0b\u5e8f,\u4ee5\u5275\u5efa\u65b0\u7684SWE-Bench\u4efb\u52d9\u3002</p>"},{"location":"other_languages/README_TW/#_5","title":"\u2b07\ufe0f \u4e0b\u8f09","text":"\u6578\u64da\u96c6 \u6a21\u578b \ud83e\udd17 SWE-bench \ud83e\udd99 SWE-Llama 13b \ud83e\udd17 \"Oracle\" \u6aa2\u7d22 \ud83e\udd99 SWE-Llama 13b (PEFT) \ud83e\udd17 BM25 \u6aa2\u7d22 13K \ud83e\udd99 SWE-Llama 7b \ud83e\udd17 BM25 \u6aa2\u7d22 27K \ud83e\udd99 SWE-Llama 7b (PEFT) \ud83e\udd17 BM25 \u6aa2\u7d22 40K \ud83e\udd17 BM25 \u6aa2\u7d22 50K (Llama tokens)"},{"location":"other_languages/README_TW/#_6","title":"\ud83c\udf4e \u6559\u7a0b","text":"<p>\u6211\u5011\u9084\u7de8\u5beb\u4e86\u4ee5\u4e0b\u535a\u5ba2\u6587\u7ae0,\u4ecb\u7d39\u5982\u4f55\u4f7f\u7528SWE-bench\u7684\u4e0d\u540c\u90e8\u5206\u3002 \u5982\u679c\u4f60\u60f3\u770b\u5230\u95dc\u65bc\u7279\u5b9a\u4e3b\u984c\u7684\u6587\u7ae0,\u8acb\u901a\u904eissue\u544a\u8a34\u6211\u5011\u3002 * [2023\u5e7411\u67081\u65e5] \u70baSWE-Bench\u6536\u96c6\u8a55\u4f30\u4efb\u52d9 (\ud83d\udd17) * [2023\u5e7411\u67086\u65e5] \u5728SWE-bench\u4e0a\u9032\u884c\u8a55\u4f30 (\ud83d\udd17)</p>"},{"location":"other_languages/README_TW/#_7","title":"\ud83d\udcab \u8ca2\u737b","text":"<p>\u6211\u5011\u5f88\u6a02\u610f\u807d\u53d6\u66f4\u5ee3\u6cdb\u7684NLP\u3001\u6a5f\u5668\u5b78\u7fd2\u548c\u8edf\u9ad4\u5de5\u7a0b\u7814\u7a76\u793e\u5340\u7684\u610f\u898b,\u6211\u5011\u6b61\u8fce\u4efb\u4f55\u8ca2\u737b\u3001\u62c9\u53d6\u8acb\u6c42\u6216\u554f\u984c\uff01 \u8981\u9019\u6a23\u505a,\u8acb\u63d0\u4ea4\u65b0\u7684\u62c9\u53d6\u8acb\u6c42\u6216\u554f\u984c,\u4e26\u76f8\u61c9\u5730\u586b\u5beb\u5c0d\u61c9\u7684\u6a21\u677f\u3002\u6211\u5011\u6703\u76e1\u5feb\u8ddf\u9032\uff01</p> <p>\u806f\u7e6b\u4eba: Carlos E. Jimenez \u548c John Yang (\u90f5\u7bb1: carlosej@princeton.edu, johnby@stanford.edu)\u3002</p>"},{"location":"other_languages/README_TW/#_8","title":"\u270d\ufe0f \u5f15\u7528","text":"<p>\u5982\u679c\u4f60\u89ba\u5f97\u6211\u5011\u7684\u5de5\u4f5c\u6709\u5e6b\u52a9,\u8acb\u4f7f\u7528\u4ee5\u4e0b\u5f15\u7528\u3002 <pre><code>@inproceedings{\n    jimenez2024swebench,\n    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},\n    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=VTF8yNQM66}\n}\n</code></pre></p>"},{"location":"other_languages/README_TW/#_9","title":"\ud83e\udeaa \u8a31\u53ef\u8b49","text":"<p>MIT\u3002\u67e5\u770b<code>LICENSE.md</code>\u3002</p>"},{"location":"reference/harness/","title":"Evaluation Harness Reference","text":"<p>This section documents the Docker-based evaluation harness for SWE-bench.</p>"},{"location":"reference/harness/#overview","title":"Overview","text":"<p>The SWE-bench evaluation harness uses Docker containers to create reproducible environments for evaluating model-generated patches on software repositories. This containerized approach ensures consistent evaluation results across different platforms and eliminates environmental discrepancies.</p> <p>The <code>swebench.harness</code> module provides the main evaluation infrastructure for SWE-bench tasks. This module is responsible for setting up Docker environments, applying patches, running tests, and determining if the patches resolve issues.</p>"},{"location":"reference/harness/#core-components","title":"Core Components","text":"<p>There are two important scripts in the <code>swebench.harness</code> module:</p> <ul> <li><code>swebench.harness.prepare_images</code>: This script can be used to build Docker images for task instances</li> <li><code>swebench.harness.run_evaluation</code>: This script is used to run the evaluation of a prediction file</li> </ul> <p>In general, you don't need to run <code>prepare_images</code> unless you're specifically interested in building Docker images locally. <code>run_evaluation</code> is the main script you'll use to run evaluations for all task instances in the SWE-bench datasets.</p>"},{"location":"reference/harness/#architecture","title":"Architecture","text":"<p>The harness operates in three main layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Instance Images       \u2502  Problem-specific configurations\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Environment Images    \u2502  Repository-specific dependencies\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Base Images           \u2502  Language and tooling support\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This layered architecture allows for efficient caching and reuse of common components while ensuring isolated evaluation for each task.</p>"},{"location":"reference/harness/#evaluation-process","title":"Evaluation Process","text":"<p>The evaluation process follows these steps:</p> <ol> <li>Setup: Prepare Docker images for each instance</li> <li>Patch Application: Apply the model-generated patch to the codebase</li> <li>Test Execution: Run the repository's test suite</li> <li>Grading: Determine if the patch resolves the issue</li> <li>Reporting: Calculate metrics and generate reports</li> </ol>"},{"location":"reference/harness/#usage","title":"Usage","text":"<p>The main entry point for the evaluation harness is the <code>swebench.harness.run_evaluation</code> module.</p> <pre><code>python -m swebench.harness.run_evaluation --help\n</code></pre>"},{"location":"reference/harness/#basic-example","title":"Basic Example","text":"<pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Lite \\\n    --predictions_path /path/to/predictions.jsonl \\\n    --max_workers 8 \\\n    --run_id my_evaluation_run\n</code></pre>"},{"location":"reference/harness/#verifying-gold-patches","title":"Verifying Gold Patches","text":"<p>To verify the gold patches (ground truth solutions):</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path gold \\\n    --max_workers 1 \\\n    --instance_ids sympy__sympy-20590 \\\n    --run_id validate-gold\n</code></pre>"},{"location":"reference/harness/#cloud-based-evaluation","title":"Cloud-based Evaluation","text":"<p>The harness supports cloud-based evaluations using Modal:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --predictions_path gold \\\n    --max_workers 1 \\\n    --instance_ids sympy__sympy-20590 \\\n    --run_id validate-gold\n    --modal true\n</code></pre> <p>You can also use sb-cli, a tool for running evaluations automatically on AWS.</p>"},{"location":"reference/harness/#parameters","title":"Parameters","text":"<p>The harness accepts various parameters to configure the evaluation:</p> <ul> <li><code>--dataset_name</code>: The name of the dataset to evaluate on</li> <li><code>--split</code>: Split of the dataset</li> <li><code>--predictions_path</code>: Path to the predictions file (or \"gold\" for ground truth)</li> <li><code>--max_workers</code>: Number of parallel evaluation workers</li> <li><code>--run_id</code>: Identifier for the evaluation run</li> <li><code>--cache_level</code>: Level of caching for Docker images</li> <li><code>--clean</code>: Whether to clean up resources after evaluation</li> <li><code>--instance_ids</code>: Specific instances to evaluate (comma-separated)</li> <li><code>--open_file_limit</code>: Open file limit</li> <li><code>--force_rebuild</code>: Force rebuild all images</li> <li><code>--log_level</code>: Logging verbosity</li> <li><code>--namespace</code>: Namespace for images</li> <li><code>--timeout</code>: Maximum time (seconds) for evaluating each instance</li> <li><code>--modal</code>: If true, run on Modal</li> </ul> <p>For a complete list of arguments, run: <pre><code>python -m swebench.harness.run_evaluation --help\n</code></pre></p>"},{"location":"reference/harness/#cache-levels","title":"Cache Levels","text":"<p>The <code>--cache_level</code> parameter controls how Docker images are cached between runs:</p> Level Description Storage Impact Speed <code>none</code> No caching Minimal (~120GB during run) Slowest <code>base</code> Cache only base image Minimal (~120GB during run) Slow <code>env</code> (default) Cache base and environment images Moderate (~100GB) Moderate <code>instance</code> Cache all images High (~2,000GB) Fastest <p>Most users should use the default <code>env</code> level, which provides a good balance between speed and storage usage.</p>"},{"location":"reference/harness/#dataset-options","title":"Dataset Options","text":"<p>The harness supports evaluation on various SWE-bench datasets:</p> <ul> <li>SWE-bench: The complete benchmark (<code>princeton-nlp/SWE-bench</code>)</li> <li>SWE-bench Lite: A smaller subset for faster evaluation (<code>princeton-nlp/SWE-bench_Lite</code>)</li> <li>SWE-bench Verified: 500 problems verified by software engineers (<code>princeton-nlp/SWE-bench_Verified</code>)</li> <li>SWE-bench Multimodal: Tasks involving visual software domains (<code>princeton-nlp/SWE-bench_Multimodal</code>)</li> </ul>"},{"location":"reference/harness/#resource-requirements","title":"Resource Requirements","text":"<p>The SWE-bench evaluation harness has the following resource requirements:</p> <ul> <li>Storage: At least 120GB free space (for any cache level)</li> <li>Memory: At least 16GB RAM recommended</li> <li>CPU: 8+ cores recommended</li> <li>Platform: x86_64 architecture recommended (arm64 support is experimental)</li> </ul>"},{"location":"reference/harness/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Evaluation times for SWE-bench Lite (on a typical machine):</p> Configuration Time (cache level = env) Time (cache level = instance) 16-core, max_workers=12 ~30 minutes ~15 minutes 8-core, max_workers=6 ~50 minutes ~15 minutes"},{"location":"reference/harness/#choosing-max_workers","title":"Choosing <code>max_workers</code>","text":"<p>The optimal number of workers depends on your system resources. We recommend:</p> <ul> <li>Use fewer than <code>min(0.75 * os.cpu_count(), 24)</code> workers</li> <li>For Docker Desktop, adjust based on CPUs available to Docker</li> <li>Overly high worker counts can slow down evaluation due to resource contention</li> </ul>"},{"location":"reference/harness/#log-files","title":"Log Files","text":"<p>The harness generates logs in two directories:</p> <ul> <li><code>logs/build_images</code>: Docker build logs</li> <li><code>logs/run_evaluation</code>: Evaluation run logs</li> </ul> <p>Final evaluation results are stored in the <code>evaluation_results</code> directory.</p>"},{"location":"reference/harness/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Docker Space Issues: If Docker runs out of disk space, prune unused images and containers with <code>docker system prune -a</code></li> <li>Build Failures: Check the build logs in <code>logs/build_images</code> for specific error details</li> <li>Evaluation Failures: Examine logs in <code>logs/run_evaluation</code> to diagnose test failures</li> <li>Resource Constraints: Reduce <code>--max_workers</code> if you experience memory pressure or system slowdowns</li> <li>Check Docker installation and permissions</li> <li>Verify sufficient disk space</li> </ol>"},{"location":"reference/inference/","title":"Inference API Reference","text":"<p>This section documents the various tools available for running inference with SWE-bench datasets.</p>"},{"location":"reference/inference/#overview","title":"Overview","text":"<p>The inference module provides tools to generate model completions for SWE-bench tasks using: - API-based models (OpenAI, Anthropic) - Local models (SWE-Llama) - Live inference on open GitHub issues</p> <p>In particular, we provide the following important scripts and sub-packages:</p> <ul> <li><code>make_datasets</code>: Contains scripts to generate new datasets for SWE-bench inference with your own prompts and issues</li> <li><code>run_api.py</code>: Generates completions using API models (OpenAI, Anthropic) for a given dataset</li> <li><code>run_llama.py</code>: Runs inference using Llama models (e.g., SWE-Llama)</li> <li><code>run_live.py</code>: Generates model completions for new issues on GitHub in real time</li> </ul>"},{"location":"reference/inference/#installation","title":"Installation","text":"<p>Depending on your inference needs, you can install different dependency sets:</p> <pre><code># For dataset generation and API-based inference\npip install -e \".[datasets]\"\n\n# For local model inference (requires GPU with CUDA)\npip install -e \".[inference]\"\n</code></pre>"},{"location":"reference/inference/#available-tools","title":"Available Tools","text":""},{"location":"reference/inference/#dataset-generation-make_datasets","title":"Dataset Generation (<code>make_datasets</code>)","text":"<p>This package contains scripts to generate new datasets for SWE-bench inference with custom prompts and issues. The datasets follow the format required for SWE-bench evaluation.</p> <p>For detailed usage instructions, see the Make Datasets Guide.</p>"},{"location":"reference/inference/#running-api-inference-run_apipy","title":"Running API Inference (<code>run_api.py</code>)","text":"<p>This script runs inference on a dataset using either the OpenAI or Anthropic API. It sorts instances by length and continually writes outputs to a specified file, so the script can be stopped and restarted without losing progress.</p> <pre><code># Example with Anthropic Claude\nexport ANTHROPIC_API_KEY=&lt;your key&gt;\npython -m swebench.inference.run_api \\\n    --dataset_name_or_path princeton-nlp/SWE-bench_oracle \\\n    --model_name_or_path claude-2 \\\n    --output_dir ./outputs\n</code></pre>"},{"location":"reference/inference/#parameters","title":"Parameters","text":"<ul> <li><code>--dataset_name_or_path</code>: HuggingFace dataset name or local path</li> <li><code>--model_name_or_path</code>: Model name (e.g., \"gpt-4\", \"claude-2\")</li> <li><code>--output_dir</code>: Directory to save model outputs</li> <li><code>--split</code>: Dataset split to use (default: \"test\")</li> <li><code>--shard_id</code>, <code>--num_shards</code>: To process only a portion of data</li> <li><code>--model_args</code>: Comma-separated key=value pairs (e.g., \"temperature=0.2,top_p=0.95\")</li> <li><code>--max_cost</code>: Maximum spending limit for API calls</li> </ul>"},{"location":"reference/inference/#running-local-inference-run_llamapy","title":"Running Local Inference (<code>run_llama.py</code>)","text":"<p>This script is similar to <code>run_api.py</code> but designed to run inference using Llama models locally. You can use it with SWE-Llama or other compatible models.</p> <pre><code>python -m swebench.inference.run_llama \\\n    --dataset_path princeton-nlp/SWE-bench_oracle \\\n    --model_name_or_path princeton-nlp/SWE-Llama-13b \\\n    --output_dir ./outputs \\\n    --temperature 0\n</code></pre>"},{"location":"reference/inference/#parameters_1","title":"Parameters","text":"<ul> <li><code>--dataset_path</code>: HuggingFace dataset name or local path</li> <li><code>--model_name_or_path</code>: Local or HuggingFace model path</li> <li><code>--output_dir</code>: Directory to save model outputs</li> <li><code>--split</code>: Dataset split to use (default: \"test\")</li> <li><code>--shard_id</code>, <code>--num_shards</code>: For processing only a portion of data</li> <li><code>--temperature</code>: Sampling temperature (default: 0)</li> <li><code>--top_p</code>: Top-p sampling parameter (default: 1)</li> <li><code>--peft_path</code>: Path to PEFT adapter</li> </ul>"},{"location":"reference/inference/#live-inference-on-github-issues-run_livepy","title":"Live Inference on GitHub Issues (<code>run_live.py</code>)","text":"<p>This tool allows you to apply SWE-bench models to real, open GitHub issues. It can be used to test models on new, unseen issues without the need for manual dataset creation.</p> <pre><code>export OPENAI_API_KEY=&lt;your key&gt;\npython -m swebench.inference.run_live \\\n    --model_name gpt-3.5-turbo-1106 \\\n    --issue_url https://github.com/huggingface/transformers/issues/26706\n</code></pre>"},{"location":"reference/inference/#prerequisites","title":"Prerequisites","text":"<p>For live inference, you'll need to install additional dependencies: - Pyserini: For BM25 retrieval - Faiss: For vector search</p> <p>Follow the installation instructions on their respective GitHub repositories: - Pyserini: Installation Guide - Faiss: Installation Guide</p>"},{"location":"reference/inference/#output-format","title":"Output Format","text":"<p>All inference scripts produce outputs in a format compatible with the SWE-bench evaluation harness. The output contains the model's generated patch for each issue, which can then be evaluated using the evaluation harness.</p>"},{"location":"reference/inference/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>When running inference on large datasets, use sharding to split the workload</li> <li>For API models, monitor costs carefully and set appropriate <code>--max_cost</code> limits</li> <li>For local models, ensure you have sufficient GPU memory for the model size</li> <li>Save intermediate outputs frequently to avoid losing progress</li> <li>When running live inference, ensure your retrieval corpus is appropriate for the repository of the issue </li> </ul>"},{"location":"reference/versioning/","title":"Versioning System","text":"<p>Create Training Data</p> <p>If you interested in creating tasks for training models to solve software engineering tasks, check out SWE-smith (Code, Paper).</p> <p>SWE-smith is a toolkit for creating execution environments and SWE-bench style task instances at scale.</p> <p>It is designed to be highly compatible with SWE-agent, to generate training data, and SWE-bench for evaluation.</p>"},{"location":"reference/versioning/#overview","title":"Overview","text":"<p>SWE-bench assigns each task instance a specific <code>version</code> with respect to its repository. This version information is crucial for ensuring reproducible execution-based evaluation, as it determines the exact installation instructions needed for that repository.</p>"},{"location":"reference/versioning/#how-versioning-works","title":"How Versioning Works","text":"<p>When task instances are created, they are assigned a version based on the repository's state at the time of issue creation. This version information enables the evaluation harness to set up the correct environment for testing the proposed patch.</p>"},{"location":"reference/versioning/#tools-for-version-management","title":"Tools for Version Management","text":""},{"location":"reference/versioning/#general-purpose-tool-get_versionspy","title":"General Purpose Tool: <code>get_versions.py</code>","text":"<p>The <code>get_versions.py</code> script is a general-purpose tool for retrieving version information. It can obtain versions through two methods:</p> <ol> <li>Reading directly from the GitHub repository</li> <li>Building the repository locally and locating appropriate version files</li> </ol> <p>The script assigns each task instance a new <code>version: &lt;value&gt;</code> key/value pair in its metadata.</p>"},{"location":"reference/versioning/#usage","title":"Usage","text":"<p>The script can be invoked via the <code>run_get_version.sh</code> wrapper script:</p> <pre><code>python get_versions.py \\\n    --instances_path   [Required] [folder] Path to candidate task instances \\\n    --retrieval_method [Required] [choice] Method to retrieve versions (\"build\", \"mix\", or \"github\") \\\n    --cleanup          [Required] [bool]   Remove testbed and conda environments upon task completion \\\n    --conda_env        [Required] [str]    Name of conda environment to run task installation within \\\n    --num_workers      [Required] [int]    Number of processes to parallelize on \\\n    --path_conda       [Required] [folder] Path to miniconda or anaconda installation \\\n    --output_dir       [Required] [folder] Path to directory to write versioned task instances to \\\n    --testbed          [Required] [folder] Path to testbed directory, for cloning GitHub repos to\n</code></pre>"},{"location":"reference/versioning/#repository-specific-version-extraction","title":"Repository-Specific Version Extraction","text":"<p>For certain repositories, SWE-bench provides specialized scripts in the <code>extract_web/</code> directory that crawl the package's website to find versions and their cutoff dates.</p> <p>These scripts (like <code>get_versions_*.py</code>) can be adapted to other repositories to check task instances' <code>creation_date</code> against the version dates.</p>"},{"location":"reference/versioning/#integration-with-evaluation","title":"Integration with Evaluation","text":"<p>The version information is used by the evaluation harness to:</p> <ol> <li>Set up the correct Docker environment for each task</li> <li>Install the proper dependencies based on the version</li> <li>Ensure consistent evaluation conditions across runs</li> </ol> <p>This versioning system is a key component in making SWE-bench evaluations reproducible and reliable. </p>"}]}